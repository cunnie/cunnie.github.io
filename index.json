[{"content":"Is it worth switching your VMware vSphere VM\u0026rsquo;s SCSI (small computer system interface) from the LSI Logic Parallel controller to the VMware Paravirtual SCSI controller? Except for ultra-high-end database servers (\u0026gt; 1M IOPS ( input/output operations per second)), the answer is \u0026ldquo;no\u0026rdquo;; the difference is negligible.\nOur benchmarks show that VMware\u0026rsquo;s Paravirtual SCSI (small computer system interface) controller offered a 2-3% performance increase in IOPS (I/O (input/output) operations per second) over the LSI Logic Parallel SCSI controller at the cost of a similar decrease in sequential performance (both read \u0026amp; write). Additionally the Paravirtual SCSI controller (pvscsi) had a slight reduction in CPU (central processing unit) usage on the host (best-case scenario is 3% lower CPU usage).\nThe Benchmarks The Paravirtual has better IOPS than the LSI Logic, but worse sequential throughput.\n  On average the Paravirtual Driver\u0026rsquo;s (red) IOPS performance is 2.5% better than the LSI Logic\u0026rsquo;s\n    Although not consistently faster, the LSI Logic averages 2% faster than the Paravirtual on sequential read operations\n    The LSI Logic\u0026rsquo;s sequential write performance is consistently faster than the Paravirtual\u0026rsquo;s by an average of 4%\n  Host CPU Utilization The ESXi host CPU utilization was trickier to measure—we had to eyeball it. You can see the two charts below, taken while we were running our benchmarks. Our guess is that the Paravirtual driver used ~3% less CPU than the LSI Logic.\nNote that 3% is a best-case scenario (you\u0026rsquo;re unlikely to get 10% improvement): the benchmarks were taken on what we affectionately refer to as \u0026ldquo;The world\u0026rsquo;s slowest Xeon\u0026rdquo;, i.e. the Intel Xeon D-1537, which clocks in at an anemic 1.7GHz (side note: We purchased it for its low TDP (Thermal Design Power), not its speed). In other words, this processor is so slow that any improvement in CPU efficiency is readily apparent.\n    Benchmark Setup We paired a slow CPU with a fast disk:\n Samsung SSD 960 2TB M.2 2280 PRO Supermicro X10SDV-8C-TLN4F+ motherboard with a soldered-on 1.7 GHz 8-core Intel Xeon Processor D-1537, and 128 GiB RAM.  We used gobonniego to benchmark the performance.\nWe ran each benchmark for 1 hour.\nWe configured the VMs with 4 vCPUs, 1 GiB RAM and a 20 GiB drive. We used 4 CPUs so that the Xeon\u0026rsquo;s slow speed wouldn\u0026rsquo;t handicap the benchmark results (we wanted the disk to be the bottleneck, not the CPU).\n  The Samsung NVMe SSD 960 Pro is a gold standard for NVMe disk performance.\n  Why These Benchmarks Are Flawed In the spirit of full disclosure, we\u0026rsquo;d like to point out the shortcomings of our benchmarks:\n  We only tested one type of datastore. We tested an NVMe (non-volatile memory express) datastore, but it would have been interesting to test VMware\u0026rsquo;s vSAN (virtual storage area network).\n  We only ran the benchmark for an hour. Had we run the benchmark longer, we would have seen different performance curves. For example, when we ran the benchmarks back-to-back we saw degradation of the throughput from almost 2 GB/s to slightly more than 1 GB/s on the sequential read \u0026amp; write tests. We attribute this degradation to saturation of the Samsung controller.\n  We never modified the kernel settings to enhance pvscsi performance. For example, went with the default settings for queue depth for device (64) and adapter (254), but this VMware knowledgebase (KB) article suggests increasing those to 254 and 1024, respectively.\n  We only tested Linux. Windows performance may be different.\n  We benchmarked slightly different versions of Linux. We tested against two very-close versions of Ubuntu Bionic, but they weren\u0026rsquo;t the exact same version. Had we used completely identical versions, we may have seen different performance numbers.\n  References   Which vSCSI controller should I choose for performance?, Mark Achetemichuk, \u0026ldquo;PVSCSI and LSI Logic Parallel/SAS are essentially the same when it comes to overall performance capability [for customers not producing 1 million IOPS]\u0026rdquo;\n  Achieving a Million I/O Operations per Second from a Single VMware vSphere® 5.0 Host, \u0026ldquo;a PVSCSI adapter provides 8% better throughput at 10% lower CPU cost\u0026rdquo;. Our benchmarks show 2.5%, 3% respectively.\n  Large-scale workloads with intensive I/O patterns might require queue depths significantly greater than Paravirtual SCSI default values (2053145), \u0026ldquo;\u0026hellip; increase PVSCSI queue depths to 254 (for device) and 1024 (for adapter)\u0026rdquo;\n  Raw Benchmark results (JSON-formatted).\n  ","permalink":"https://blog.nono.io/post/pvscsi/","summary":"Is it worth switching your VMware vSphere VM\u0026rsquo;s SCSI (small computer system interface) from the LSI Logic Parallel controller to the VMware Paravirtual SCSI controller? Except for ultra-high-end database servers (\u0026gt; 1M IOPS ( input/output operations per second)), the answer is \u0026ldquo;no\u0026rdquo;; the difference is negligible.\nOur benchmarks show that VMware\u0026rsquo;s Paravirtual SCSI (small computer system interface) controller offered a 2-3% performance increase in IOPS (I/O (input/output) operations per second) over the LSI Logic Parallel SCSI controller at the cost of a similar decrease in sequential performance (both read \u0026amp; write).","title":"Disk Controller Benchmarks: VMware Paravirtual's vs. LSI Logic Parallel's"},{"content":"  In our previous post, we configured our GKE (Google Kubernetes Engine) to use Let\u0026rsquo;s Encrypt TLS certificates. In this post, the capstone of our series, we install Concourse CI.\nInstallation These instructions are a more-opinionated version of the canonical instructions for the Concourse CI Helm chart found here: https://github.com/concourse/concourse-chart.\nFirst Install: with Helm We use helm to install Concourse. We first add the Helm repo, and then install it. We take the opportunity to bump the default login time from 24 hours to ten days (duration=240h) because we hate re-authenticating to our Concourse every morning. Replace gke.nono.io with your DNS record:\nkubectl delete ingress kuard # to free up https://gke.nono.io helm repo add concourse https://concourse-charts.storage.googleapis.com/ helm install gke-nono-io concourse/concourse \\  --set concourse.web.externalUrl=https://gke.nono.io \\  --set concourse.web.auth.duration=240h \\  --set \u0026#39;web.ingress.enabled=true\u0026#39; \\  --set \u0026#39;web.ingress.annotations.cert-manager\\.io/issuer=letsencrypt-prod\u0026#39; \\  --set \u0026#39;web.ingress.annotations.kubernetes\\.io/ingress.class=nginx\u0026#39; \\  --set \u0026#39;web.ingress.hosts={gke.nono.io}\u0026#39; \\  --set \u0026#39;web.ingress.tls[0].hosts[0]=gke.nono.io\u0026#39; \\  --set \u0026#39;web.ingress.tls[0].secretName=gke.nono.io\u0026#39; \\  We wait approximately 30 seconds for the acquisition of the TLS certificate for gke.nono.io, then browse to our site https://gke.nono.io. If we get an HTTP status 503, then wait another ten seconds and try again.\nFirst Upgrade: Locking Down Concourse [This section was added later and has not been thoroughly tested; if you find any mistakes, please let us know. Thanks.]\nOur Concourse is insecure: we haven\u0026rsquo;t changed the default private keys. Our Concourse is public-facing, and we must change the keys lest evildoers compromise us. The Concourse README warns:\n For your convenience, this chart provides some default values for secrets, but it is recommended that you generate and manage these secrets outside the Helm chart.\n Let\u0026rsquo;s make our keys. The Concourse documentation provides two ways to do it, but we\u0026rsquo;re gonna show a third way:\nmkdir -p secrets/ for KEY in session_signing_key tsa_host_key worker_key; do ssh-keygen -t rsa -b 4096 -m PEM -f secrets/$KEY -C $KEY \u0026lt; /dev/null done rm secrets/session_signing_key.pub # \u0026#34;You can remove the session_signing_key.pub file if you have one, it is not needed by any process in Concourse\u0026#34; While we\u0026rsquo;re locking things down, we also remove the local user \u0026ldquo;test\u0026rdquo; (along with the easy-to-guess password, \u0026ldquo;test\u0026rdquo;). We do this by setting secrets.localUsers to \u0026ldquo;\u0026rdquo;. Just to be safe, we disable local auth entirely (we set concourse.web.localAuth.enabled to false).\nhelm upgrade gke-nono-io concourse/concourse \\  --set concourse.web.externalUrl=https://gke.nono.io \\  --set concourse.web.auth.duration=240h \\  --set \u0026#39;web.ingress.enabled=true\u0026#39; \\  --set \u0026#39;web.ingress.annotations.cert-manager\\.io/issuer=letsencrypt-prod\u0026#39; \\  --set \u0026#39;web.ingress.annotations.kubernetes\\.io/ingress.class=nginx\u0026#39; \\  --set \u0026#39;web.ingress.hosts={gke.nono.io}\u0026#39; \\  --set \u0026#39;web.ingress.tls[0].hosts[0]=gke.nono.io\u0026#39; \\  --set \u0026#39;web.ingress.tls[0].secretName=gke.nono.io\u0026#39; \\  \\  --set-file secrets.sessionSigningKey=secrets/session_signing_key \\  --set-file secrets.hostKey=secrets/tsa_host_key \\  --set-file secrets.hostKeyPub=secrets/tsa_host_key.pub \\  --set-file secrets.workerKey=secrets/worker_key \\  --set-file secrets.workerKeyPub=secrets/worker_key.pub \\  --set secrets.localUsers=\u0026#34;\u0026#34; \\  --set concourse.web.localAuth.enabled=false \\  Third Upgrade: now with GitHub OAuth We have a Concourse CI server, but we can\u0026rsquo;t log in. What to do?\nWe want to authenticate against our GitHub organization, \u0026ldquo;blabbertabber\u0026rdquo;, so we browse to our organization (https://github.com/blabbertabber) → Settings → Developer Settings → OAuth Apps → New OAuth App.\nNote: \u0026ldquo;Note that the client must be created under an organization if you want to authorize users based on organization/team membership.\u0026quot;\nHere\u0026rsquo;s how we filled out ours. Make sure to replace gke.nono.io with your URL. The authorization callback URL is particularly important; don\u0026rsquo;t mess it up:\n  We click \u0026ldquo;Register Application\u0026rdquo;, which brings us to the next screen, where we get the Client ID (2317874f614900d21bdd) and then click \u0026ldquo;Generate a new client secret\u0026rdquo; to get the Client secret (08d670a1916193d9294705e223fb0a09d0fccb08). Don\u0026rsquo;t forget to click \u0026ldquo;Update Application\u0026rdquo;!\n  Now we can add the five GitHub OAuth-related lines to our helm upgrade command. Replace the GitHub org blabbertabber and the GitHub Client ID and Client Secret with the ones you\u0026rsquo;ve created:\nhelm upgrade gke-nono-io concourse/concourse \\  --set concourse.web.externalUrl=https://gke.nono.io \\  --set concourse.web.auth.duration=240h \\  --set \u0026#39;web.ingress.enabled=true\u0026#39; \\  --set \u0026#39;web.ingress.annotations.cert-manager\\.io/issuer=letsencrypt-prod\u0026#39; \\  --set \u0026#39;web.ingress.annotations.kubernetes\\.io/ingress.class=nginx\u0026#39; \\  --set \u0026#39;web.ingress.hosts={gke.nono.io}\u0026#39; \\  --set \u0026#39;web.ingress.tls[0].hosts[0]=gke.nono.io\u0026#39; \\  --set \u0026#39;web.ingress.tls[0].secretName=gke.nono.io\u0026#39; \\  \\  --set-file secrets.sessionSigningKey=secrets/session_signing_key \\  --set-file secrets.hostKey=secrets/tsa_host_key \\  --set-file secrets.hostKeyPub=secrets/tsa_host_key.pub \\  --set-file secrets.workerKey=secrets/worker_key \\  --set-file secrets.workerKeyPub=secrets/worker_key.pub \\  --set secrets.localUsers=\u0026#34;\u0026#34; \\  --set concourse.web.localAuth.enabled=false \\  \\  --set concourse.web.auth.mainTeam.github.org=blabbertabber \\  --set concourse.web.auth.github.enabled=true \\  --set secrets.githubClientId=5e4ffee9dfdced62ebe3 \\  --set secrets.githubClientSecret=549e10b1680ead9cafa30d4c9a715681cec9b074 \\  We wait our 30 seconds, and then browse to our URL: https://gke.nono.io. We log in with GitHub Auth. We authorize our app. We download \u0026amp; install our fly CLI. Then we log in:\nfly -t gke login -c https://gke.nono.io # click the link # click \u0026#34;Authorize blabbertabber\u0026#34; # see \u0026#34;login successful!\u0026#34; We create the following simple pipeline file, simple.yml:\njobs: - name: simple plan: - task: simple config: platform: linux image_resource: type: docker-image source: repository: fedora run: path: \u0026#34;true\u0026#34; Let\u0026rsquo;s fly our new pipeline:\nfly -t gke set-pipeline -p simple -c simple.yml fly -t gke expose-pipeline -p simple fly -t gke unpause-pipeline -p simple We browse to our Concourse and see the sweet green of success (it\u0026rsquo;ll take a minute or two to run):\n  Yay! We\u0026rsquo;re done.\nPro-tip Rather than having an onerous number of --set arguments to our helm upgrade command, we find it easier to modify the corresponding settings in the values.yml file and pass it to our invocation of helm, i.e. helm upgrade -f values.yml .... Here\u0026rsquo;s our file of overrides.\nAddendum: Keeping Concourse Up-to-date [Warning: this procedure has not been tested as-is (my setup is slightly different). Let us know if this procedure doesn\u0026rsquo;t work.]\nBlindly upgrading Concourse without reading the release notes is a recipe for disaster; however, that\u0026rsquo;s what we\u0026rsquo;re going to show you. Let\u0026rsquo;s update the Helm repos first.\nhelm repo update Now let\u0026rsquo;s upgrade our install:\nhelm upgrade gke-nono-io concourse/concourse \\  --set concourse.web.externalUrl=https://gke.nono.io \\  --set concourse.web.auth.duration=240h \\  --set \u0026#39;web.ingress.enabled=true\u0026#39; \\  --set \u0026#39;web.ingress.annotations.cert-manager\\.io/issuer=letsencrypt-prod\u0026#39; \\  --set \u0026#39;web.ingress.annotations.kubernetes\\.io/ingress.class=nginx\u0026#39; \\  --set \u0026#39;web.ingress.hosts={gke.nono.io}\u0026#39; \\  --set \u0026#39;web.ingress.tls[0].hosts[0]=gke.nono.io\u0026#39; \\  --set \u0026#39;web.ingress.tls[0].secretName=gke.nono.io\u0026#39; \\  \\  --set-file secrets.sessionSigningKey=secrets/session_signing_key \\  --set-file secrets.hostKey=secrets/tsa_host_key \\  --set-file secrets.hostKeyPub=secrets/tsa_host_key.pub \\  --set-file secrets.workerKey=secrets/worker_key \\  --set-file secrets.workerKeyPub=secrets/worker_key.pub \\  --set secrets.localUsers=\u0026#34;\u0026#34; \\  --set concourse.web.localAuth.enabled=false \\  \\  --set concourse.web.auth.mainTeam.github.org=blabbertabber \\  --set concourse.web.auth.github.enabled=true \\  --set secrets.githubClientId=5e4ffee9dfdced62ebe3 \\  --set secrets.githubClientSecret=549e10b1680ead9cafa30d4c9a715681cec9b074 \\  Browse to your Concourse server, and check that it has the updated version number.\n References  Concourse CI Helm chart: https://github.com/concourse/concourse-chart Helm Chart Install: Advanced Usage of the “Set” Argument: https://itnext.io/helm-chart-install-advanced-usage-of-the-set-argument-3e214b69c87a Creating an OAuth App on GitHub: https://docs.github.com/en/developers/apps/building-oauth-apps/creating-an-oauth-app  Updates/Errata 2021-11-13 Added section on keeping Concourse up-to-date.\n2021-11-14 Added section on locking down Concourse.\n","permalink":"https://blog.nono.io/post/concourse_on_k8s-4/","summary":"In our previous post, we configured our GKE (Google Kubernetes Engine) to use Let\u0026rsquo;s Encrypt TLS certificates. In this post, the capstone of our series, we install Concourse CI.\nInstallation These instructions are a more-opinionated version of the canonical instructions for the Concourse CI Helm chart found here: https://github.com/concourse/concourse-chart.\nFirst Install: with Helm We use helm to install Concourse. We first add the Helm repo, and then install it.","title":"Concourse CI on Kubernetes (GKE), Part 4: Concourse"},{"content":"  In our previous blog post, we configured ingress to our Kubernetes cluster but were disappointed to discover that the TLS certificates were self-signed. In this post we\u0026rsquo;ll remedy that by installing cert-manager, the Cloud native certificate management tool.\nDisclaimer: most of this blog post was lifted whole cloth from the most-excellent cert-manager documentation. We merely condensed it \u0026amp; made it more opinionated.\nInstallation Let\u0026rsquo;s add the Jetstack Helm Repository:\nhelm repo add jetstack https://charts.jetstack.io helm repo update Let\u0026rsquo;s install cert-manager:\nhelm install \\  cert-manager jetstack/cert-manager \\  --namespace cert-manager \\  --create-namespace \\  --version v1.6.0 \\  --set installCRDs=true Verifying Installation Do we see all three pods?\nkubectl get pods --namespace cert-manager Now let\u0026rsquo;s create an issuer to test the webhook:\ncat \u0026lt;\u0026lt;EOF \u0026gt; test-resources.yaml apiVersion: v1 kind: Namespace metadata: name: cert-manager-test --- apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: test-selfsigned namespace: cert-manager-test spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: selfsigned-cert namespace: cert-manager-test spec: dnsNames: - example.com secretName: selfsigned-cert-tls issuerRef: name: test-selfsigned EOF And now let\u0026rsquo;s apply those resources:\nkubectl apply -f test-resources.yaml sleep 10 kubectl describe certificate -n cert-manager-test | grep \u0026#34;has been successfully\u0026#34; kubectl delete -f test-resources.yaml 4. Deploy an Example Service Let\u0026rsquo;s install the sample services to test the controller:\nkubectl apply -f https://netlify.cert-manager.io/docs/tutorials/acme/example/deployment.yaml kubectl apply -f https://netlify.cert-manager.io/docs/tutorials/acme/example/service.yaml Let\u0026rsquo;s download and edit the Ingress (we\u0026rsquo;ve already configured gke.nono.io to point to the GCP/GKE load balancer at 34.135.26.144). Replace gke.nono.io with the DNS record of your load balancer set up in the previous blog post:\ncurl -o ingress-kuard.yml -L https://netlify.cert-manager.io/docs/tutorials/acme/example/ingress.yaml sed -i \u0026#39;\u0026#39; \u0026#34;s/example.example.com/gke.nono.io/g\u0026#34; ingress-kuard.yml kubectl apply -f ingress-kuard.yml Let\u0026rsquo;s use curl to check the GKE load balancer. Replace gke.nono.io with the DNS record of your load balancer set up in the previous blog post:\ncurl -kivL -H \u0026#39;Host: gke.nono.io\u0026#39; \u0026#39;http://gke.nono.io\u0026#39; You should see output similar to the following (note that the cert is still self-signed):\n... * Server certificate: * subject: O=Acme Co; CN=Kubernetes Ingress Controller Fake Certificate 6. Configure Let’s Encrypt Issuer Let\u0026rsquo;s deploy the staging \u0026amp; production issuers. Replace brian.cunnie@gmail.com with your email address:\nkubectl apply -f \u0026lt;( curl -o- https://cert-manager.io/docs/tutorials/acme/example/staging-issuer.yaml | sed \u0026#39;s/user@example.com/brian.cunnie@gmail.com/\u0026#39;) kubectl apply -f \u0026lt;( curl -o- https://cert-manager.io/docs/tutorials/acme/example/production-issuer.yaml | sed \u0026#39;s/user@example.com/brian.cunnie@gmail.com/\u0026#39;) # check to make sure they were deployed: kubectl describe issuer letsencrypt-staging kubectl describe issuer letsencrypt-prod 7. Step 7 - Deploy a TLS Ingress Resource Let\u0026rsquo;s deploy the ingress resource using annotations to obtain the certificate. Replace gke.nono.io with the DNS record of your load balancer set up in the previous blog post:\nkubectl apply -f \u0026lt;( curl -o- https://cert-manager.io/docs/tutorials/acme/example/ingress-tls.yaml | sed \u0026#39;s/example.example.com/gke.nono.io/\u0026#39;) kubectl get certificate # takes ~30s to become ready (\u0026#34;READY\u0026#34; == \u0026#34;True\u0026#34;) kubectl describe certificate quickstart-example-tls kubectl describe secret quickstart-example-tls Let\u0026rsquo;s use curl again to check the GKE load balancer\u0026rsquo;s certificate. Replace gke.nono.io with the DNS record of your load balancer set up in the previous blog post:\ncurl -kivL -H \u0026#39;Host: gke.nono.io\u0026#39; \u0026#39;http://gke.nono.io\u0026#39; You should see output similar to the following:\n... * Server certificate: * subject: CN=gke.nono.io * start date: Sep 1 22:02:55 2021 GMT * expire date: Nov 30 22:02:54 2021 GMT * issuer: C=US; O=(STAGING) Let's Encrypt; CN=(STAGING) Artificial Apricot R3 Great! We have the staging cert, but that\u0026rsquo;s not quite good enough—we want a real certificate. Let\u0026rsquo;s upgrade to the production certificate. As usual, Replace gke.nono.io with the DNS record of your load balancer set up in the previous blog post:\nkubectl apply -f \u0026lt;( curl -o- https://cert-manager.io/docs/tutorials/acme/example/ingress-tls-final.yaml | sed \u0026#39;s/example.example.com/gke.nono.io/\u0026#39;) kubectl delete secret quickstart-example-tls # triggers the process to get a new certificate kubectl get certificate # takes ~30s to become ready (\u0026#34;READY\u0026#34; == \u0026#34;True\u0026#34;) kubectl describe certificate quickstart-example-tls kubectl describe secret quickstart-example-tls Let\u0026rsquo;s use curl one more time to check the GKE load balancer\u0026rsquo;s certificate. Replace gke.nono.io with the DNS record of your load balancer set up in the previous blog post:\ncurl -kivL -H \u0026#39;Host: gke.nono.io\u0026#39; \u0026#39;http://gke.nono.io\u0026#39; You should see output similar to the following:\n... * Server certificate: * subject: CN=gke.nono.io * start date: Sep 1 22:11:36 2021 GMT * expire date: Nov 30 22:11:35 2021 GMT * issuer: C=US; O=Let's Encrypt; CN=R3 * SSL certificate verify ok. And now browse (replacing gke.nono.io with the DNS record of your load balancer): https://gke.nono.io/. Yes, we get an HTTP 503 status, but our certificate is valid!\nStay Tuned! Stay tuned for our next installment, where we install Concourse CI on GKE.\n References  cert-manager documentation: https://cert-manager.io/docs/  Updates/Errata 2021-11-13 Bumped the cert-manager version 1.5.0 → 1.6.0\n","permalink":"https://blog.nono.io/post/concourse_on_k8s-3/","summary":"In our previous blog post, we configured ingress to our Kubernetes cluster but were disappointed to discover that the TLS certificates were self-signed. In this post we\u0026rsquo;ll remedy that by installing cert-manager, the Cloud native certificate management tool.\nDisclaimer: most of this blog post was lifted whole cloth from the most-excellent cert-manager documentation. We merely condensed it \u0026amp; made it more opinionated.\nInstallation Let\u0026rsquo;s add the Jetstack Helm Repository:","title":"Concourse CI on Kubernetes (GKE), Part 3: TLS"},{"content":"  In our previous blog post, we set up our Kubernetes cluster and deployed a pod running nginx, but the experience was disappointing—we couldn\u0026rsquo;t browse to our pod. Let\u0026rsquo;s fix that by deploying the nginx Ingress controller.\nAcquire the External IP Address (Elastic IP) We\u0026rsquo;ll use the Google Cloud console to acquire the external address [external address] for our load balancer.\nNavigate to VPC network → External IP addresses → Reserve Static Address:\n Name: gke-nono-io (or \u0026ldquo;gke-\u0026rdquo; and whatever your domain is, with dashes not dots) Description: Ingress for GKE    In our example, we acquire the IP address, 34.135.26.144.\nCreate DNS Record to Point to Acquired IP Address You\u0026rsquo;ll need a DNS domain for this part. In our examples, we use the domain \u0026ldquo;nono.io\u0026rdquo;, so whenever you see \u0026ldquo;nono.io\u0026rdquo;, substitute \u0026amp; replace your domain. Similarly, whenever you see \u0026ldquo;34.135.26.144\u0026rdquo;, substitute your external IP address.\nAdding a DNS record is outside the scope of the humble blog post (we use BIND, but these days services such as AWS\u0026rsquo;s Route 53 are all the rage).\nWe create the DNS address record \u0026ldquo;gke.nono.io\u0026rdquo; to point to \u0026ldquo;34.135.26.144\u0026rdquo;; Let\u0026rsquo;s test to make sure it\u0026rsquo;s set up properly:\ndig gke.nono.io +short # should return 34.135.26.144 Create Kubernetes Ingress nginx Manifest Files We\u0026rsquo;re going to shamelessly copy the canonical Ingress nginx manifest files and modify them to include our static IP address:\ncurl -L https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.48.1/deploy/static/provider/cloud/deploy.yaml \\  -o nginx-ingress-controller.yml nvim nginx-ingress-controller.yml We need to add our IP address to our load balancer Kubernetes service. search for the string \u0026ldquo;LoadBalancer\u0026rdquo; and add the IP address as shown below (don\u0026rsquo;t include the plus sign \u0026ldquo;+\u0026rdquo; in your file):\nspec: type: LoadBalancer externalTrafficPolicy: Local + loadBalancerIP: 34.135.26.144  ports: - name: http port: 80 Let\u0026rsquo;s apply our changes:\nkubectl apply -f nginx-ingress-controller.yml Let\u0026rsquo;s wait for the change to have completed\nkubectl wait --namespace ingress-nginx \\  --for=condition=ready pod \\  --selector=app.kubernetes.io/component=controller \\  --timeout=120s Let\u0026rsquo;s browse to our endpoint: http://gke.nono.io. We see the nginx \u0026ldquo;404 Not Found\u0026rdquo; status page, but that\u0026rsquo;s reassuring: it means we\u0026rsquo;ve properly set up the nginx controller, but haven\u0026rsquo;t yet set up Ingress to our existing pods.\nBefore we set up Ingress, let\u0026rsquo;s check our HTTPS endpoint: https://gke.nono.io.\nWait, what is this? We\u0026rsquo;re seeing an unsettling message, \u0026ldquo;Warning: Potential Security Risk Ahead\u0026rdquo; (Chrome users may see \u0026ldquo;Your connection is not private\u0026rdquo;; Safari users, \u0026ldquo;This Connection Is Not Private\u0026rdquo;). We\u0026rsquo;re upset—we don\u0026rsquo;t want to be seen as losers who are using self-signed TLS certificates; we want to be winners who are using certificates from Let\u0026rsquo;s Encrypt.\nStay Tuned! Stay tuned for our next installment, where we configure Let\u0026rsquo;s Encrypt certificates for our TLS (Transport Layer Security) endpoints.\n References  The canonical Kubernetes documentation for deploying the nginx Ingress controller on GKE https://kubernetes.github.io/ingress-nginx/deploy/#gce-gke  Footnotes external address\nYou can also acquire the external address via the command line (don\u0026rsquo;t forget to change \u0026ldquo;blabbertabber\u0026rdquo; to your project\u0026rsquo;s name):\ngcloud compute addresses create gke-nono-io --project=blabbertabber --description=Ingress\\ for\\ GKE --region=us-central1 Or, for the truly advanced among you, you can modify your terraform templates to acquire the address for you. The terraform site has great documentation, and here\u0026rsquo;s the snippet you\u0026rsquo;ll need:\nmodule \u0026#34;address-fe\u0026#34; { source = \u0026#34;terraform-google-modules/address/google\u0026#34; version = \u0026#34;0.1.0\u0026#34; names = [ \u0026#34;gke-nono-io\u0026#34;] global = true } ","permalink":"https://blog.nono.io/post/concourse_on_k8s-2/","summary":"In our previous blog post, we set up our Kubernetes cluster and deployed a pod running nginx, but the experience was disappointing—we couldn\u0026rsquo;t browse to our pod. Let\u0026rsquo;s fix that by deploying the nginx Ingress controller.\nAcquire the External IP Address (Elastic IP) We\u0026rsquo;ll use the Google Cloud console to acquire the external address [external address] for our load balancer.\nNavigate to VPC network → External IP addresses → Reserve Static Address:","title":"Concourse CI on Kubernetes (GKE), Part 2: Ingress"},{"content":"  Let\u0026rsquo;s deploy Concourse, a continuous-integration, continuous delivery (CI/CD) application (similar to Jenkins and CircleCI).\nWe\u0026rsquo;ll deploy it to Google Cloud, to our Google Kubernetes Engine (GKE).\nIn this post, we\u0026rsquo;ll use HashiCorp\u0026rsquo;s Terraform to create our cluster.\nWe assume you\u0026rsquo;ve already installed the terraform command-line interface (CLI) and created a Google Cloud account.\nmkdir -p ~/workspace/gke cd ~/workspace/gke Next we download the terraform templates and terraform vars file:\ncurl -OL https://raw.githubusercontent.com/cunnie/deployments/6b230118399f4326094b4d60e21cda32e8c6f321/terraform/gcp/gke/gke.tf curl -OL https://raw.githubusercontent.com/cunnie/deployments/6b230118399f4326094b4d60e21cda32e8c6f321/terraform/gcp/gke/vpc.tf curl -OL https://raw.githubusercontent.com/cunnie/deployments/6b230118399f4326094b4d60e21cda32e8c6f321/terraform/gcp/gke/terraform.tfvars curl -OL https://raw.githubusercontent.com/cunnie/deployments/6b230118399f4326094b4d60e21cda32e8c6f321/terraform/gcp/gke/outputs.tf At this point we hear cries of protest, \u0026ldquo;What?! Downloading dubious files from sketchy software developers on the internet? Files whose provenance is murky at best?\u0026rdquo;\nLet us reassure you: the provenance of these files is crystal-clear: they have been patterned after templates from HashiCorp\u0026rsquo;s excellent tutorial, Provision a GKE Cluster (Google Cloud), and the companion git repo, https://github.com/hashicorp/learn-terraform-provision-gke-cluster. [provenance]\nLet\u0026rsquo;s login with gcloud:\ngcloud auth application-default login (if you get a command not found error, then it means you need to install Google Cloud\u0026rsquo;s CLI; the HashiCorp tutorial has great instructions.)\nLet\u0026rsquo;s customize our terraform.tfvars file. At the very least, change the project_id to your Google Cloud\u0026rsquo;s project\u0026rsquo;s ID. If you\u0026rsquo;re not sure what that is, you can find it on the Google console:\n  Let\u0026rsquo;s use neovim (or your editor of choice):\nnvim terraform.tfvars Let\u0026rsquo;s change the Project ID to \u0026ldquo;my-google-project-id\u0026rdquo; (assuming that\u0026rsquo;s your Google Project\u0026rsquo;s name, which it isn\u0026rsquo;t):\n-project_id = \u0026#34;blabbertabber\u0026#34; -friendly_project_id = \u0026#34;nono\u0026#34; +project_id = \u0026#34;my-google-project-id\u0026#34; +friendly_project_id = \u0026#34;my-google-project-id\u0026#34; We\u0026rsquo;re ready to terraform!\nterraform init terraform apply # answer \u0026#34;yes\u0026#34; when asked, \u0026#34;Do you want to perform these actions?\u0026#34; The terraform apply takes ~10 minutes to complete. Now let\u0026rsquo;s get our cluster credentials:\ngcloud container clusters get-credentials $(terraform output -raw kubernetes_cluster_name) --zone $(terraform output -raw zone) We have a cluster at this point—let\u0026rsquo;s test by deploying nginx:\nkubectl run nginx --image=nginx kubectl get pods You should see the following output:\nNAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 2m21s Save that terraform.tfstate file! Save the terraform.tfstate file; if you lose it, it becomes much more difficult to make changes to your terraform infrastructure (you\u0026rsquo;ll have to manually tear it down \u0026amp; start from scratch).\nWe won\u0026rsquo;t tell you how or where to save it, but we will tell you that we\u0026rsquo;ve chosen to save ours in a public GitHub repo. This is a bad idea! terraform.tfstate files often contain secrets which you do not want to make public. Ours doesn\u0026rsquo;t appear to contain any secrets, and we like to have it publicly viewable for instructional purposes, but we may have made a terrible mistake by publishing it.\nStay Tuned! Stay tuned for the next installment, where we configure load balancers and install Concourse CI.\n References  HashiCorp\u0026rsquo;s excellent tutorial, Provision a GKE Cluster (Google Cloud) https://learn.hashicorp.com/terraform/kubernetes/provision-gke-cluster Companion GitHub repository, https://github.com/hashicorp/learn-terraform-provision-gke-cluster https://github.com/cloudfoundry/bosh-community-stemcell-ci-infra is the repo that contains the scripts to spin up Concourse CI on GKE, and may be of interest to those who would like a more automated way of spinning up Concourse on GKE. This repo is actively maintained, and is used by the team that produces the BOSH Bionic stemcells. \u0026ldquo;It also uses config connector instead of Terraform for managing the cloud SQL instance https://cloud.google.com/config-connector/docs/overview.\u0026rdquo; Thanks, Ruben Koster! https://github.com/pivotal-cf/pci-infrastructure/tree/master/k8s \u0026ldquo;sets up a GKE cluster using Terraform, Vault with Google KMS [Key Management Service] integration, Concourse with Vault integration, as well as cert-manager, Kubernetes ingress-nginx and external DNS that hooks into Google Cloud\u0026rsquo;s DNS to create our external IP.\u0026rdquo; Thanks Brian Rieger! https://github.com/skyscrapers/terraform-concourse is a good resource for those interested in deploying to AWS instead of GCP. Thanks Ringo De Smet!  Updates/Errata 2021-09-16 Added two additional references for more complete/more automated ways to spin up Concourse on GKE.\n2021-09-30 Added an additional reference for those interested in deploying to AWS.\nFootnotes provenance\nThis begs the question, \u0026ldquo;If we\u0026rsquo;re patterning our templates after HashiCorp\u0026rsquo;s, why not use HashiCorp\u0026rsquo;s directly? Why change the templates?\u0026rdquo;\nOur answer: \u0026ldquo;If you want to use the HashiCorp templates, by all means do so—they\u0026rsquo;re great templates!\u0026rdquo;\nOur templates have been modified from HashiCorp\u0026rsquo;s to suit our purposes; for example, we split the templates into a virtual private cloud (VPC) (vpc.tf) template and a Google Kubernetes Engine (gke) (gke.tf) template. It seemed like a good idea at the time. Also, we didn\u0026rsquo;t want to spend a lot of money, so instead of three instances in the region, we modified the template to place two instances in the same availability zone.\n[e2-medium instances cost $24.46 / month in the region us-central1. We didn\u0026rsquo;t want to spend the extra $25 for a third instance.]\nFinally, we didn\u0026rsquo;t like the name of our Google Cloud project (\u0026ldquo;blabbertabber\u0026rdquo;): it was too long \u0026amp; referred to a project we had mothballed months ago. We wanted a shorter and friendlier name (\u0026ldquo;nono\u0026rdquo;), and we were loath to create a brand new Google Cloud project, so we modified the templates to include a \u0026ldquo;friendly\u0026rdquo; project name.\n","permalink":"https://blog.nono.io/post/concourse_on_k8s-1/","summary":"Let\u0026rsquo;s deploy Concourse, a continuous-integration, continuous delivery (CI/CD) application (similar to Jenkins and CircleCI).\nWe\u0026rsquo;ll deploy it to Google Cloud, to our Google Kubernetes Engine (GKE).\nIn this post, we\u0026rsquo;ll use HashiCorp\u0026rsquo;s Terraform to create our cluster.\nWe assume you\u0026rsquo;ve already installed the terraform command-line interface (CLI) and created a Google Cloud account.\nmkdir -p ~/workspace/gke cd ~/workspace/gke Next we download the terraform templates and terraform vars file:","title":"Concourse CI on Kubernetes (GKE), Part 1: Terraform"},{"content":"Why am I creating a new blog? What was wrong with the old blog? Why don\u0026rsquo;t I use Medium?\nThe short version: The old blog is frozen in time, like a prince caught in amber 1 or a dandy in aspic 2. I can no longer post to it.\nThe old blog, the Pivotal Engineering Journal, which many of Pivotal\u0026rsquo;s engineers contributed to, was archived a year after VMware acquired Pivotal. Every acquisition brings changes, and this was, in the scheme of things, a very minor one. At least VMware kept the blog instead of simply discarding it.\nDozens of my blog posts, representing hundreds of hours of work, are now tucked away in an even smaller corner of the internet. That\u0026rsquo;s okay: technical posts have a short shelf life. They have served their purpose.\nBut I cannot lay the fault at the feet of the VMware acquisition; I, too, had a part: My interest in blogging had waned, my output, diminished. In times past I had blogged as frequently as every two months, but then slowed to a crawl: once every six months, and now it has been over a year since I wrote a technical blog post.\nThen the wind shifted, and I again felt the urge to write. Perhaps a VMware corporate blog? Maybe not. I wasn\u0026rsquo;t sure if they\u0026rsquo;d be hands-off, allowing me to choose my topics, express my opinions.\nPerhaps Medium? I discounted that, too, for I find the experience of reading articles on Medium unpleasant, especially the plaintive warning that accompanies every post: \u0026ldquo;You have 2 free member-only stories left this month. Upgrade for unlimited access.\u0026rdquo; I wanted my readers to have unfettered access to my writing.\nFinally, I like writing in Markdown using Neovim: less mouse, more keyboard.\nThe winner? A combination of Hugo and GitHub Pages.\n","permalink":"https://blog.nono.io/post/why_new_blog/","summary":"Why am I creating a new blog? What was wrong with the old blog? Why don\u0026rsquo;t I use Medium?\nThe short version: The old blog is frozen in time, like a prince caught in amber 1 or a dandy in aspic 2. I can no longer post to it.\nThe old blog, the Pivotal Engineering Journal, which many of Pivotal\u0026rsquo;s engineers contributed to, was archived a year after VMware acquired Pivotal.","title":"The Old Blog is Dead. Long Live the New Blog!"}]