[{"content":" Network Diagram. We want to maximize the throughput from the blue box (the client) to the green box (HAProxy)\nSummary We were able to push through almost 450 MB/sec through HAProxy (which terminated our SSL) by carefully matching our 4-core HAProxy with 2 x 4-core Gorouters (which were on a much slower ESXi host).\nResults Bandwidth MB/second Configuration 201.27MB 1 HAProxy: 1 vCPU 136.47MB 1 HAProxy: 2 vCPUs 270.56MB 2 Gorouters: 1 vCPU 350.48MB 2 Gorouters: 2 vCPUs 447.49MB 1 HAProxy, 2 Gorouters: 4 vCPUs 0. HAProxy with 1 vCPU HAProxy had only 1 vCPU during this iteration, and the CPU was maxed to 100% during the test (according to htop). We suspect that TLS was the culprit for much of the traffic: HAProxy terminated TLS traffic inbound, and initiated TLS to the Gorouters.\n1. HAProxy with 2 vCPUs Surprisingly, adding a second vCPU (2 vCPUs, 1 socket) made things worse; bandwidth dropped \u0026gt;30%.\nAlthough the HAProxy never maxed-out its CPU, the Gorouter did, pinned at 100% during the duration of the test (htop).\n2. Two Gorouters Previously we only had 1 Gorouter backing the HAProxy, but now we doubled it to 2 Gorouters. The doubling of VMs notwithstanding, the Gorouters\u0026rsquo; CPUs were still pegged at 100%.\n2. Two Gorouters with 2 vCPUs Previously our Gorouters had 1 vCPU. We doubled it to 2 vCPUs. We saw that our HAProxy\u0026rsquo;s 2 cores were pegged at 100%, and the Gorouters\u0026rsquo; 2 cores were almost maxed-out.\nNote that the Gorouters are on a Xeon which can be charitably characterized as \u0026ldquo;the world\u0026rsquo;s slowest Xeon\u0026rdquo;. In other words, don\u0026rsquo;t extrapolate the ratios of HAProxies to Gorouters from this particularly lopsided setup.\n2. One HAProxy, Two Gorouters with 4 vCPUs We were able to push through almost 450 MB/s, but the HAProxy\u0026rsquo;s CPU was almost, but not quite, 100%. We\u0026rsquo;ve reached the hardware limits of our homelab—we don\u0026rsquo;t have enough physical cores to tune any further, so we bring this post to a close.\nReferences Test suite: https://github.com/wg/wrk Test suite invocation: ./wrk -t8 -c32 -d60s https://10.9.250.10/10mb -H \u0026#34;Host: dora.foundry.fun\u0026#34; Our 10MB app: https://github.com/cloudfoundry/cf-acceptance-tests Our change to the code (thanks Matthew Kocher!): --- a/assets/dora/dora.rb +++ b/assets/dora/dora.rb @@ -39,6 +39,11 @@ class Dora \u0026lt; Sinatra::Base end end + get \u0026#39;/10mb\u0026#39; do + require \u0026#39;securerandom\u0026#39; + $ten_mb ||= SecureRandom.random_bytes(10 * 1024 * 1024) + end + ","permalink":"https://blog.nono.io/post/tuning_haproxy/","summary":"Network Diagram. We want to maximize the throughput from the blue box (the client) to the green box (HAProxy)\nSummary We were able to push through almost 450 MB/sec through HAProxy (which terminated our SSL) by carefully matching our 4-core HAProxy with 2 x 4-core Gorouters (which were on a much slower ESXi host).\nResults Bandwidth MB/second Configuration 201.27MB 1 HAProxy: 1 vCPU 136.47MB 1 HAProxy: 2 vCPUs 270.56MB 2 Gorouters: 1 vCPU 350.","title":"Tuning HAProxy in a vSphere Environment"},{"content":" The Cloud Foundry Acceptance Tests are the gold standard to test the proper functioning of your Cloud Foundry deployment. This guide tells you how to run them. When in doubt, refer to the README.\nQuick Start cd ~/workspace/ git clone git@github.com:cloudfoundry/cf-acceptance-tests.git cd cf-acceptance-tests . ./.envrc bin/update_submodules cp example-cats-config.json cats-config.json export CONFIG=cats-config.json cf api api.cf.nono.io # or whatever your Cloud Foundry\u0026#39;s API endpoint is cf login -u admin cf create-space -o system system # don\u0026#39;t worry if it\u0026#39;s already created cf t -o system -s system cf enable-feature-flag diego_docker # necessary if you\u0026#39;re running the Docker tests (`\u0026#34;include_docker\u0026#34;: true`) Let\u0026rsquo;s configure our cats-config.json. You should know the values for all the replacements except credhub_secret; we\u0026rsquo;ll explain how to get that next:\n-IN DEVELOPMENT { - \u0026#34;api\u0026#34;: \u0026#34;api.DOMAIN.com\u0026#34;, - \u0026#34;apps_domain\u0026#34;: \u0026#34;DOMAIN.com\u0026#34;, + \u0026#34;api\u0026#34;: \u0026#34;api.cf.nono.io\u0026#34;, + \u0026#34;apps_domain\u0026#34;: \u0026#34;foundry.fun\u0026#34;, \u0026#34;admin_user\u0026#34;: \u0026#34;admin\u0026#34;, - \u0026#34;admin_password\u0026#34;: \u0026#34;PASSWORD\u0026#34;, + \u0026#34;admin_password\u0026#34;: \u0026#34;MySecretAdminPassword\u0026#34;, \u0026#34;credhub_mode\u0026#34;: \u0026#34;assisted\u0026#34;, - \u0026#34;credhub_client\u0026#34;: \u0026#34;CREDHUB_CLIENT\u0026#34;, - \u0026#34;credhub_secret\u0026#34;: \u0026#34;CREDHUB_SECRET\u0026#34;, + \u0026#34;credhub_client\u0026#34;: \u0026#34;credhub_admin_client\u0026#34;, + \u0026#34;credhub_secret\u0026#34;: \u0026#34;XDBlXvmH7aN3IG2czVfzvitu5dTHIj\u0026#34;, \u0026#34;artifacts_directory\u0026#34;: \u0026#34;logs\u0026#34;, \u0026#34;skip_ssl_validation\u0026#34;: true, \u0026#34;timeout_scale\u0026#34;: 1, By default you\u0026rsquo;re running the CredHub tests, so you need credhub_secret, but getting it is a multi-step affair. First, get the credentials to communicate with the BOSH Director\u0026rsquo;s CredHub:\nbosh int --path /instance_groups/name=bosh/jobs/name=uaa/properties/uaa/scim/users/name=credhub_cli_user bosh-vsphere.yml [where bosh-vsphere.yml is your BOSH Director\u0026rsquo;s manifest]\nThe output should look something like the following:\ngroups: - credhub.read - credhub.write name: credhub_cli_user password: SomeKrazyPassword Now that we have the password (\u0026ldquo;SomeKrazyPassword\u0026rdquo;), let\u0026rsquo;s authenticate to the BOSH Director\u0026rsquo;s CredHub using the CredHub CLI:\ncredhub api bosh-vsphere.nono.io:8844 --skip-tls-validation # assuming your BOSH Director\u0026#39;s hostname is \u0026#34;bosh-vsphere.nono.io\u0026#34; credhub login --username=credhub_cli_user --password=SomeKrazyPassword credhub find -n / # we don\u0026#39;t need this, but it gives us a complete list of creds credhub get -n /bosh-vsphere/cf/credhub_admin_client_secret # your path might be different, e.g. \u0026#34;/bosh/TAS/credhub_admin_client_secret\u0026#34; id: 1b238b69-72ac-4a73-a124-b67da71da572 name: /bosh-vsphere/cf/credhub_admin_client_secret type: password value: XDBlXvmH7aN3IG2czVfzvitu5dTHIj version_created_at: \u0026#34;2021-11-18T23:25:53Z\u0026#34; Aha! Now in our cats-config.json, set \u0026quot;credhub_secret\u0026quot;: \u0026quot;XDBlXvmH7aN3IG2czVfzvitu5dTHIj\u0026quot;.\nBut we\u0026rsquo;re still not done: we need to create a security group that allows the apps to communicate with Cloud Foundry\u0026rsquo;s CredHub (separate \u0026amp; distinct from the BOSH Director\u0026rsquo;s CredHub) (yes, it\u0026rsquo;s confusing).\ncf create-security-group credhub \u0026lt;(echo \u0026#39;[{\u0026#34;protocol\u0026#34;:\u0026#34;tcp\u0026#34;,\u0026#34;destination\u0026#34;:\u0026#34;10.0.0.0/8\u0026#34;,\u0026#34;ports\u0026#34;:\u0026#34;8443,8844\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;credhub\u0026#34;}]\u0026#39;) cf bind-running-security-group credhub cf bind-staging-security-group credhub Now let\u0026rsquo;s run our tests!\nbin/test -nodes=6 Troubleshooting Docker Failures [Fail] [docker] Docker Application Lifecycle running a docker app with a start command [BeforeEach] retains its start command through starts and stops /home/cunnie/workspace/cf-acceptance-tests/docker/docker_lifecycle.go:53 [Fail] [docker] Docker Application Lifecycle running a docker app without a start command [BeforeEach] handles docker-defined metadata and environment variables correctly /home/cunnie/workspace/cf-acceptance-tests/docker/docker_lifecycle.go:80 [Fail] [docker] Docker Application Lifecycle running a docker app without a start command [BeforeEach] when env vars are set with \u0026#39;cf set-env\u0026#39; prefers the env vars from cf set-env over those in the Dockerfile /home/cunnie/workspace/cf-acceptance-tests/docker/docker_lifecycle.go:80 Means you\u0026rsquo;ve forgotten to cf enable-feature-flag diego_docker.\nTroubleshoot Service Binding Failures (CredHub) [Fail] [credhub] service bindings during staging [BeforeEach] [assisted credhub] has CredHub references in VCAP_SERVICES interpolated /home/cunnie/workspace/cf-acceptance-tests/credhub/service_bindings.go:123 [Fail] [credhub] service bindings during staging [BeforeEach] [non-assisted credhub] still contains CredHub references in VCAP_SERVICES /home/cunnie/workspace/cf-acceptance-tests/credhub/service_bindings.go:123 [Fail] [credhub] service bindings during runtime service bindings to credhub enabled broker [assisted credhub] [BeforeEach] the broker returns credhub-ref in the credentials block /home/cunnie/workspace/cf-acceptance-tests/credhub/service_bindings.go:123 [Fail] [credhub] service bindings during runtime service bindings to credhub enabled broker [assisted credhub] [BeforeEach] the bound app gets CredHub refs in VCAP_SERVICES interpolated /home/cunnie/workspace/cf-acceptance-tests/credhub/service_bindings.go:123 Means you\u0026rsquo;ve either set the wrong credhub_secret in cats-config.json or you forgot to create \u0026amp; bind the credhub security group.\n","permalink":"https://blog.nono.io/post/underground_guide_to_cf_acceptance/","summary":"The Cloud Foundry Acceptance Tests are the gold standard to test the proper functioning of your Cloud Foundry deployment. This guide tells you how to run them. When in doubt, refer to the README.\nQuick Start cd ~/workspace/ git clone git@github.com:cloudfoundry/cf-acceptance-tests.git cd cf-acceptance-tests . ./.envrc bin/update_submodules cp example-cats-config.json cats-config.json export CONFIG=cats-config.json cf api api.cf.nono.io # or whatever your Cloud Foundry\u0026#39;s API endpoint is cf login -u admin cf create-space -o system system # don\u0026#39;t worry if it\u0026#39;s already created cf t -o system -s system cf enable-feature-flag diego_docker # necessary if you\u0026#39;re running the Docker tests (`\u0026#34;include_docker\u0026#34;: true`) Let\u0026rsquo;s configure our cats-config.","title":"The Underground Guide to Cloud Foundry Acceptance Tests"},{"content":" Recreating the Cluster We want to recreate our cluster while preserving our Vault and Concourse data (we want to recreate our GKE regional cluster as a zonal cluster to take advantage of the GKE free tier which saves us $74.40 per month).\nNote: when we say, \u0026ldquo;recreate the cluster\u0026rdquo;, we really mean, \u0026ldquo;recreate the cluster\u0026rdquo;. We destroy the old cluster, including our worker nodes and persistent volumes.\nBackup Vault In the following example, our storage path is /vault/data, but there\u0026rsquo;s a chance that yours is different. If it is, replace occurrences of /vault/data with your storage path:\n# Find the storage path kubectl exec -it vault-0 -n vault -- cat /tmp/storageconfig.hcl # look for storage.path, e.g. \u0026#34;/vault/data\u0026#34; # We need to do this in two steps because Vault\u0026#39;s tar is BusyBox\u0026#39;s, not GNU\u0026#39;s kubectl exec -it -n vault vault-0 -- tar czf /tmp/vault_bkup.tgz /vault/data # We encode it in base64 to avoid \u0026#34;tar: Damaged tar archive\u0026#34; kubectl exec -it -n vault vault-0 -- base64 /tmp/vault_bkup.tgz \u0026gt; ~/Downloads/vault_bkup.tgz.base64 Note: this backup is very specific to our configuration; if your configuration is different (e.g. Consul storage backend, Disaster Recovery Replication enabled), then refer to the Vault documentation.\nCheck that the backup is valid (that the .tar file isn\u0026rsquo;t corrupted):\nbase64 -d \u0026lt; ~/Downloads/vault_bkup.tgz.base64 | tar tvf - Backup Concourse CI\u0026rsquo;s Database Note: the name of our Helm Concourse CI release is \u0026ldquo;ci-nono-io\u0026rdquo; (its URL is https://ci.nono.io). Remember: when you see it, substitute the name of your Helm Concourse CI release. You\u0026rsquo;ll see the release name in pod names (\u0026ldquo;ci-nono-io-postgresql-0\u0026rdquo;), secrets (\u0026ldquo;ci-nono-io-postgresql\u0026rdquo;), etc. Similarly, our Helm Vault release\u0026rsquo;s name is \u0026ldquo;vault\u0026rdquo;. Yours is probably the same.\nNow we move onto backup up Concourse. In the command below, our Concourse CI\u0026rsquo;s PostgreSQL\u0026rsquo;s pod\u0026rsquo;s name is ci-nono-io-postgresql-0. Substitute your pod\u0026rsquo;s name.\n# the postgres user \u0026#34;concourse\u0026#34;\u0026#39;s password is \u0026#34;concourse\u0026#34; echo concourse | \\ kubectl exec -it ci-nono-io-postgresql-0 -- \\ pg_dump -Fc -U concourse concourse \\ \u0026gt; ~/Downloads/concourse.dump Check that the backup is valid. The following command should complete without errors:\npg_restore -l ~/Downloads/concourse.dump Recreate the Cluster We burn our cluster to the ground \u0026amp; recreate it from scratch:\nterraform destroy ... Restore Vault We\u0026rsquo;ve deployed Vault (helm install vault hashicorp/vault ....), now let\u0026rsquo;s restore our old vault\u0026rsquo;s data:\n# Confirm the storage path kubectl exec -it vault-0 -n vault -- cat /tmp/storageconfig.hcl # look for storage.path, e.g. \u0026#34;/vault/data\u0026#34; kubectl exec -it vault-0 -n vault -- sh -c \u0026#34;rm -r /vault/data/*\u0026#34; kubectl exec -it -n vault vault-0 -- \\ sh -c \u0026#34;base64 -d | tar xzvf -\u0026#34; \u0026lt; ~/Downloads/vault_bkup.tgz.base64 At this point we can browse to our vault (ours is https://vault.nono.io) and unseal it. Our original unsealing keys should work. We log in using our root token and browse to make sure our secrets are there.\nRestore Concourse We\u0026rsquo;ve deployed Concourse (helm install ci-nono-io concourse/concourse ...), but haven\u0026rsquo;t logged in or configured any pipelines. The install is pristine. Let\u0026rsquo;s restore the database. First, let\u0026rsquo;s get the postgres database\u0026rsquo;s postgres user\u0026rsquo;s password. Our secret is ci-nono-io-postgresql; substitute yours appropriately in the following command:\nkubectl get secret ci-nono-io-postgresql -o json \\ | jq -r \u0026#39;.data.\u0026#34;postgresql-postgres-password\u0026#34;\u0026#39; \\ | base64 -d In our case, the postgres user\u0026rsquo;s password is \u0026ldquo;uywhz4bUkJ\u0026rdquo;.\nOur PostgreSQL pod\u0026rsquo;s name is ci-nono-io-postgresql-0; substitute yours in the command below:\nkubectl cp ~/Downloads/concourse.dump ci-nono-io-postgresql-0:/tmp/concourse.dump kubectl exec -it ci-nono-io-postgresql-0 -- bash psql --username=postgres # enter the password from above Now let\u0026rsquo;s drop the old database. Note that we must first disallow additional database connections and terminate existing database connections before dropping the database:\nUPDATE pg_database SET datallowconn = false WHERE datname = \u0026#39;concourse\u0026#39;; SELECT pg_terminate_backend (pid) FROM pg_stat_activity WHERE pg_stat_activity.datname = \u0026#39;concourse\u0026#39;; DROP DATABASE concourse; CREATE DATABASE concourse; EXIT; Use the postgres password again when prompted below:\npg_restore --dbname=concourse --username=postgres /tmp/concourse.dump Now browse to your Concourse CI. Try logging in. Try kicking off a build. If your resources are having trouble, yielding messages such as \u0026ldquo;run check: find or create container on worker \u0026hellip; disappeared from worker\u0026rdquo;, wait a few minutes. It should clear up on its own.\nTroubleshooting If you uninstall Concourse CI, helm uninstall ..., remember to delete any lingering Persistent Volume Claims (pvc), kubectl delete pvc ..., before reinstalling, lest your postgresql-postgres-password secret becomes out-of-sync with the actual password.\nIf, when triggering a Concourse job that depends on a Vault secret, the job aborts with the error failed to interpolate task config: undefined vars:, then you probably forgot to --set secrets.vaultAuthParam=... when helm install ci-nono-io concourse/concourse .... Fix by running helm upgrade ... --set secrets.vaultAuthParam=...\nReferences How to Backup PostgreSQL Database for Concourse CI. We have mixed feelings about this post: on one hand, they do a nice description of backing up the Concourse CI database; on the other hand, they never bother restoring the database to make sure their procedure is correct (spoiler: it isn\u0026rsquo;t). ","permalink":"https://blog.nono.io/post/concourse_on_k8s-6/","summary":"Recreating the Cluster We want to recreate our cluster while preserving our Vault and Concourse data (we want to recreate our GKE regional cluster as a zonal cluster to take advantage of the GKE free tier which saves us $74.40 per month).\nNote: when we say, \u0026ldquo;recreate the cluster\u0026rdquo;, we really mean, \u0026ldquo;recreate the cluster\u0026rdquo;. We destroy the old cluster, including our worker nodes and persistent volumes.\nBackup Vault In the following example, our storage path is /vault/data, but there\u0026rsquo;s a chance that yours is different.","title":"Concourse CI on Kubernetes (GKE), Part 6: Concourse \u0026 Vault: Backup \u0026 Restore"},{"content":"Is it worth switching your VMware vSphere VM\u0026rsquo;s SCSI (small computer system interface) from the LSI Logic Parallel controller to the VMware Paravirtual SCSI controller? Except for ultra-high-end database servers (\u0026gt; 1M IOPS ( input/output operations per second)), the answer is \u0026ldquo;no\u0026rdquo;; the difference is negligible.\nOur benchmarks show that VMware\u0026rsquo;s Paravirtual SCSI (small computer system interface) controller offered a 2-3% performance increase in IOPS (I/O (input/output) operations per second) over the LSI Logic Parallel SCSI controller at the cost of a similar decrease in sequential performance (both read \u0026amp; write). Additionally the Paravirtual SCSI controller (pvscsi) had a slight reduction in CPU (central processing unit) usage on the host (best-case scenario is 3% lower CPU usage).\nThe Benchmarks The Paravirtual has better IOPS than the LSI Logic, but worse sequential throughput.\nOn average the Paravirtual Driver\u0026rsquo;s (red) IOPS performance is 2.5% better than the LSI Logic\u0026rsquo;s\nAlthough not consistently faster, the LSI Logic averages 2% faster than the Paravirtual on sequential read operations\nThe LSI Logic\u0026rsquo;s sequential write performance is consistently faster than the Paravirtual\u0026rsquo;s by an average of 4%\nHost CPU Utilization The ESXi host CPU utilization was trickier to measure—we had to eyeball it. You can see the two charts below, taken while we were running our benchmarks. Our guess is that the Paravirtual driver used ~3% less CPU than the LSI Logic.\nNote that 3% is a best-case scenario (you\u0026rsquo;re unlikely to get 10% improvement): the benchmarks were taken on what we affectionately refer to as \u0026ldquo;The world\u0026rsquo;s slowest Xeon\u0026rdquo;, i.e. the Intel Xeon D-1537, which clocks in at an anemic 1.7GHz (side note: We purchased it for its low TDP (Thermal Design Power), not its speed). In other words, this processor is so slow that any improvement in CPU efficiency is readily apparent.\nBenchmark Setup We paired a slow CPU with a fast disk:\nSamsung SSD 960 2TB M.2 2280 PRO Supermicro X10SDV-8C-TLN4F+ motherboard with a soldered-on 1.7 GHz 8-core Intel Xeon Processor D-1537, and 128 GiB RAM. We used gobonniego to benchmark the performance.\nWe ran each benchmark for 1 hour.\nWe configured the VMs with 4 vCPUs, 1 GiB RAM and a 20 GiB drive. We used 4 CPUs so that the Xeon\u0026rsquo;s slow speed wouldn\u0026rsquo;t handicap the benchmark results (we wanted the disk to be the bottleneck, not the CPU).\nThe Samsung NVMe SSD 960 Pro is a gold standard for NVMe disk performance.\nWhy These Benchmarks Are Flawed In the spirit of full disclosure, we\u0026rsquo;d like to point out the shortcomings of our benchmarks:\nWe only tested one type of datastore. We tested an NVMe (non-volatile memory express) datastore, but it would have been interesting to test VMware\u0026rsquo;s vSAN (virtual storage area network).\nWe only ran the benchmark for an hour. Had we run the benchmark longer, we would have seen different performance curves. For example, when we ran the benchmarks back-to-back we saw degradation of the throughput from almost 2 GB/s to slightly more than 1 GB/s on the sequential read \u0026amp; write tests. We attribute this degradation to saturation of the Samsung controller.\nWe never modified the kernel settings to enhance pvscsi performance. For example, went with the default settings for queue depth for device (64) and adapter (254), but this VMware knowledgebase (KB) article suggests increasing those to 254 and 1024, respectively.\nWe only tested Linux. Windows performance may be different.\nWe benchmarked slightly different versions of Linux. We tested against two very-close versions of Ubuntu Bionic, but they weren\u0026rsquo;t the exact same version. Had we used completely identical versions, we may have seen different performance numbers.\nReferences Which vSCSI controller should I choose for performance?, Mark Achetemichuk, \u0026ldquo;PVSCSI and LSI Logic Parallel/SAS are essentially the same when it comes to overall performance capability [for customers not producing 1 million IOPS]\u0026rdquo;\nAchieving a Million I/O Operations per Second from a Single VMware vSphere® 5.0 Host, \u0026ldquo;a PVSCSI adapter provides 8% better throughput at 10% lower CPU cost\u0026rdquo;. Our benchmarks show 2.5%, 3% respectively.\nLarge-scale workloads with intensive I/O patterns might require queue depths significantly greater than Paravirtual SCSI default values (2053145), \u0026ldquo;\u0026hellip; increase PVSCSI queue depths to 254 (for device) and 1024 (for adapter)\u0026rdquo;\nRaw Benchmark results (JSON-formatted).\n","permalink":"https://blog.nono.io/post/pvscsi/","summary":"Is it worth switching your VMware vSphere VM\u0026rsquo;s SCSI (small computer system interface) from the LSI Logic Parallel controller to the VMware Paravirtual SCSI controller? Except for ultra-high-end database servers (\u0026gt; 1M IOPS ( input/output operations per second)), the answer is \u0026ldquo;no\u0026rdquo;; the difference is negligible.\nOur benchmarks show that VMware\u0026rsquo;s Paravirtual SCSI (small computer system interface) controller offered a 2-3% performance increase in IOPS (I/O (input/output) operations per second) over the LSI Logic Parallel SCSI controller at the cost of a similar decrease in sequential performance (both read \u0026amp; write).","title":"Disk Controller Benchmarks: VMware Paravirtual's vs. LSI Logic Parallel's"},{"content":" In our previous post, we configured our GKE Concourse CI server, which was the capstone of the series. But we were wrong: this post is the capstone in the series. In this post, we install Vault and configure our Concourse CI server to use Vault to retrieve secrets.\nInstallation Most of these instructions are derived from the Hashicorp tutorial, Vault on Kubernetes Deployment Guide.\nCreate a DNS A record which points to the IP address of your GKE load balancer. In our case, we created vault.nono.io which points to 104.155.144.4.\nCreate the vault namespace and deploy the TLS issuers to that namespace. Replace brian.cunnie@gmail.com with your email address:\nkubectl create namespace vault kubectl apply -f \u0026lt;( curl -o- https://cert-manager.io/docs/tutorials/acme/example/staging-issuer.yaml | sed \u0026#39;s/user@example.com/brian.cunnie@gmail.com/\u0026#39;) -n vault kubectl apply -f \u0026lt;( curl -o- https://cert-manager.io/docs/tutorials/acme/example/production-issuer.yaml | sed \u0026#39;s/user@example.com/brian.cunnie@gmail.com/\u0026#39;) -n vault Let\u0026rsquo;s create vault-values.yml, which contains the necessary customizations for our Vault server. Replace vault.nono.io with your DNS record:\ninjector: enabled: false server: ingress: enabled: true labels: traffic: external annotations: cert-manager.io/issuer: \u0026#34;letsencrypt-prod\u0026#34; kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; hosts: - host: vault.nono.io paths: [] tls: - hosts: - vault.nono.io secretName: vault.nono.io ui: enabled: true Let\u0026rsquo;s use Helm to install Vault:\nhelm repo add hashicorp https://helm.releases.hashicorp.com helm install vault hashicorp/vault \\ --namespace vault \\ -f vault-values.yml \\ --wait Let\u0026rsquo;s initialize our pristine Vault server. We want only one key (\u0026quot;secret_shares\u0026quot;: 1) to unseal the vault; we\u0026rsquo;re not a nuclear missile silo that needs three separate keys to trigger a launch.\ncurl \\ --request POST \\ --data \u0026#39;{\u0026#34;secret_shares\u0026#34;: 1, \u0026#34;secret_threshold\u0026#34;: 1}\u0026#39; \\ https://vault.nono.io/v1/sys/init | jq Record root_token and keys; you\u0026rsquo;ll need them to unseal the Vault and log in, respectively. Replace VAULT_KEY and VAULT_TOKEN with your keys and root_token:\nexport VAULT_KEY=5a302397xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx export VAULT_TOKEN=s.QmByxxxxxxxxxxxxxxxxxxxx export VAULT_ADDR=https://vault.nono.io curl \\ --request POST \\ --data \u0026#39;{\u0026#34;key\u0026#34;: \u0026#34;\u0026#39;$VAULT_KEY\u0026#39;\u0026#34;}\u0026#39; \\ $VAULT_ADDR/v1/sys/unseal | jq # check initialization status curl $VAULT_ADDR/v1/sys/init Concourse Integration Much of this section was shamelessly copied from the excellent canonical Concourse documentation, The Vault credential manager.\nConfigure the Secrets Engine Create a key-value concourse/ path in Vault for Concourse to access its secrets:\nvault secrets enable -version=1 -path=concourse kv Create concourse-policy.hcl so that our Concourse server has access to that path:\npath \u0026#34;concourse/*\u0026#34; { policy = \u0026#34;read\u0026#34; } Let\u0026rsquo;s upload that policy to Vault:\nvault policy write concourse concourse-policy.hcl Configure a Vault approle for Concourse Let\u0026rsquo;s enable the approle backend on Vault:\nvault auth enable approle Let\u0026rsquo;s create the Concourse approle:\nvault write auth/approle/role/concourse policies=concourse period=1h We need the approle\u0026rsquo;s role_id and secret_id to set in our Concourse server:\nvault read auth/approle/role/concourse/role-id # role_id 045e3a37-6cc4-4f6b-4312-36eed80f7adc vault write -f auth/approle/role/concourse/secret-id # secret_id 59b8015d-8d4a-fcce-f689-xxxxxxxxxxxx Configure Concourse to Use Vault Let\u0026rsquo;s configure our Concourse deployment to use Vault. We append yet even more arguments to our already-gargantuan [values_file] command to deploy Concourse. Replace gke-nono-io with the name of your Helm release, gke.nono.io with the hostname of your Concourse server, and replace all the various and sundry credentials, too:\nhelm upgrade gke-nono-io concourse/concourse \\ --set concourse.web.externalUrl=https://gke.nono.io \\ --set concourse.web.auth.duration=240h \\ --set \u0026#39;web.ingress.enabled=true\u0026#39; \\ --set \u0026#39;web.ingress.annotations.cert-manager\\.io/issuer=letsencrypt-prod\u0026#39; \\ --set \u0026#39;web.ingress.annotations.kubernetes\\.io/ingress.class=nginx\u0026#39; \\ --set \u0026#39;web.ingress.hosts={gke.nono.io}\u0026#39; \\ --set \u0026#39;web.ingress.tls[0].hosts[0]=gke.nono.io\u0026#39; \\ --set \u0026#39;web.ingress.tls[0].secretName=gke.nono.io\u0026#39; \\ \\ --set-file secrets.sessionSigningKey=secrets/session_signing_key \\ --set-file secrets.hostKey=secrets/tsa_host_key \\ --set-file secrets.hostKeyPub=secrets/tsa_host_key.pub \\ --set-file secrets.workerKey=secrets/worker_key \\ --set-file secrets.workerKeyPub=secrets/worker_key.pub \\ \\ --set secrets.localUsers=\u0026#34;\u0026#34; \\ --set concourse.web.localAuth.enabled=false \\ --set concourse.web.auth.mainTeam.github.org=blabbertabber \\ --set concourse.web.auth.github.enabled=true \\ --set secrets.githubClientId=5e4ffee9dfdced62ebe3 \\ --set secrets.githubClientSecret=549e10b1680ead9cafa30d4c9a715681cec9b074 \\ \\ --set concourse.web.vault.enabled=true \\ --set concourse.web.vault.url=https://vault.nono.io:443 \\ --set concourse.web.vault.authBackend=approle \\ --set concourse.web.vault.useAuthParam=true \\ --set secrets.vaultAuthParam=\u0026#34;role_id:045e3a37-6cc4-4f6b-4312-36eed80f7adc\\\\,secret_id:59b8015d-8d4a-fcce-f689-xxxxxxxxxxxx\u0026#34; \\ \\ --wait Gotchas: you need :443 at the end of the vault URL [443], and you need the double-backslash before the comma in the vaultAuthParam [double_backslash].\nPutting It Together Let\u0026rsquo;s create a secret which we\u0026rsquo;ll interpolate into our pipeline. We create the key under the Vault path concourse/main (main is our Concourse team\u0026rsquo;s name. If you\u0026rsquo;re not sure what your Concourse team name, it\u0026rsquo;s probably main):\nvault kv put concourse/main/ozymandias-secret value=\u0026#34;Look on my Works, ye Mighty, and despair\\!\u0026#34; Let\u0026rsquo;s create a simple Concourse pipeline definition, pipeline-ozymandias.yml\njobs: - name: ozymandias-job plan: - task: ozymandias-task config: platform: linux image_resource: type: docker-image source: repository: fedora run: path: echo args: - \u0026#34;Ozymandias says:\u0026#34; - ((ozymandias-secret)) Let\u0026rsquo;s fly our new pipeline. Replace nono with your Concourse target\u0026rsquo;s name:\nfly -t gke set-pipeline -p ozymandias-pipeline -c pipeline-ozymandias.yml fly -t gke expose-pipeline -p ozymandias-pipeline fly -t gke unpause-pipeline -p ozymandias-pipeline fly -t gke trigger-job -j ozymandias-pipeline/ozymandias-job Let\u0026rsquo;s browse to our job, expand ozymandias-task by clicking on it, and allow ourselves to luxuriate in the sweet smell of success:\nIf you instead see an aborted Concourse job with the error, failed to interpolate task config: undefined vars: ozymandias-secret, you probably have mangled the authentication credentials (vaultAuthParam) setting.\nWhat We Did Wrong Hashicorp warns:\nVault should not be deployed in a public internet facing environment\nWe\u0026rsquo;re doing the exact opposite of what they suggest. If we\u0026rsquo;re going to the trouble of deploying Vault, we want to make sure we can use it from everywhere, security be damned; we don\u0026rsquo;t want to waste our time sprinkling separate Vault deployments like magic fairy dust on each of our environments.\nHashicorp also warns:\nFor a production-ready install, we suggest that the Vault Helm chart is installed in high availability (HA) mode\nWe\u0026rsquo;re installing in standalone mode, not HA mode. We think HA mode is overkill for our use case. Also, our GKE cluster is small because we pay for it out-of-pocket, and we don\u0026rsquo;t have the resources to spend on HA mode.\nAddendum: Updating Here\u0026rsquo;s the correct process for updating:\nUpdate vault. Then unseal the vault. Update Concourse. Addendum: Motivation \u0026ldquo;Why go to all this trouble to install Vault? Wasn\u0026rsquo;t the Concourse server enough?\u0026rdquo; you might ask.\nThe reason we installed Vault was that there were two things we wanted to accomplish that assumed Vault (or another secret-store such as CredHub):\nWe wanted a Concourse pipeline \u0026ldquo;to build and publish a container image\u0026rdquo;. The tutorial\u0026rsquo;s pipelines assumed a secret-store. We didn\u0026rsquo;t have one. We didn\u0026rsquo;t want to be losers that didn\u0026rsquo;t have a secret-store; we wanted to be one of the cool kids.\nWe wanted a Concourse pipeline that used Platform Automation to deploy a Tanzu Ops Manager. We\u0026rsquo;re part of the development team that supports Ops Manager, and we like having our own testbed (instead of the corporate ones) to develop/troubleshoot. Note: there\u0026rsquo;s nothing wrong with the corporate testbeds—they\u0026rsquo;re pretty awesome—but we like having our own personal testbed, as well as our own personal Concourse CI server.\n\u0026ldquo;Why use Vault instead of CredHub? Isn\u0026rsquo;t CredHub is more tightly integrated into the Cloud Foundry ecosystem?\u0026rdquo; you might also ask. The answer is simple: we like a GUI. Yes, we know that using the GUI not as macho as using the CLI, but so what? A well-designed GUI can present data in a more digestible fashion than a CLI.\nAlso, we wanted to learn how to use Vault, and this was a good opportunity. Besides, Vault was written in Golang, which is more fashionable than CredHub\u0026rsquo;s Java (and starts more quickly, too).\nUpdates/Errata 2022-07-04 Added the order in which to apply Vault \u0026amp; Concourse updates.\nFootnotes values_file\nAre you sick of the gargantuan command to deploy Concourse? Then do what we do—use a Helm values file (e.g. concourse-values.yml). You can see ours here. With that file our command to deploy Concourse becomes manageably smaller:\nhelm upgrade gke.nono.io concourse/concourse \\ -f concourse-values.yml \\ ... 443\nYou may think, \u0026ldquo;That :443 at the end of https://vault.nono.io:443 is redundant \u0026amp; superfluous; I\u0026rsquo;ll redact that on my version. Any programmer worth his salt knows that the https scheme defaults to port 443.\u0026rdquo;\nWell I got news for you, Sunshine: you need that :443; if you skip it, you\u0026rsquo;ll get the dreaded, \u0026ldquo;failed to interpolate task config: timed out to login to vault\u0026rdquo; error when your Concourse task attempts to interpolate a variable.\nIn some ways it\u0026rsquo;s similar to the openssl s_client -connect vault.nono.io:443 command which insists that you specify the port even though 99% of the time that port is going to be 443. What, you\u0026rsquo;re not familiar with the openssl s_client -connect command? Well stick around, it\u0026rsquo;ll come in useful when you have to debug certs on someone else\u0026rsquo;s server.\ndouble_backslash\nYou need the double backslash before the comma; if you don\u0026rsquo;t have it, the following error will rear its ugly head when you attempt helm upgrade:\nError: INSTALLATION FAILED: failed parsing --set data: key \u0026#34;secret_id:59b8015d-8d4a-fcce-f689-xxxxxxxxxxxx\u0026#34; has no value ","permalink":"https://blog.nono.io/post/concourse_on_k8s-5/","summary":"In our previous post, we configured our GKE Concourse CI server, which was the capstone of the series. But we were wrong: this post is the capstone in the series. In this post, we install Vault and configure our Concourse CI server to use Vault to retrieve secrets.\nInstallation Most of these instructions are derived from the Hashicorp tutorial, Vault on Kubernetes Deployment Guide.\nCreate a DNS A record which points to the IP address of your GKE load balancer.","title":"Concourse CI on Kubernetes (GKE), Part 5: Vault"},{"content":" In our previous post, we configured our GKE (Google Kubernetes Engine) to use Let\u0026rsquo;s Encrypt TLS certificates. In this post, the capstone of our series, we install Concourse CI.\nInstallation These instructions are a more-opinionated version of the canonical instructions for the Concourse CI Helm chart found here: https://github.com/concourse/concourse-chart.\nFirst Install: with Helm We use helm to install Concourse. We first add the Helm repo, and then install it. We take the opportunity to bump the default login time from 24 hours to ten days (duration=240h) because we hate re-authenticating to our Concourse every morning. Replace gke.nono.io with your DNS record:\nkubectl delete ingress kuard # to free up https://gke.nono.io helm repo add concourse https://concourse-charts.storage.googleapis.com/ helm install gke-nono-io concourse/concourse \\ --set concourse.web.externalUrl=https://gke.nono.io \\ --set concourse.web.auth.duration=240h \\ --set \u0026#39;web.ingress.enabled=true\u0026#39; \\ --set \u0026#39;web.ingress.annotations.cert-manager\\.io/issuer=letsencrypt-prod\u0026#39; \\ --set \u0026#39;web.ingress.annotations.kubernetes\\.io/ingress.class=nginx\u0026#39; \\ --set \u0026#39;web.ingress.hosts={gke.nono.io}\u0026#39; \\ --set \u0026#39;web.ingress.tls[0].hosts[0]=gke.nono.io\u0026#39; \\ --set \u0026#39;web.ingress.tls[0].secretName=gke.nono.io\u0026#39; \\ \\ --wait Browse to our site https://gke.nono.io. You\u0026rsquo;ll see a secure connection icon \u0026amp; the initial Concourse CI login page.\nFirst Upgrade: Locking Down Concourse Our Concourse is insecure: we haven\u0026rsquo;t changed the default private keys. Our Concourse is public-facing, and we must change the keys lest evildoers compromise us. The Concourse README warns:\nFor your convenience, this chart provides some default values for secrets, but it is recommended that you generate and manage these secrets outside the Helm chart.\nLet\u0026rsquo;s make our keys. The Concourse documentation provides two excellent ways to do it, and we\u0026rsquo;ve modified one of the ways to suit our setup. Replace gke.nono.io with your DNS record:\nmkdir -p secrets/ for KEY in session_signing_key tsa_host_key worker_key; do ssh-keygen -t rsa -b 4096 -m PEM -f secrets/$KEY -C $KEY \u0026lt; /dev/null done rm secrets/session_signing_key.pub # \u0026#34;You can remove the session_signing_key.pub file if you have one, it is not needed by any process in Concourse\u0026#34; Let\u0026rsquo;s re-deploy our Concourse with our newly-generated secrets. Replace gke.nono.io with your DNS record:\nhelm upgrade gke-nono-io concourse/concourse \\ --set concourse.web.externalUrl=https://gke.nono.io \\ --set concourse.web.auth.duration=240h \\ --set \u0026#39;web.ingress.enabled=true\u0026#39; \\ --set \u0026#39;web.ingress.annotations.cert-manager\\.io/issuer=letsencrypt-prod\u0026#39; \\ --set \u0026#39;web.ingress.annotations.kubernetes\\.io/ingress.class=nginx\u0026#39; \\ --set \u0026#39;web.ingress.hosts={gke.nono.io}\u0026#39; \\ --set \u0026#39;web.ingress.tls[0].hosts[0]=gke.nono.io\u0026#39; \\ --set \u0026#39;web.ingress.tls[0].secretName=gke.nono.io\u0026#39; \\ \\ --set-file secrets.sessionSigningKey=secrets/session_signing_key \\ --set-file secrets.hostKey=secrets/tsa_host_key \\ --set-file secrets.hostKeyPub=secrets/tsa_host_key.pub \\ --set-file secrets.workerKey=secrets/worker_key \\ --set-file secrets.workerKeyPub=secrets/worker_key.pub \\ \\ --wait Third Upgrade: now with GitHub OAuth We have a Concourse CI server, and we\u0026rsquo;ve generated our own keys, but we\u0026rsquo;re still not secure: people can log in with the user \u0026ldquo;test\u0026rdquo; using the password \u0026ldquo;test\u0026rdquo;. Yes, really.\nWe don\u0026rsquo;t want to have any hard-coded users; we want to authenticate against our GitHub organization, \u0026ldquo;blabbertabber\u0026rdquo;, so we browse to our organization (https://github.com/blabbertabber) → Settings → Developer Settings → OAuth Apps → New OAuth App.\nNote: \u0026ldquo;Note that the client must be created under an organization if you want to authorize users based on organization/team membership.\u0026rdquo;\nHere\u0026rsquo;s how we filled out ours. Replace gke.nono.io with your URL. The authorization callback URL is particularly important; don\u0026rsquo;t mess it up:\nWe click \u0026ldquo;Register Application\u0026rdquo;, which brings us to the next screen, where we get the Client ID (5e4ffee9dfdced62ebe3) and then click \u0026ldquo;Generate a new client secret\u0026rdquo; to get the Client secret (549e10b1680ead9cafa30d4c9a715681cec9b074). Don\u0026rsquo;t forget to click \u0026ldquo;Update Application\u0026rdquo;!\nNow we can add the five GitHub OAuth-related lines to our helm upgrade command. Replace the GitHub org blabbertabber, the GitHub Client ID and Client Secret with the ones you\u0026rsquo;ve created, gke.nono.io with your DNS record:\nWhile we\u0026rsquo;re locking things down, we also remove the local user \u0026ldquo;test\u0026rdquo; (along with the easy-to-guess password, \u0026ldquo;test\u0026rdquo;). We do this by setting secrets.localUsers to \u0026ldquo;\u0026rdquo;. To be safe, we also disable local auth (we set concourse.web.localAuth.enabled to false).\nhelm upgrade gke-nono-io concourse/concourse \\ --set concourse.web.externalUrl=https://gke.nono.io \\ --set concourse.web.auth.duration=240h \\ --set \u0026#39;web.ingress.enabled=true\u0026#39; \\ --set \u0026#39;web.ingress.annotations.cert-manager\\.io/issuer=letsencrypt-prod\u0026#39; \\ --set \u0026#39;web.ingress.annotations.kubernetes\\.io/ingress.class=nginx\u0026#39; \\ --set \u0026#39;web.ingress.hosts={gke.nono.io}\u0026#39; \\ --set \u0026#39;web.ingress.tls[0].hosts[0]=gke.nono.io\u0026#39; \\ --set \u0026#39;web.ingress.tls[0].secretName=gke.nono.io\u0026#39; \\ \\ --set-file secrets.sessionSigningKey=secrets/session_signing_key \\ --set-file secrets.hostKey=secrets/tsa_host_key \\ --set-file secrets.hostKeyPub=secrets/tsa_host_key.pub \\ --set-file secrets.workerKey=secrets/worker_key \\ --set-file secrets.workerKeyPub=secrets/worker_key.pub \\ \\ --set secrets.localUsers=\u0026#34;\u0026#34; \\ --set concourse.web.localAuth.enabled=false \\ --set concourse.web.auth.mainTeam.github.org=blabbertabber \\ --set concourse.web.auth.github.enabled=true \\ --set secrets.githubClientId=5e4ffee9dfdced62ebe3 \\ --set secrets.githubClientSecret=549e10b1680ead9cafa30d4c9a715681cec9b074 \\ \\ --wait Browse to our URL: https://gke.nono.io. We log in with GitHub Auth. We authorize our app. We download \u0026amp; install our fly CLI. Then we log in. Replace gke.nono.io with your DNS record:\nfly -t gke login -c https://gke.nono.io # click the link # click \u0026#34;Authorize blabbertabber\u0026#34; # see \u0026#34;login successful!\u0026#34; We create the following simple pipeline file, simple.yml:\njobs: - name: simple plan: - task: simple config: platform: linux image_resource: type: docker-image source: repository: fedora run: path: \u0026#34;true\u0026#34; Let\u0026rsquo;s fly our new pipeline:\nfly -t gke set-pipeline -p simple -c simple.yml fly -t gke expose-pipeline -p simple fly -t gke unpause-pipeline -p simple We browse to our Concourse and see the sweet green of success (it\u0026rsquo;ll take a minute or two to run):\nYay! We\u0026rsquo;re done.\nPro-tip Rather than having an onerous number of --set arguments to our helm upgrade command, we find it easier to modify the corresponding settings in the values.yml file and pass it to our invocation of helm, i.e. helm upgrade -f values.yml .... Here\u0026rsquo;s our file of overrides.\nAddendum: Keeping Concourse Up-to-date Blindly upgrading Concourse without reading the release notes is a recipe for disaster; however, that\u0026rsquo;s what we\u0026rsquo;re going to show you. Let\u0026rsquo;s update the Helm repos first.\nhelm repo update Now let\u0026rsquo;s upgrade our install. Replace gke.nono.io with your DNS record:\nhelm upgrade gke-nono-io concourse/concourse \\ --set concourse.web.externalUrl=https://gke.nono.io \\ --set concourse.web.auth.duration=240h \\ --set \u0026#39;web.ingress.enabled=true\u0026#39; \\ --set \u0026#39;web.ingress.annotations.cert-manager\\.io/issuer=letsencrypt-prod\u0026#39; \\ --set \u0026#39;web.ingress.annotations.kubernetes\\.io/ingress.class=nginx\u0026#39; \\ --set \u0026#39;web.ingress.hosts={gke.nono.io}\u0026#39; \\ --set \u0026#39;web.ingress.tls[0].hosts[0]=gke.nono.io\u0026#39; \\ --set \u0026#39;web.ingress.tls[0].secretName=gke.nono.io\u0026#39; \\ \\ --set-file secrets.sessionSigningKey=secrets/session_signing_key \\ --set-file secrets.hostKey=secrets/tsa_host_key \\ --set-file secrets.hostKeyPub=secrets/tsa_host_key.pub \\ --set-file secrets.workerKey=secrets/worker_key \\ --set-file secrets.workerKeyPub=secrets/worker_key.pub \\ \\ --set secrets.localUsers=\u0026#34;\u0026#34; \\ --set concourse.web.localAuth.enabled=false \\ --set concourse.web.auth.mainTeam.github.org=blabbertabber \\ --set concourse.web.auth.github.enabled=true \\ --set secrets.githubClientId=5e4ffee9dfdced62ebe3 \\ --set secrets.githubClientSecret=549e10b1680ead9cafa30d4c9a715681cec9b074 \\ \\ --wait Browse to your Concourse server, and check that it has the updated version number.\nReferences Concourse CI Helm chart: https://github.com/concourse/concourse-chart Helm Chart Install: Advanced Usage of the “Set” Argument: https://itnext.io/helm-chart-install-advanced-usage-of-the-set-argument-3e214b69c87a Creating an OAuth App on GitHub: https://docs.github.com/en/developers/apps/building-oauth-apps/creating-an-oauth-app Updates/Errata 2021-11-13 Added section on keeping Concourse up-to-date.\n2021-11-14 Added section on locking down Concourse.\n","permalink":"https://blog.nono.io/post/concourse_on_k8s-4/","summary":"In our previous post, we configured our GKE (Google Kubernetes Engine) to use Let\u0026rsquo;s Encrypt TLS certificates. In this post, the capstone of our series, we install Concourse CI.\nInstallation These instructions are a more-opinionated version of the canonical instructions for the Concourse CI Helm chart found here: https://github.com/concourse/concourse-chart.\nFirst Install: with Helm We use helm to install Concourse. We first add the Helm repo, and then install it. We take the opportunity to bump the default login time from 24 hours to ten days (duration=240h) because we hate re-authenticating to our Concourse every morning.","title":"Concourse CI on Kubernetes (GKE), Part 4: Concourse"},{"content":" In our previous blog post, we configured ingress to our Kubernetes cluster but were disappointed to discover that the TLS certificates were self-signed. In this post we\u0026rsquo;ll remedy that by installing cert-manager, the Cloud native certificate management tool.\nDisclaimer: most of this blog post was lifted whole cloth from the most-excellent cert-manager documentation. We merely condensed it \u0026amp; made it more opinionated.\nInstallation Let\u0026rsquo;s add the Jetstack Helm Repository:\nhelm repo add jetstack https://charts.jetstack.io helm repo update Let\u0026rsquo;s install cert-manager:\nhelm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.6.1 \\ --set installCRDs=true Verifying Installation Do we see all three pods?\nkubectl get pods --namespace cert-manager Now let\u0026rsquo;s create an issuer to test the webhook:\ncat \u0026lt;\u0026lt;EOF \u0026gt; test-resources.yaml apiVersion: v1 kind: Namespace metadata: name: cert-manager-test --- apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: test-selfsigned namespace: cert-manager-test spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: selfsigned-cert namespace: cert-manager-test spec: dnsNames: - example.com secretName: selfsigned-cert-tls issuerRef: name: test-selfsigned EOF And now let\u0026rsquo;s apply those resources:\nkubectl apply -f test-resources.yaml sleep 10 kubectl describe certificate -n cert-manager-test | grep \u0026#34;has been successfully\u0026#34; kubectl delete -f test-resources.yaml 4. Deploy an Example Service Let\u0026rsquo;s install the sample services to test the controller:\nkubectl apply -f https://netlify.cert-manager.io/docs/tutorials/acme/example/deployment.yaml kubectl apply -f https://netlify.cert-manager.io/docs/tutorials/acme/example/service.yaml Let\u0026rsquo;s download and edit the Ingress (we\u0026rsquo;ve already configured gke.nono.io to point to the GCP/GKE load balancer at 34.135.26.144). Replace gke.nono.io with the DNS record of your load balancer set up in the previous blog post:\ncurl -o ingress-kuard.yml -L https://netlify.cert-manager.io/docs/tutorials/acme/example/ingress.yaml sed -i \u0026#39;\u0026#39; \u0026#34;s/example.example.com/gke.nono.io/g\u0026#34; ingress-kuard.yml kubectl apply -f ingress-kuard.yml Let\u0026rsquo;s use curl to check the GKE load balancer. Replace gke.nono.io with the DNS record of your load balancer set up in the previous blog post:\ncurl -kivL -H \u0026#39;Host: gke.nono.io\u0026#39; https://gke.nono.io You should see output similar to the following (note that the cert is still self-signed):\n... * Server certificate: * subject: O=Acme Co; CN=Kubernetes Ingress Controller Fake Certificate 6. Configure Let’s Encrypt Issuer Let\u0026rsquo;s deploy the staging \u0026amp; production issuers. Replace brian.cunnie@gmail.com with your email address:\nkubectl apply -f \u0026lt;( curl -o- https://cert-manager.io/docs/tutorials/acme/example/staging-issuer.yaml | sed \u0026#39;s/user@example.com/brian.cunnie@gmail.com/\u0026#39;) kubectl apply -f \u0026lt;( curl -o- https://cert-manager.io/docs/tutorials/acme/example/production-issuer.yaml | sed \u0026#39;s/user@example.com/brian.cunnie@gmail.com/\u0026#39;) # check to make sure they were deployed: kubectl describe issuer letsencrypt-staging kubectl describe issuer letsencrypt-prod 7. Step 7 - Deploy a TLS Ingress Resource Let\u0026rsquo;s deploy the ingress resource using annotations to obtain the certificate. Replace gke.nono.io with the DNS record of your load balancer set up in the previous blog post:\nkubectl apply -f \u0026lt;( curl -o- https://cert-manager.io/docs/tutorials/acme/example/ingress-tls.yaml | sed \u0026#39;s/example.example.com/gke.nono.io/\u0026#39;) kubectl get certificate # takes ~30s to become ready (\u0026#34;READY\u0026#34; == \u0026#34;True\u0026#34;) kubectl describe certificate quickstart-example-tls kubectl describe secret quickstart-example-tls Let\u0026rsquo;s use curl again to check the GKE load balancer\u0026rsquo;s certificate. Replace gke.nono.io with the DNS record of your load balancer set up in the previous blog post:\ncurl -kivL -H \u0026#39;Host: gke.nono.io\u0026#39; https://gke.nono.io You should see output similar to the following:\n... * Server certificate: * subject: CN=gke.nono.io * start date: Sep 1 22:02:55 2021 GMT * expire date: Nov 30 22:02:54 2021 GMT * issuer: C=US; O=(STAGING) Let\u0026#39;s Encrypt; CN=(STAGING) Artificial Apricot R3 Great! We have the staging cert, but that\u0026rsquo;s not quite good enough—we want a real certificate. Let\u0026rsquo;s upgrade to the production certificate. As usual, Replace gke.nono.io with the DNS record of your load balancer set up in the previous blog post:\nkubectl apply -f \u0026lt;( curl -o- https://cert-manager.io/docs/tutorials/acme/example/ingress-tls-final.yaml | sed \u0026#39;s/example.example.com/gke.nono.io/\u0026#39;) kubectl delete secret quickstart-example-tls # triggers the process to get a new certificate kubectl get certificate # takes ~30s to become ready (\u0026#34;READY\u0026#34; == \u0026#34;True\u0026#34;) kubectl describe certificate quickstart-example-tls kubectl describe secret quickstart-example-tls Let\u0026rsquo;s use curl one more time to check the GKE load balancer\u0026rsquo;s certificate. Replace gke.nono.io with the DNS record of your load balancer set up in the previous blog post:\ncurl -kivL -H \u0026#39;Host: gke.nono.io\u0026#39; https://gke.nono.io You should see output similar to the following:\n... * Server certificate: * subject: CN=gke.nono.io * start date: Sep 1 22:11:36 2021 GMT * expire date: Nov 30 22:11:35 2021 GMT * issuer: C=US; O=Let\u0026#39;s Encrypt; CN=R3 * SSL certificate verify ok. And now browse (replacing gke.nono.io with the DNS record of your load balancer): https://gke.nono.io/. Yes, we get an HTTP 503 status, but our certificate is valid!\nStay Tuned! Stay tuned for our next installment, where we install Concourse CI on GKE.\nReferences cert-manager documentation: https://cert-manager.io/docs/ Updates/Errata 2022-01-08 Bumped the cert-manager version 1.6.0 → 1.6.1; fixed scheme (was http; now is https)\n2021-11-13 Bumped the cert-manager version 1.5.0 → 1.6.0\n","permalink":"https://blog.nono.io/post/concourse_on_k8s-3/","summary":"In our previous blog post, we configured ingress to our Kubernetes cluster but were disappointed to discover that the TLS certificates were self-signed. In this post we\u0026rsquo;ll remedy that by installing cert-manager, the Cloud native certificate management tool.\nDisclaimer: most of this blog post was lifted whole cloth from the most-excellent cert-manager documentation. We merely condensed it \u0026amp; made it more opinionated.\nInstallation Let\u0026rsquo;s add the Jetstack Helm Repository:","title":"Concourse CI on Kubernetes (GKE), Part 3: TLS"},{"content":" In our previous blog post, we set up our Kubernetes cluster and deployed a pod running nginx, but the experience was disappointing—we couldn\u0026rsquo;t browse to our pod. Let\u0026rsquo;s fix that by deploying the nginx Ingress controller.\nAcquire the External IP Address (Elastic IP) We\u0026rsquo;ll use the Google Cloud console to acquire the external address [external address] for our load balancer.\nNavigate to VPC network → External IP addresses → Reserve Static Address:\nName: gke-nono-io (or \u0026ldquo;gke-\u0026rdquo; and whatever your domain is, with dashes not dots) Description: Ingress for GKE In our example, we acquire the IP address, 34.135.26.144.\nCreate DNS Record to Point to Acquired IP Address You\u0026rsquo;ll need a DNS domain for this part. In our examples, we use the domain \u0026ldquo;nono.io\u0026rdquo;, so whenever you see \u0026ldquo;nono.io\u0026rdquo;, substitute \u0026amp; replace your domain. Similarly, whenever you see \u0026ldquo;34.135.26.144\u0026rdquo;, substitute your external IP address.\nAdding a DNS record is outside the scope of the humble blog post (we use BIND, but these days services such as AWS\u0026rsquo;s Route 53 are all the rage).\nWe create the DNS address record \u0026ldquo;gke.nono.io\u0026rdquo; to point to \u0026ldquo;34.135.26.144\u0026rdquo;; Let\u0026rsquo;s test to make sure it\u0026rsquo;s set up properly:\ndig gke.nono.io +short # should return 34.135.26.144 Create Kubernetes Ingress nginx Manifest Files We\u0026rsquo;re going to shamelessly copy the canonical Ingress nginx manifest files and modify them to include our static IP address:\n[Much of the following is shamelessly copied from the ingress-nginx docs]\nAssign cluster-admin permissions:\nkubectl create clusterrolebinding cluster-admin-binding \\ --clusterrole cluster-admin \\ --user $(gcloud config get-value account) Let\u0026rsquo;s download our controller manifest and edit it:\ncurl -L https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.0/deploy/static/provider/cloud/deploy.yaml \\ -o nginx-ingress-controller.yml nvim nginx-ingress-controller.yml We need to add our IP address to our load balancer Kubernetes service. search for the string \u0026ldquo;LoadBalancer\u0026rdquo; and add the IP address as shown below (don\u0026rsquo;t include the plus sign \u0026ldquo;+\u0026rdquo; in your file):\nspec: type: LoadBalancer externalTrafficPolicy: Local + loadBalancerIP: 34.135.26.144 ports: - name: http port: 80 Let\u0026rsquo;s apply our changes:\nkubectl apply -f nginx-ingress-controller.yml Let\u0026rsquo;s wait for the change to have completed\nkubectl wait --namespace ingress-nginx \\ --for=condition=ready pod \\ --selector=app.kubernetes.io/component=controller \\ --timeout=120s Let\u0026rsquo;s browse to our endpoint: http://gke.nono.io. We see the nginx \u0026ldquo;404 Not Found\u0026rdquo; status page, but that\u0026rsquo;s reassuring: it means we\u0026rsquo;ve properly set up the nginx controller, but haven\u0026rsquo;t yet set up Ingress to our existing pods.\nBefore we set up Ingress, let\u0026rsquo;s check our HTTPS endpoint: https://gke.nono.io.\nWait, what is this? We\u0026rsquo;re seeing an unsettling message, \u0026ldquo;Warning: Potential Security Risk Ahead\u0026rdquo; (Chrome users may see \u0026ldquo;Your connection is not private\u0026rdquo;; Safari users, \u0026ldquo;This Connection Is Not Private\u0026rdquo;). We\u0026rsquo;re upset—we don\u0026rsquo;t want to be seen as losers who are using self-signed TLS certificates; we want to be winners who are using certificates from Let\u0026rsquo;s Encrypt.\nStay Tuned! Stay tuned for our next installment, where we configure Let\u0026rsquo;s Encrypt certificates for our TLS (Transport Layer Security) endpoints.\nReferences The canonical Kubernetes documentation for deploying the nginx Ingress controller on GKE https://kubernetes.github.io/ingress-nginx/deploy/#gce-gke Footnotes external address\nYou can also acquire the external address via the command line (don\u0026rsquo;t forget to change \u0026ldquo;blabbertabber\u0026rdquo; to your project\u0026rsquo;s name):\ngcloud compute addresses create gke-nono-io --project=blabbertabber --description=Ingress\\ for\\ GKE --region=us-central1 Or, for the truly advanced among you, you can modify your terraform templates to acquire the address for you. The terraform site has great documentation, and here\u0026rsquo;s the snippet you\u0026rsquo;ll need:\nmodule \u0026#34;address-fe\u0026#34; { source = \u0026#34;terraform-google-modules/address/google\u0026#34; version = \u0026#34;0.1.0\u0026#34; names = [ \u0026#34;gke-nono-io\u0026#34;] global = true } Updates/Errata 2022-01-08 Bumped nginx controller 0.48.1 → 1.1.0\n","permalink":"https://blog.nono.io/post/concourse_on_k8s-2/","summary":"In our previous blog post, we set up our Kubernetes cluster and deployed a pod running nginx, but the experience was disappointing—we couldn\u0026rsquo;t browse to our pod. Let\u0026rsquo;s fix that by deploying the nginx Ingress controller.\nAcquire the External IP Address (Elastic IP) We\u0026rsquo;ll use the Google Cloud console to acquire the external address [external address] for our load balancer.\nNavigate to VPC network → External IP addresses → Reserve Static Address:","title":"Concourse CI on Kubernetes (GKE), Part 2: Ingress"},{"content":" Let\u0026rsquo;s deploy Concourse, a continuous-integration, continuous delivery (CI/CD) application (similar to Jenkins and CircleCI).\nWe\u0026rsquo;ll deploy it to Google Cloud, to our Google Kubernetes Engine (GKE).\nIn this post, we\u0026rsquo;ll use HashiCorp\u0026rsquo;s Terraform to create our cluster.\nWe assume you\u0026rsquo;ve already installed the terraform command-line interface (CLI) and created a Google Cloud account.\nmkdir -p ~/workspace/gke cd ~/workspace/gke Next we download the terraform templates and terraform vars file:\ncurl -OL https://raw.githubusercontent.com/cunnie/deployments/6b230118399f4326094b4d60e21cda32e8c6f321/terraform/gcp/gke/gke.tf curl -OL https://raw.githubusercontent.com/cunnie/deployments/6b230118399f4326094b4d60e21cda32e8c6f321/terraform/gcp/gke/vpc.tf curl -OL https://raw.githubusercontent.com/cunnie/deployments/6b230118399f4326094b4d60e21cda32e8c6f321/terraform/gcp/gke/terraform.tfvars curl -OL https://raw.githubusercontent.com/cunnie/deployments/6b230118399f4326094b4d60e21cda32e8c6f321/terraform/gcp/gke/outputs.tf At this point we hear cries of protest, \u0026ldquo;What?! Downloading dubious files from sketchy software developers on the internet? Files whose provenance is murky at best?\u0026rdquo;\nLet us reassure you: the provenance of these files is crystal-clear: they have been patterned after templates from HashiCorp\u0026rsquo;s excellent tutorial, Provision a GKE Cluster (Google Cloud), and the companion git repo, https://github.com/hashicorp/learn-terraform-provision-gke-cluster. [provenance]\nLet\u0026rsquo;s login with gcloud:\ngcloud auth application-default login (if you get a command not found error, then it means you need to install Google Cloud\u0026rsquo;s CLI; the HashiCorp tutorial has great instructions.)\nLet\u0026rsquo;s customize our terraform.tfvars file. At the very least, change the project_id to your Google Cloud\u0026rsquo;s project\u0026rsquo;s ID. If you\u0026rsquo;re not sure what that is, you can find it on the Google console:\nLet\u0026rsquo;s use neovim (or your editor of choice):\nnvim terraform.tfvars Let\u0026rsquo;s change the Project ID to \u0026ldquo;my-google-project-id\u0026rdquo; (assuming that\u0026rsquo;s your Google Project\u0026rsquo;s name, which it isn\u0026rsquo;t):\n-project_id = \u0026#34;blabbertabber\u0026#34; -friendly_project_id = \u0026#34;nono\u0026#34; +project_id = \u0026#34;my-google-project-id\u0026#34; +friendly_project_id = \u0026#34;my-google-project-id\u0026#34; We\u0026rsquo;re ready to terraform!\nterraform init terraform apply # answer \u0026#34;yes\u0026#34; when asked, \u0026#34;Do you want to perform these actions?\u0026#34; The terraform apply takes ~10 minutes to complete. Now let\u0026rsquo;s get our cluster credentials:\ngcloud container clusters get-credentials $(terraform output -raw kubernetes_cluster_name) --zone $(terraform output -raw zone) We have a cluster at this point—let\u0026rsquo;s test by deploying nginx:\nkubectl run nginx --image=nginx kubectl get pods You should see the following output:\nNAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 2m21s Save that terraform.tfstate file! Save the terraform.tfstate file; if you lose it, it becomes much more difficult to make changes to your terraform infrastructure (you\u0026rsquo;ll have to manually tear it down \u0026amp; start from scratch).\nWe won\u0026rsquo;t tell you how or where to save it, but we will tell you that we\u0026rsquo;ve chosen to save ours in a public GitHub repo. This is a bad idea! terraform.tfstate files often contain secrets which you do not want to make public. Ours doesn\u0026rsquo;t appear to contain any secrets, and we like to have it publicly viewable for instructional purposes, but we may have made a terrible mistake by publishing it.\nStay Tuned! Stay tuned for the next installment, where we configure load balancers and install Concourse CI.\nReferences HashiCorp\u0026rsquo;s excellent tutorial, Provision a GKE Cluster (Google Cloud) https://learn.hashicorp.com/terraform/kubernetes/provision-gke-cluster Companion GitHub repository, https://github.com/hashicorp/learn-terraform-provision-gke-cluster https://github.com/cloudfoundry/bosh-community-stemcell-ci-infra is the repo that contains the scripts to spin up Concourse CI on GKE, and may be of interest to those who would like a more automated way of spinning up Concourse on GKE. This repo is actively maintained, and is used by the team that produces the BOSH Bionic stemcells. \u0026ldquo;It also uses config connector instead of Terraform for managing the cloud SQL instance https://cloud.google.com/config-connector/docs/overview.\u0026rdquo; Thanks, Ruben Koster! https://github.com/pivotal-cf/pci-infrastructure/tree/master/k8s \u0026ldquo;sets up a GKE cluster using Terraform, Vault with Google KMS [Key Management Service] integration, Concourse with Vault integration, as well as cert-manager, Kubernetes ingress-nginx and external DNS that hooks into Google Cloud\u0026rsquo;s DNS to create our external IP.\u0026rdquo; Thanks Brian Rieger! https://github.com/skyscrapers/terraform-concourse is a good resource for those interested in deploying to AWS instead of GCP. Thanks Ringo De Smet! How to Download \u0026amp; Install Terraform on Windows, Linux, Mac is a tutorial for installing the terraform CLI. Martyna Łokuciejewska asked me nicely to link to it, so I did. Updates/Errata 2022-03-12 Added a reference to installing the Terraform CLI.\n2022-01-02 Pointed out that the modifications to the terraform configuration enable the creation of a Zonal cluster, which qualifies for the GKE free tier.\n2021-09-30 Added an additional reference for those interested in deploying to AWS.\n2021-09-16 Added two additional references for more complete/more automated ways to spin up Concourse on GKE.\nFootnotes provenance\nThis begs the question, \u0026ldquo;If we\u0026rsquo;re patterning our templates after HashiCorp\u0026rsquo;s, why not use HashiCorp\u0026rsquo;s directly? Why change the templates?\u0026rdquo;\nOur templates are $74.40 per month cheaper than Hashicorp\u0026rsquo;s. Specifically, our templates create a Zonal cluster; Hashicorp\u0026rsquo;s create a Regional cluster. A Zonal cluster qualifies for the GKE free tier:\nThe GKE free tier provides $74.40 in monthly credits per billing account that are applied to zonal and Autopilot clusters. If you only use a single Zonal or Autopilot cluster, this credit will at least cover the complete cost of that cluster each month\nBut if that\u0026rsquo;s not a good reason for you, then by all means use Hashicorp\u0026rsquo;s templates—they\u0026rsquo;re great templates!\nWe\u0026rsquo;ve made other tweaks to the templates as well, for example, we split the templates into a virtual private cloud (VPC) (vpc.tf) template and a Google Kubernetes Engine (gke) (gke.tf) template. It seemed like a good idea at the time. Also, we didn\u0026rsquo;t want to spend a lot of money, so instead of three instances in the region, we modified the template to place two instances in the same availability zone (creating a Zonal cluster).\n[e2-medium instances cost $24.46 / month in the region us-central1. We didn\u0026rsquo;t want to spend the extra $25 for a third instance.]\nFinally, we didn\u0026rsquo;t like the name of our Google Cloud project (\u0026ldquo;blabbertabber\u0026rdquo;): it was too long \u0026amp; referred to a project we had mothballed months ago. We wanted a shorter and friendlier name (\u0026ldquo;nono\u0026rdquo;), and we were loath to create a brand new Google Cloud project, so we modified the templates to include a \u0026ldquo;friendly\u0026rdquo; project name.\n","permalink":"https://blog.nono.io/post/concourse_on_k8s-1/","summary":"Let\u0026rsquo;s deploy Concourse, a continuous-integration, continuous delivery (CI/CD) application (similar to Jenkins and CircleCI).\nWe\u0026rsquo;ll deploy it to Google Cloud, to our Google Kubernetes Engine (GKE).\nIn this post, we\u0026rsquo;ll use HashiCorp\u0026rsquo;s Terraform to create our cluster.\nWe assume you\u0026rsquo;ve already installed the terraform command-line interface (CLI) and created a Google Cloud account.\nmkdir -p ~/workspace/gke cd ~/workspace/gke Next we download the terraform templates and terraform vars file:\ncurl -OL https://raw.","title":"Concourse CI on Kubernetes (GKE), Part 1: Terraform"},{"content":"Why am I creating a new blog? What was wrong with the old blog? Why don\u0026rsquo;t I use Medium?\nThe short version: The old blog is frozen in time, like a prince caught in amber 1 or a dandy in aspic 2. I can no longer post to it.\nThe old blog, the Pivotal Engineering Journal, which many of Pivotal\u0026rsquo;s engineers contributed to, was archived a year after VMware acquired Pivotal. Every acquisition brings changes, and this was, in the scheme of things, a very minor one. At least VMware kept the blog instead of simply discarding it.\nDozens of my blog posts, representing hundreds of hours of work, are now tucked away in an even smaller corner of the internet. That\u0026rsquo;s okay: technical posts have a short shelf life. They have served their purpose.\nBut I cannot lay the fault at the feet of the VMware acquisition; I, too, had a part: My interest in blogging had waned, my output, diminished. In times past I had blogged as frequently as every two months, but then slowed to a crawl: once every six months, and now it has been over a year since I wrote a technical blog post.\nThen the wind shifted, and I again felt the urge to write. Perhaps a VMware corporate blog? Maybe not. I wasn\u0026rsquo;t sure if they\u0026rsquo;d be hands-off, allowing me to choose my topics, express my opinions.\nPerhaps Medium? I discounted that, too, for I find the experience of reading articles on Medium unpleasant, especially the plaintive warning that accompanies every post: \u0026ldquo;You have 2 free member-only stories left this month. Upgrade for unlimited access.\u0026rdquo; I wanted my readers to have unfettered access to my writing.\nFinally, I like writing in Markdown using Neovim: less mouse, more keyboard.\nThe winner? A combination of Hugo and GitHub Pages.\n","permalink":"https://blog.nono.io/post/why_new_blog/","summary":"Why am I creating a new blog? What was wrong with the old blog? Why don\u0026rsquo;t I use Medium?\nThe short version: The old blog is frozen in time, like a prince caught in amber 1 or a dandy in aspic 2. I can no longer post to it.\nThe old blog, the Pivotal Engineering Journal, which many of Pivotal\u0026rsquo;s engineers contributed to, was archived a year after VMware acquired Pivotal.","title":"The Old Blog is Dead. Long Live the New Blog!"}]