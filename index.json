[{"content":"  In our previous blog post, we configured ingress to our Kubernetes cluster but were disappointed to discover that the TLS certificates were self-signed. In this post we\u0026rsquo;ll remedy that by installing cert-manager, the Cloud native certificate management tool.\nDisclaimer: most of this blog post was lifted whole cloth from the most-excellent cert-manager documentation. We merely condensed it \u0026amp; made it more opinionated.\nInstallation Let\u0026rsquo;s add the Jetstack Helm Repository:\nhelm repo add jetstack https://charts.jetstack.io helm repo update Let\u0026rsquo;s install cert-manager:\nhelm install \\  cert-manager jetstack/cert-manager \\  --namespace cert-manager \\  --create-namespace \\  --version v1.5.0 \\  --set installCRDs=true Verifying Installation Do we see all three pods?\nkubectl get pods --namespace cert-manager Now let\u0026rsquo;s create an issuer to test the webhook:\ncat \u0026lt;\u0026lt;EOF \u0026gt; test-resources.yaml apiVersion: v1 kind: Namespace metadata: name: cert-manager-test --- apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: test-selfsigned namespace: cert-manager-test spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: selfsigned-cert namespace: cert-manager-test spec: dnsNames: - example.com secretName: selfsigned-cert-tls issuerRef: name: test-selfsigned EOF And now let\u0026rsquo;s apply those resources:\nkubectl apply -f test-resources.yaml sleep 10 kubectl describe certificate -n cert-manager-test | grep \u0026#34;has been successfully\u0026#34; kubectl delete -f test-resources.yaml 4. Deploy an Example Service Let\u0026rsquo;s install the sample services to test the controller:\nkubectl apply -f https://netlify.cert-manager.io/docs/tutorials/acme/example/deployment.yaml kubectl apply -f https://netlify.cert-manager.io/docs/tutorials/acme/example/service.yaml Let\u0026rsquo;s download and edit the Ingress (we\u0026rsquo;ve already configured gke.nono.io to point to the GCP/GKE load balancer at 34.135.26.144). Replace gke.nono.io with the DNS record of your load balancer set up in the previous blog post:\ncurl -o ingress-kuard.yml -L https://netlify.cert-manager.io/docs/tutorials/acme/example/ingress.yaml sed -i \u0026#39;\u0026#39; \u0026#34;s/example.example.com/gke.nono.io/g\u0026#34; ingress-kuard.yml kubectl apply -f ingress-kuard.yml Let\u0026rsquo;s use curl to check the GKE load balancer. Replace gke.nono.io with the DNS record of your load balancer set up in the previous blog post:\ncurl -kivL -H \u0026#39;Host: gke.nono.io\u0026#39; \u0026#39;http://gke.nono.io\u0026#39; You should see output similar to the following (note that the cert is still self-signed):\n... * Server certificate: * subject: O=Acme Co; CN=Kubernetes Ingress Controller Fake Certificate 6. Configure Let’s Encrypt Issuer Let\u0026rsquo;s deploy the staging \u0026amp; production issuers. Replace brian.cunnie@gmail.com with your email address:\nkubectl apply -f \u0026lt;( curl -o- https://cert-manager.io/docs/tutorials/acme/example/staging-issuer.yaml | sed \u0026#39;s/user@example.com/brian.cunnie@gmail.com/\u0026#39;) kubectl apply -f \u0026lt;( curl -o- https://cert-manager.io/docs/tutorials/acme/example/production-issuer.yaml | sed \u0026#39;s/user@example.com/brian.cunnie@gmail.com/\u0026#39;) # check to make sure they were deployed: kubectl describe issuer letsencrypt-staging kubectl describe issuer letsencrypt-prod 7. Step 7 - Deploy a TLS Ingress Resource Let\u0026rsquo;s deploy the ingress resource using annotations to obtain the certificate. Replace gke.nono.io with the DNS record of your load balancer set up in the previous blog post:\nkubectl apply -f \u0026lt;( curl -o- https://cert-manager.io/docs/tutorials/acme/example/ingress-tls.yaml | sed \u0026#39;s/example.example.com/gke.nono.io/\u0026#39;) kubectl get certificate # takes ~30s to become ready (\u0026#34;READY\u0026#34; == \u0026#34;True\u0026#34;) kubectl describe certificate quickstart-example-tls kubectl describe secret quickstart-example-tls Let\u0026rsquo;s use curl again to check the GKE load balancer\u0026rsquo;s certificate. Replace gke.nono.io with the DNS record of your load balancer set up in the previous blog post:\ncurl -kivL -H \u0026#39;Host: gke.nono.io\u0026#39; \u0026#39;http://gke.nono.io\u0026#39; You should see output similar to the following:\n... * Server certificate: * subject: CN=gke.nono.io * start date: Sep 1 22:02:55 2021 GMT * expire date: Nov 30 22:02:54 2021 GMT * issuer: C=US; O=(STAGING) Let's Encrypt; CN=(STAGING) Artificial Apricot R3 Great! We have the staging cert, but that\u0026rsquo;s not quite good enough—we want a real certificate. Let\u0026rsquo;s upgrade to the production certificate. As usual, Replace gke.nono.io with the DNS record of your load balancer set up in the previous blog post:\nkubectl apply -f \u0026lt;( curl -o- https://cert-manager.io/docs/tutorials/acme/example/ingress-tls-final.yaml | sed \u0026#39;s/example.example.com/gke.nono.io/\u0026#39;) kubectl delete secret quickstart-example-tls # triggers the process to get a new certificate kubectl get certificate # takes ~30s to become ready (\u0026#34;READY\u0026#34; == \u0026#34;True\u0026#34;) kubectl describe certificate quickstart-example-tls kubectl describe secret quickstart-example-tls Let\u0026rsquo;s use curl one more time to check the GKE load balancer\u0026rsquo;s certificate. Replace gke.nono.io with the DNS record of your load balancer set up in the previous blog post:\ncurl -kivL -H \u0026#39;Host: gke.nono.io\u0026#39; \u0026#39;http://gke.nono.io\u0026#39; You should see output similar to the following:\n... * Server certificate: * subject: CN=gke.nono.io * start date: Sep 1 22:11:36 2021 GMT * expire date: Nov 30 22:11:35 2021 GMT * issuer: C=US; O=Let's Encrypt; CN=R3 * SSL certificate verify ok. And now browse (replacing gke.nono.io with the DNS record of your load balancer): https://gke.nono.io/. Yes, we get an HTTP 503 status, but our certificate is valid!\nStay Tuned! Stay tuned for our next installment, where we install Concourse CI on GKE.\n References  cert-manager documentation: https://cert-manager.io/docs/  ","permalink":"https://blog.nono.io/post/concourse_on_k8s-3/","summary":"In our previous blog post, we configured ingress to our Kubernetes cluster but were disappointed to discover that the TLS certificates were self-signed. In this post we\u0026rsquo;ll remedy that by installing cert-manager, the Cloud native certificate management tool.\nDisclaimer: most of this blog post was lifted whole cloth from the most-excellent cert-manager documentation. We merely condensed it \u0026amp; made it more opinionated.\nInstallation Let\u0026rsquo;s add the Jetstack Helm Repository:","title":"Concourse CI on Kubernetes (GKE), Part 3: TLS"},{"content":"  In our previous blog post, we set up our Kubernetes cluster and deployed a pod running nginx, but the experience was disappointing—we couldn\u0026rsquo;t browse to our pod. Let\u0026rsquo;s fix that by deploying the nginx Ingress controller.\nAcquire the External IP Address (Elastic IP) We\u0026rsquo;ll use the Google Cloud console to acquire the external address [external address] for our load balancer.\nNavigate to VPC network → External IP addresses → Reserve Static Address:\n Name: gke-nono-io (or \u0026ldquo;gke-\u0026rdquo; and whatever your domain is, with dashes not dots) Description: Ingress for GKE    In our example, we acquire the IP address, 34.135.26.144.\nCreate DNS Record to Point to Acquired IP Address You\u0026rsquo;ll need a DNS domain for this part. In our examples, we use the domain \u0026ldquo;nono.io\u0026rdquo;, so whenever you see \u0026ldquo;nono.io\u0026rdquo;, substitute \u0026amp; replace your domain. Similarly, whenever you see \u0026ldquo;34.135.26.144\u0026rdquo;, substitute your external IP address.\nAdding a DNS record is outside the scope of the humble blog post (we use BIND, but these days services such as AWS\u0026rsquo;s Route 53 are all the rage).\nWe create the DNS address record \u0026ldquo;gke.nono.io\u0026rdquo; to point to \u0026ldquo;34.135.26.144\u0026rdquo;; Let\u0026rsquo;s test to make sure it\u0026rsquo;s set up properly:\ndig gke.nono.io +short # should return 34.135.26.144 Create Kubernetes Ingress nginx Manifest Files We\u0026rsquo;re going to shamelessly copy the canonical Ingress nginx manifest files and modify them to include our static IP address:\ncurl -L https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.48.1/deploy/static/provider/cloud/deploy.yaml \\  -o nginx-ingress-controller.yml nvim nginx-ingress-controller.yml We need to add our IP address to our load balancer Kubernetes service. search for the string \u0026ldquo;LoadBalancer\u0026rdquo; and add the IP address as shown below (don\u0026rsquo;t include the plus sign \u0026ldquo;+\u0026rdquo; in your file):\nspec: type: LoadBalancer externalTrafficPolicy: Local + loadBalancerIP: 34.135.26.144  ports: - name: http port: 80 Let\u0026rsquo;s apply our changes:\nkubectl apply -f nginx-ingress-controller.yml Let\u0026rsquo;s wait for the change to have completed\nkubectl wait --namespace ingress-nginx \\  --for=condition=ready pod \\  --selector=app.kubernetes.io/component=controller \\  --timeout=120s Let\u0026rsquo;s browse to our endpoint: http://gke.nono.io. We see the nginx \u0026ldquo;404 Not Found\u0026rdquo; status page, but that\u0026rsquo;s reassuring: it means we\u0026rsquo;ve properly set up the nginx controller, but haven\u0026rsquo;t yet set up Ingress to our existing pods.\nBefore we set up Ingress, let\u0026rsquo;s check our HTTPS endpoint: https://gke.nono.io.\nWait, what is this? We\u0026rsquo;re seeing an unsettling message, \u0026ldquo;Warning: Potential Security Risk Ahead\u0026rdquo; (Chrome users may see \u0026ldquo;Your connection is not private\u0026rdquo;; Safari users, \u0026ldquo;This Connection Is Not Private\u0026rdquo;). We\u0026rsquo;re upset—we don\u0026rsquo;t want to be seen as losers who are using self-signed TLS certificates; we want to be winners who are using certificates from Let\u0026rsquo;s Encrypt.\nStay Tuned! Stay tuned for our next installment, where we configure Let\u0026rsquo;s Encrypt certificates for our TLS (Transport Layer Security) endpoints.\n References  The canonical Kubernetes documentation for deploying the nginx Ingress controller on GKE https://kubernetes.github.io/ingress-nginx/deploy/#gce-gke  Footnotes external address\nYou can also acquire the external address via the command line (don\u0026rsquo;t forget to change \u0026ldquo;blabbertabber\u0026rdquo; to your project\u0026rsquo;s name):\ngcloud compute addresses create gke-nono-io --project=blabbertabber --description=Ingress\\ for\\ GKE --region=us-central1 Or, for the truly advanced among you, you can modify your terraform templates to acquire the address for you. The terraform site has great documentation, and here\u0026rsquo;s the snippet you\u0026rsquo;ll need:\nmodule \u0026#34;address-fe\u0026#34; { source = \u0026#34;terraform-google-modules/address/google\u0026#34; version = \u0026#34;0.1.0\u0026#34; names = [ \u0026#34;gke-nono-io\u0026#34;] global = true } ","permalink":"https://blog.nono.io/post/concourse_on_k8s-2/","summary":"In our previous blog post, we set up our Kubernetes cluster and deployed a pod running nginx, but the experience was disappointing—we couldn\u0026rsquo;t browse to our pod. Let\u0026rsquo;s fix that by deploying the nginx Ingress controller.\nAcquire the External IP Address (Elastic IP) We\u0026rsquo;ll use the Google Cloud console to acquire the external address [external address] for our load balancer.\nNavigate to VPC network → External IP addresses → Reserve Static Address:","title":"Concourse CI on Kubernetes (GKE), Part 2: Ingress"},{"content":"  Let\u0026rsquo;s deploy Concourse, a continuous-integration, continuous delivery (CI/CD) application (similar to Jenkins and CircleCI).\nWe\u0026rsquo;ll deploy it to Google Cloud, to our Google Kubernetes Engine (GKE).\nIn this post, we\u0026rsquo;ll use HashiCorp\u0026rsquo;s Terraform to create our cluster.\nWe assume you\u0026rsquo;ve already installed the terraform command-line interface (CLI) and created a Google Cloud account.\nmkdir -p ~/workspace/gke cd ~/workspace/gke Next we download the terraform templates and terraform vars file:\ncurl -OL https://raw.githubusercontent.com/cunnie/deployments/6b230118399f4326094b4d60e21cda32e8c6f321/terraform/gcp/gke/gke.tf curl -OL https://raw.githubusercontent.com/cunnie/deployments/6b230118399f4326094b4d60e21cda32e8c6f321/terraform/gcp/gke/vpc.tf curl -OL https://raw.githubusercontent.com/cunnie/deployments/6b230118399f4326094b4d60e21cda32e8c6f321/terraform/gcp/gke/terraform.tfvars curl -OL https://raw.githubusercontent.com/cunnie/deployments/6b230118399f4326094b4d60e21cda32e8c6f321/terraform/gcp/gke/outputs.tf At this point we hear cries of protest, \u0026ldquo;What?! Downloading dubious files from sketchy software developers on the internet? Files whose provenance is murky at best?\u0026rdquo;\nLet us reassure you: the provenance of these files is crystal-clear: they have been patterned after templates from HashiCorp\u0026rsquo;s excellent tutorial, Provision a GKE Cluster (Google Cloud), and the companion git repo, https://github.com/hashicorp/learn-terraform-provision-gke-cluster. [provenance]\nLet\u0026rsquo;s login with gcloud:\ngcloud auth application-default login (if you get a command not found error, then it means you need to install Google Cloud\u0026rsquo;s CLI; the HashiCorp tutorial has great instructions.)\nLet\u0026rsquo;s customize our terraform.tfvars file. At the very least, change the project_id to your Google Cloud\u0026rsquo;s project\u0026rsquo;s ID. If you\u0026rsquo;re not sure what that is, you can find it on the Google console:\n  Let\u0026rsquo;s use neovim (or your editor of choice):\nnvim terraform.tfvars Let\u0026rsquo;s change the Project ID to \u0026ldquo;my-google-project-id\u0026rdquo; (assuming that\u0026rsquo;s your Google Project\u0026rsquo;s name, which it isn\u0026rsquo;t):\n-project_id = \u0026#34;blabbertabber\u0026#34; -friendly_project_id = \u0026#34;nono\u0026#34; +project_id = \u0026#34;my-google-project-id\u0026#34; +friendly_project_id = \u0026#34;my-google-project-id\u0026#34; We\u0026rsquo;re ready to terraform!\nterraform init terraform apply # answer \u0026#34;yes\u0026#34; when asked, \u0026#34;Do you want to perform these actions?\u0026#34; The terraform apply takes ~10 minutes to complete. Now let\u0026rsquo;s get our cluster credentials:\ngcloud container clusters get-credentials $(terraform output -raw kubernetes_cluster_name) --zone $(terraform output -raw zone) We have a cluster at this point—let\u0026rsquo;s test by deploying nginx:\nkubectl run nginx --image=nginx kubectl get pods You should see the following output:\nNAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 2m21s Save that terraform.tfstate file! Save the terraform.tfstate file; if you lose it, it becomes much more difficult to make changes to your terraform infrastructure (you\u0026rsquo;ll have to manually tear it down \u0026amp; start from scratch).\nWe won\u0026rsquo;t tell you how or where to save it, but we will tell you that we\u0026rsquo;ve chosen to save ours in a public GitHub repo. This is a bad idea! terraform.tfstate files often contain secrets which you do not want to make public. Ours doesn\u0026rsquo;t appear to contain any secrets, and we like to have it publicly viewable for instructional purposes, but we may have made a terrible mistake by publishing it.\nStay Tuned! Stay tuned for the next installment, where we configure load balancers and install Concourse CI.\n References  HashiCorp\u0026rsquo;s excellent tutorial, Provision a GKE Cluster (Google Cloud) https://learn.hashicorp.com/terraform/kubernetes/provision-gke-cluster Companion GitHub repository, https://github.com/hashicorp/learn-terraform-provision-gke-cluster  Footnotes provenance\nThis begs the question, \u0026ldquo;If we\u0026rsquo;re patterning our templates after HashiCorp\u0026rsquo;s, why not use HashiCorp\u0026rsquo;s directly? Why change the templates?\u0026rdquo;\nOur answer: \u0026ldquo;If you want to use the HashiCorp templates, by all means do so—they\u0026rsquo;re great templates!\u0026rdquo;\nOur templates have been modified from HashiCorp\u0026rsquo;s to suit our purposes; for example, we split the templates into a virtual private cloud (VPC) (vpc.tf) template and a Google Kubernetes Engine (gke) (gke.tf) template. It seemed like a good idea at the time. Also, we didn\u0026rsquo;t want to spend a lot of money, so instead of three instances in the region, we modified the template to place two instances in the same availability zone.\n[e2-medium instances cost $24.46 / month in the region us-central1. We didn\u0026rsquo;t want to spend the extra $25 for a third instance.]\nFinally, we didn\u0026rsquo;t like the name of our Google Cloud project (\u0026ldquo;blabbertabber\u0026rdquo;): it was too long \u0026amp; referred to a project we had mothballed months ago. We wanted a shorter and friendlier name (\u0026ldquo;nono\u0026rdquo;), and we were loath to create a brand new Google Cloud project, so we modified the templates to include a \u0026ldquo;friendly\u0026rdquo; project name.\n","permalink":"https://blog.nono.io/post/concourse_on_k8s-1/","summary":"Let\u0026rsquo;s deploy Concourse, a continuous-integration, continuous delivery (CI/CD) application (similar to Jenkins and CircleCI).\nWe\u0026rsquo;ll deploy it to Google Cloud, to our Google Kubernetes Engine (GKE).\nIn this post, we\u0026rsquo;ll use HashiCorp\u0026rsquo;s Terraform to create our cluster.\nWe assume you\u0026rsquo;ve already installed the terraform command-line interface (CLI) and created a Google Cloud account.\nmkdir -p ~/workspace/gke cd ~/workspace/gke Next we download the terraform templates and terraform vars file:","title":"Concourse CI on Kubernetes (GKE), Part 1: Terraform"},{"content":"Why am I creating a new blog? What was wrong with the old blog? Why don\u0026rsquo;t I use Medium?\nThe short version: The old blog is frozen in time, like a prince caught in amber 1 or a dandy in aspic 2. I can no longer post to it.\nThe old blog, the Pivotal Engineering Journal, which many of Pivotal\u0026rsquo;s engineers contributed to, was archived a year after VMware acquired Pivotal. Every acquisition brings changes, and this was, in the scheme of things, a very minor one. At least VMware kept the blog instead of simply discarding it.\nDozens of my blog posts, representing hundreds of hours of work, are now tucked away in an even smaller corner of the internet. That\u0026rsquo;s okay: technical posts have a short shelf life. They have served their purpose.\nBut I cannot lay the fault at the feet of the VMware acquisition; I, too, had a part: My interest in blogging had waned, my output, diminished. In times past I had blogged as frequently as every two months, but then slowed to a crawl: once every six months, and now it has been over a year since I wrote a technical blog post.\nThen the wind shifted, and I again felt the urge to write. Perhaps a VMware corporate blog? Maybe not. I wasn\u0026rsquo;t sure if they\u0026rsquo;d be hands-off, allowing me to choose my topics, express my opinions.\nPerhaps Medium? I discounted that, too, for I find the experience of reading articles on Medium unpleasant, especially the plaintive warning that accompanies every post: \u0026ldquo;You have 2 free member-only stories left this month. Upgrade for unlimited access.\u0026rdquo; I wanted my readers to have unfettered access to my writing.\nFinally, I like writing in Markdown using Neovim: less mouse, more keyboard.\nThe winner? A combination of Hugo and GitHub Pages.\n","permalink":"https://blog.nono.io/post/why_new_blog/","summary":"Why am I creating a new blog? What was wrong with the old blog? Why don\u0026rsquo;t I use Medium?\nThe short version: The old blog is frozen in time, like a prince caught in amber 1 or a dandy in aspic 2. I can no longer post to it.\nThe old blog, the Pivotal Engineering Journal, which many of Pivotal\u0026rsquo;s engineers contributed to, was archived a year after VMware acquired Pivotal.","title":"The Old Blog is Dead. Long Live the New Blog!"}]