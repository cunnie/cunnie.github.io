<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Brian Cunnie&#39;s Technical Blog</title>
    <link>https://blog.nono.io/post/</link>
    <description>Recent content in Posts on Brian Cunnie&#39;s Technical Blog</description>
    <image>
      <url>https://nono.io/images/brian_cunnie_profile.jpg</url>
      <link>https://nono.io/images/brian_cunnie_profile.jpg</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 08 Jan 2022 04:55:18 -0800</lastBuildDate><atom:link href="https://blog.nono.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Concourse CI on Kubernetes (GKE), Part 6: Concourse &amp; Vault: Backup &amp; Restore</title>
      <link>https://blog.nono.io/post/concourse_on_k8s-6/</link>
      <pubDate>Sat, 08 Jan 2022 04:55:18 -0800</pubDate>
      
      <guid>https://blog.nono.io/post/concourse_on_k8s-6/</guid>
      <description>Recreating the Cluster We want to recreate our cluster while preserving our Vault and Concourse data (we want to recreate our GKE regional cluster as a zonal cluster to take advantage of the GKE free tier which saves us $74.40 per month).
Note: when we say, &amp;ldquo;recreate the cluster&amp;rdquo;, we really mean, &amp;ldquo;recreate the cluster&amp;rdquo;. We destroy the old cluster, including our worker nodes and persistent volumes.
Backup Vault In the following example, our storage path is /vault/data, but there&amp;rsquo;s a chance that yours is different.</description>
    </item>
    
    <item>
      <title>Disk Controller Benchmarks: VMware Paravirtual&#39;s vs. LSI Logic Parallel&#39;s</title>
      <link>https://blog.nono.io/post/pvscsi/</link>
      <pubDate>Fri, 19 Nov 2021 08:12:28 -0700</pubDate>
      
      <guid>https://blog.nono.io/post/pvscsi/</guid>
      <description>Is it worth switching your VMware vSphere VM&amp;rsquo;s SCSI (small computer system interface) from the LSI Logic Parallel controller to the VMware Paravirtual SCSI controller? Except for ultra-high-end database servers (&amp;gt; 1M IOPS ( input/output operations per second)), the answer is &amp;ldquo;no&amp;rdquo;; the difference is negligible.
Our benchmarks show that VMware&amp;rsquo;s Paravirtual SCSI (small computer system interface) controller offered a 2-3% performance increase in IOPS (I/O (input/output) operations per second) over the LSI Logic Parallel SCSI controller at the cost of a similar decrease in sequential performance (both read &amp;amp; write).</description>
    </item>
    
    <item>
      <title>Concourse CI on Kubernetes (GKE), Part 5: Vault</title>
      <link>https://blog.nono.io/post/concourse_on_k8s-5/</link>
      <pubDate>Thu, 18 Nov 2021 03:46:53 -0800</pubDate>
      
      <guid>https://blog.nono.io/post/concourse_on_k8s-5/</guid>
      <description>In our previous post, we configured our GKE Concourse CI server, which was the capstone of the series. But we were wrong: this post is the capstone in the series. In this post, we install Vault and configure our Concourse CI server to use Vault to retrieve secrets.
Installation Most of these instructions are derived from the Hashicorp tutorial, Vault on Kubernetes Deployment Guide.
Create a DNS A record which points to the IP address of your GKE load balancer.</description>
    </item>
    
    <item>
      <title>Concourse CI on Kubernetes (GKE), Part 4: Concourse</title>
      <link>https://blog.nono.io/post/concourse_on_k8s-4/</link>
      <pubDate>Wed, 01 Sep 2021 18:39:26 -0700</pubDate>
      
      <guid>https://blog.nono.io/post/concourse_on_k8s-4/</guid>
      <description>In our previous post, we configured our GKE (Google Kubernetes Engine) to use Let&amp;rsquo;s Encrypt TLS certificates. In this post, the capstone of our series, we install Concourse CI.
Installation These instructions are a more-opinionated version of the canonical instructions for the Concourse CI Helm chart found here: https://github.com/concourse/concourse-chart.
First Install: with Helm We use helm to install Concourse. We first add the Helm repo, and then install it. We take the opportunity to bump the default login time from 24 hours to ten days (duration=240h) because we hate re-authenticating to our Concourse every morning.</description>
    </item>
    
    <item>
      <title>Concourse CI on Kubernetes (GKE), Part 3: TLS</title>
      <link>https://blog.nono.io/post/concourse_on_k8s-3/</link>
      <pubDate>Wed, 11 Aug 2021 16:55:31 -0700</pubDate>
      
      <guid>https://blog.nono.io/post/concourse_on_k8s-3/</guid>
      <description>In our previous blog post, we configured ingress to our Kubernetes cluster but were disappointed to discover that the TLS certificates were self-signed. In this post we&amp;rsquo;ll remedy that by installing cert-manager, the Cloud native certificate management tool.
Disclaimer: most of this blog post was lifted whole cloth from the most-excellent cert-manager documentation. We merely condensed it &amp;amp; made it more opinionated.
Installation Let&amp;rsquo;s add the Jetstack Helm Repository:</description>
    </item>
    
    <item>
      <title>Concourse CI on Kubernetes (GKE), Part 2: Ingress</title>
      <link>https://blog.nono.io/post/concourse_on_k8s-2/</link>
      <pubDate>Sat, 07 Aug 2021 14:49:17 -0700</pubDate>
      
      <guid>https://blog.nono.io/post/concourse_on_k8s-2/</guid>
      <description>In our previous blog post, we set up our Kubernetes cluster and deployed a pod running nginx, but the experience was disappointing—we couldn&amp;rsquo;t browse to our pod. Let&amp;rsquo;s fix that by deploying the nginx Ingress controller.
Acquire the External IP Address (Elastic IP) We&amp;rsquo;ll use the Google Cloud console to acquire the external address [external address] for our load balancer.
Navigate to VPC network → External IP addresses → Reserve Static Address:</description>
    </item>
    
    <item>
      <title>Concourse CI on Kubernetes (GKE), Part 1: Terraform</title>
      <link>https://blog.nono.io/post/concourse_on_k8s-1/</link>
      <pubDate>Fri, 06 Aug 2021 16:38:00 -0700</pubDate>
      
      <guid>https://blog.nono.io/post/concourse_on_k8s-1/</guid>
      <description>Let&amp;rsquo;s deploy Concourse, a continuous-integration, continuous delivery (CI/CD) application (similar to Jenkins and CircleCI).
We&amp;rsquo;ll deploy it to Google Cloud, to our Google Kubernetes Engine (GKE).
In this post, we&amp;rsquo;ll use HashiCorp&amp;rsquo;s Terraform to create our cluster.
We assume you&amp;rsquo;ve already installed the terraform command-line interface (CLI) and created a Google Cloud account.
mkdir -p ~/workspace/gke cd ~/workspace/gke Next we download the terraform templates and terraform vars file:
curl -OL https://raw.</description>
    </item>
    
    <item>
      <title>The Old Blog is Dead. Long Live the New Blog!</title>
      <link>https://blog.nono.io/post/why_new_blog/</link>
      <pubDate>Sun, 18 Jul 2021 10:34:36 -0700</pubDate>
      
      <guid>https://blog.nono.io/post/why_new_blog/</guid>
      <description>Why am I creating a new blog? What was wrong with the old blog? Why don&amp;rsquo;t I use Medium?
The short version: The old blog is frozen in time, like a prince caught in amber 1 or a dandy in aspic 2. I can no longer post to it.
The old blog, the Pivotal Engineering Journal, which many of Pivotal&amp;rsquo;s engineers contributed to, was archived a year after VMware acquired Pivotal.</description>
    </item>
    
  </channel>
</rss>
