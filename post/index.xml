<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Brian Cunnie&#39;s Technical Blog</title>
    <link>https://blog.nono.io/post/</link>
    <description>Recent content in Posts on Brian Cunnie&#39;s Technical Blog</description>
    <image>
      <url>https://nono.io/images/brian_cunnie_profile.jpg</url>
      <link>https://nono.io/images/brian_cunnie_profile.jpg</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 25 Jun 2023 08:05:37 -0800</lastBuildDate><atom:link href="https://blog.nono.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How to Install a TLS Certificate on NSX 4.1</title>
      <link>https://blog.nono.io/post/nsx_tls/</link>
      <pubDate>Sun, 25 Jun 2023 08:05:37 -0800</pubDate>
      
      <guid>https://blog.nono.io/post/nsx_tls/</guid>
      <description>Create CSR (certificate signing request):
export CN=nsx.nono.io # change for your NSX&amp;#39;s fully-qualified domain name openssl ecparam -name P-256 -genkey -out ${CN}.key openssl req \ -new \ -key ${CN}.key \ -out ${CN}.csr \ -sha256 \ -nodes \ -subj &amp;#34;/C=US/ST=California/L=San Francisco/O=nono.io/CN=${CN}/emailAddress=brian.cunnie@gmail.com&amp;#34; \ -config &amp;lt;(cat &amp;lt;&amp;lt;EOF [ req ] distinguished_name = req_distinguished_name req_extensions = req_ext [ req_distinguished_name ] [ req_ext ] subjectAltName = @alt_names [alt_names] DNS.1 = ${CN} EOF ) You&amp;rsquo;ll need to catenate your certificate onto its CA bundle (certificate chain); In my case, I did the following:</description>
    </item>
    
    <item>
      <title>On-premise is Almost Four Times Cheaper * than the Cloud</title>
      <link>https://blog.nono.io/post/on-premise_vs_cloud/</link>
      <pubDate>Wed, 04 Jan 2023 19:42:50 -0800</pubDate>
      
      <guid>https://blog.nono.io/post/on-premise_vs_cloud/</guid>
      <description>* If you don&amp;rsquo;t count the amount of time spent maintaining the on-premise equipment.
Abstract My 48-VM (virtual machine) homelab configuration costs me approximately $430/month in hardware, electricity, virtualization software, and internet, but an equivalent configuration on AWS (Amazon Web Services) would cost $1,660/month (almost four times as expensive)!
Disclosures:
I work for VMware, which sells on-premise virtualization software (i.e. vSphere). I didn&amp;rsquo;t put a dollar value on the time spent maintaining on-premise because I had a hard time assigning a dollar value.</description>
    </item>
    
    <item>
      <title>The Least Secure Way to Back Up vCenter 8.0 with TrueNAS 13.0</title>
      <link>https://blog.nono.io/post/backup_vcenter_w_truenas/</link>
      <pubDate>Mon, 02 Jan 2023 16:34:10 -0800</pubDate>
      
      <guid>https://blog.nono.io/post/backup_vcenter_w_truenas/</guid>
      <description>We&amp;rsquo;re going to set up automated backups for a vCenter which we were forced to rebuild over the winter break because the unexpected reboot of the file server hosting the iSCSI datastore backing the vCenter&amp;rsquo;s disk drive caused unrecoverable database corruption, and we had no backups.
Log into your TrueNAS server via its web interface, e.g. https://nas.nono.io Browse to &amp;ldquo;Services&amp;rdquo; Start FTP (by toggling the &amp;ldquo;Running&amp;rdquo; slider) and configure it to start automatically Remember to start the FTP service and configure it to start automatically.</description>
    </item>
    
    <item>
      <title>Creating Multi-Platform Docker Images with Concourse</title>
      <link>https://blog.nono.io/post/multi-platform_docker_images_with_concourse/</link>
      <pubDate>Fri, 25 Nov 2022 08:13:55 -0800</pubDate>
      
      <guid>https://blog.nono.io/post/multi-platform_docker_images_with_concourse/</guid>
      <description>Concourse CI/CD (continuous integration/continuous delivery) can create multi-platform Docker images. This blog post describes how.
A multi-platform docker image is one that contains &amp;ldquo;variants for different architectures&amp;rdquo;.
Docker images are often created for a single architecture (&amp;ldquo;instruction set architecture&amp;rdquo; or &amp;ldquo;ISA&amp;rdquo;), typically Intel&amp;rsquo;s/AMD&amp;rsquo;s x86-64, but with the advent of ARM64-based offerings such as AWS&amp;rsquo;s Graviton and Apple&amp;rsquo;s M1/M2, It&amp;rsquo;s becoming more common to build multi-platform images to avoid the heavy emulation performance penalty (typically &amp;gt;10x) when running an image on a different architecture.</description>
    </item>
    
    <item>
      <title>How to Install a TLS Certificate on vCenter Server Appliance (VCSA) 8.0</title>
      <link>https://blog.nono.io/post/vcenter_8.0_tls/</link>
      <pubDate>Wed, 02 Nov 2022 10:16:22 +0000</pubDate>
      
      <guid>https://blog.nono.io/post/vcenter_8.0_tls/</guid>
      <description>Quickstart First, create your key and your CSR (Certificate Signing Request). In the following example, we are creating a CSR for our vCenter host, &amp;ldquo;vcenter-80.nono.io&amp;rdquo;:
CN=vcenter-80.nono.io # &amp;#34;CN&amp;#34; is the abbreviation for &amp;#34;Common Name&amp;#34; openssl genrsa -out $CN.key 3072 openssl req \ -new \ -key $CN.key \ -out $CN.csr \ -sha256 \ -subj &amp;#34;/C=US/ST=California/L=San Francisco/O=nono.io/OU=homelab/CN=${CN}/emailAddress=brian.cunnie@gmail.com&amp;#34; \ -config &amp;lt;(cat &amp;lt;&amp;lt;EOF [ req ] distinguished_name = req_distinguished_name req_extensions = req_ext [ req_distinguished_name ] [ req_ext ] subjectAltName = @alt_names [alt_names] DNS.</description>
    </item>
    
    <item>
      <title>Tuning HAProxy in a vSphere Environment</title>
      <link>https://blog.nono.io/post/tuning_haproxy/</link>
      <pubDate>Sat, 10 Sep 2022 09:33:02 -0700</pubDate>
      
      <guid>https://blog.nono.io/post/tuning_haproxy/</guid>
      <description>Network Diagram. We want to maximize the throughput from the blue box (the client) to the green box (HAProxy)
Summary We were able to push through almost 450 MB/sec through HAProxy (which terminated our SSL) by carefully matching our 4-core HAProxy with 2 x 4-core Gorouters (which were on a much slower ESXi host).
Results Bandwidth MB/second Configuration 201.27MB 1 HAProxy: 1 vCPU 136.47MB 1 HAProxy: 2 vCPUs 270.56MB 2 Gorouters: 1 vCPU 350.</description>
    </item>
    
    <item>
      <title>The Underground Guide to Cloud Foundry Acceptance Tests</title>
      <link>https://blog.nono.io/post/underground_guide_to_cf_acceptance/</link>
      <pubDate>Mon, 04 Jul 2022 12:46:03 -0700</pubDate>
      
      <guid>https://blog.nono.io/post/underground_guide_to_cf_acceptance/</guid>
      <description>The Cloud Foundry Acceptance Tests are the gold standard to test the proper functioning of your Cloud Foundry deployment. This guide tells you how to run them. When in doubt, refer to the README.
Quick Start cd ~/workspace/ git clone git@github.com:cloudfoundry/cf-acceptance-tests.git cd cf-acceptance-tests . ./.envrc bin/update_submodules cp example-cats-config.json cats-config.json export CONFIG=cats-config.json cf api api.cf.nono.io # or whatever your Cloud Foundry&amp;#39;s API endpoint is cf login -u admin cf create-space -o system system # don&amp;#39;t worry if it&amp;#39;s already created cf t -o system -s system cf enable-feature-flag diego_docker # necessary if you&amp;#39;re running the Docker tests (`&amp;#34;include_docker&amp;#34;: true`) Let&amp;rsquo;s configure our cats-config.</description>
    </item>
    
    <item>
      <title>Concourse CI on Kubernetes (GKE), Part 6: Concourse &amp; Vault: Backup &amp; Restore</title>
      <link>https://blog.nono.io/post/concourse_on_k8s-6/</link>
      <pubDate>Sat, 08 Jan 2022 04:55:18 -0800</pubDate>
      
      <guid>https://blog.nono.io/post/concourse_on_k8s-6/</guid>
      <description>Recreating the Cluster We want to recreate our cluster while preserving our Vault and Concourse data (we want to recreate our GKE regional cluster as a zonal cluster to take advantage of the GKE free tier which saves us $74.40 per month).
Note: when we say, &amp;ldquo;recreate the cluster&amp;rdquo;, we really mean, &amp;ldquo;recreate the cluster&amp;rdquo;. We destroy the old cluster, including our worker nodes and persistent volumes.
Backup Vault In the following example, our storage path is /vault/data, but there&amp;rsquo;s a chance that yours is different.</description>
    </item>
    
    <item>
      <title>Disk Controller Benchmarks: VMware Paravirtual&#39;s vs. LSI Logic Parallel&#39;s</title>
      <link>https://blog.nono.io/post/pvscsi/</link>
      <pubDate>Fri, 19 Nov 2021 08:12:28 -0700</pubDate>
      
      <guid>https://blog.nono.io/post/pvscsi/</guid>
      <description>Is it worth switching your VMware vSphere VM&amp;rsquo;s SCSI (small computer system interface) from the LSI Logic Parallel controller to the VMware Paravirtual SCSI controller? Except for ultra-high-end database servers (&amp;gt; 1M IOPS ( input/output operations per second)), the answer is &amp;ldquo;no&amp;rdquo;; the difference is negligible.
Our benchmarks show that VMware&amp;rsquo;s Paravirtual SCSI (small computer system interface) controller offered a 2-3% performance increase in IOPS (I/O (input/output) operations per second) over the LSI Logic Parallel SCSI controller at the cost of a similar decrease in sequential performance (both read &amp;amp; write).</description>
    </item>
    
    <item>
      <title>Concourse CI on Kubernetes (GKE), Part 5: Vault</title>
      <link>https://blog.nono.io/post/concourse_on_k8s-5/</link>
      <pubDate>Thu, 18 Nov 2021 03:46:53 -0800</pubDate>
      
      <guid>https://blog.nono.io/post/concourse_on_k8s-5/</guid>
      <description>In our previous post, we configured our GKE Concourse CI server, which was the capstone of the series. But we were wrong: this post is the capstone in the series. In this post, we install Vault and configure our Concourse CI server to use Vault to retrieve secrets.
Installation Most of these instructions are derived from the Hashicorp tutorial, Vault on Kubernetes Deployment Guide.
Create a DNS A record which points to the IP address of your GKE load balancer.</description>
    </item>
    
    <item>
      <title>Concourse CI on Kubernetes (GKE), Part 4: Concourse</title>
      <link>https://blog.nono.io/post/concourse_on_k8s-4/</link>
      <pubDate>Wed, 01 Sep 2021 18:39:26 -0700</pubDate>
      
      <guid>https://blog.nono.io/post/concourse_on_k8s-4/</guid>
      <description>In our previous post, we configured our GKE (Google Kubernetes Engine) to use Let&amp;rsquo;s Encrypt TLS certificates. In this post, the capstone of our series, we install Concourse CI.
Installation These instructions are a more-opinionated version of the canonical instructions for the Concourse CI Helm chart found here: https://github.com/concourse/concourse-chart.
First Install: with Helm We use helm to install Concourse. We first add the Helm repo, and then install it. We take the opportunity to bump the default login time from 24 hours to ten days (duration=240h) because we hate re-authenticating to our Concourse every morning.</description>
    </item>
    
    <item>
      <title>Concourse CI on Kubernetes (GKE), Part 3: TLS</title>
      <link>https://blog.nono.io/post/concourse_on_k8s-3/</link>
      <pubDate>Wed, 11 Aug 2021 16:55:31 -0700</pubDate>
      
      <guid>https://blog.nono.io/post/concourse_on_k8s-3/</guid>
      <description>In our previous blog post, we configured ingress to our Kubernetes cluster but were disappointed to discover that the TLS certificates were self-signed. In this post we&amp;rsquo;ll remedy that by installing cert-manager, the Cloud native certificate management tool.
Disclaimer: most of this blog post was lifted whole cloth from the most-excellent cert-manager documentation. We merely condensed it &amp;amp; made it more opinionated.
Installation Let&amp;rsquo;s add the Jetstack Helm Repository:</description>
    </item>
    
    <item>
      <title>Concourse CI on Kubernetes (GKE), Part 2: Ingress</title>
      <link>https://blog.nono.io/post/concourse_on_k8s-2/</link>
      <pubDate>Sat, 07 Aug 2021 14:49:17 -0700</pubDate>
      
      <guid>https://blog.nono.io/post/concourse_on_k8s-2/</guid>
      <description>In our previous blog post, we set up our Kubernetes cluster and deployed a pod running nginx, but the experience was disappointing—we couldn&amp;rsquo;t browse to our pod. Let&amp;rsquo;s fix that by deploying the nginx Ingress controller.
Acquire the External IP Address (Elastic IP) We&amp;rsquo;ll use the Google Cloud console to acquire the external address [external address] for our load balancer.
Navigate to VPC network → External IP addresses → Reserve Static Address:</description>
    </item>
    
    <item>
      <title>Concourse CI on Kubernetes (GKE), Part 1: Terraform</title>
      <link>https://blog.nono.io/post/concourse_on_k8s-1/</link>
      <pubDate>Fri, 06 Aug 2021 16:38:00 -0700</pubDate>
      
      <guid>https://blog.nono.io/post/concourse_on_k8s-1/</guid>
      <description>Let&amp;rsquo;s deploy Concourse, a continuous-integration, continuous delivery (CI/CD) application (similar to Jenkins and CircleCI).
We&amp;rsquo;ll deploy it to Google Cloud, to our Google Kubernetes Engine (GKE).
In this post, we&amp;rsquo;ll use HashiCorp&amp;rsquo;s Terraform to create our cluster.
We assume you&amp;rsquo;ve already installed the terraform command-line interface (CLI) and created a Google Cloud account.
mkdir -p ~/workspace/gke cd ~/workspace/gke Next we download the terraform templates and terraform vars file:
curl -OL https://raw.</description>
    </item>
    
    <item>
      <title>The Old Blog is Dead. Long Live the New Blog!</title>
      <link>https://blog.nono.io/post/why_new_blog/</link>
      <pubDate>Sun, 18 Jul 2021 10:34:36 -0700</pubDate>
      
      <guid>https://blog.nono.io/post/why_new_blog/</guid>
      <description>Why am I creating a new blog? What was wrong with the old blog? Why don&amp;rsquo;t I use Medium?
The short version: The old blog is frozen in time, like a prince caught in amber 1 or a dandy in aspic 2. I can no longer post to it.
The old blog, the Pivotal Engineering Journal, which many of Pivotal&amp;rsquo;s engineers contributed to, was archived a year after VMware acquired Pivotal.</description>
    </item>
    
    <item>
      <title>Flow Your Tests Like Your Code</title>
      <link>https://blog.nono.io/post/go-flow-tests-like-code/</link>
      <pubDate>Sun, 16 Feb 2020 17:16:22 +0000</pubDate>
      
      <guid>https://blog.nono.io/post/go-flow-tests-like-code/</guid>
      <description>My co-worker Belinda Liu turned to me and said, &amp;ldquo;I don&amp;rsquo;t like these tests at all; they&amp;rsquo;re hard to follow, and I&amp;rsquo;m not sure what they&amp;rsquo;re testing.&amp;rdquo;
I looked at the tests that I had spent much of yesterday afternoon working on. She was right: they were hard to follow (even for me, who had written some of them!).
How had we gotten here? Our code was straightforward, but our tests were byzantine (excessively complicated).</description>
    </item>
    
    <item>
      <title>How To Enable IPv6 on Your Cloud Foundry&#39;s HAProxy</title>
      <link>https://blog.nono.io/post/haproxy-ipv6/</link>
      <pubDate>Sat, 01 Feb 2020 01:16:22 +0000</pubDate>
      
      <guid>https://blog.nono.io/post/haproxy-ipv6/</guid>
      <description>0. Abstract HAProxy is an optional load balancer included in the canonical open source Cloud Foundry deployment. Its intended use is on IaaSes (Infrastructures as a Service) that do not offer built-in load balancers [0]. On vSphere, this means without the optional network virtualization solutions, NSX-T and NSX-V. This blog post describes how to assign an IPv6 address to an HAProxy load balancer in a Cloud Foundry deployment.
1. Pre-requisites Users following this blog post should be familiar with BOSH, BOSH&amp;rsquo;s manifest operations files, IPv6, and deploying Cloud Foundry using cf-deployment.</description>
    </item>
    
    <item>
      <title>A High-performing Mid-range NAS Server, Part 3: 10 GbE</title>
      <link>https://blog.nono.io/post/nas-performance-tuning/</link>
      <pubDate>Sat, 25 May 2019 17:16:22 +0000</pubDate>
      
      <guid>https://blog.nono.io/post/nas-performance-tuning/</guid>
      <description>Abstract &amp;ldquo;How much faster will my VM&amp;rsquo;s disks be if I upgrade my ZFS-based (Z File System) NAS to 10 GbE?&amp;rdquo; The disks will be faster, in some cases, much faster. Our experience is that sequential read throughput will be 1.4✕ faster, write throughput, 10✕ faster, and IOPS, 1.6✕ faster.
We ran a three-hour benchmark on our NAS server before and after upgrading to 10 GbE. We ran the benchmark again after upgrading.</description>
    </item>
    
    <item>
      <title>Transferring Time-based One-time Passwords to a New Smartphone</title>
      <link>https://blog.nono.io/post/totp/</link>
      <pubDate>Mon, 21 Jan 2019 11:16:22 +0000</pubDate>
      
      <guid>https://blog.nono.io/post/totp/</guid>
      <description>Abstract Smartphone authenticator apps such as Google Authenticator and Authy implement software tokens that are &amp;ldquo;two-step verification services using the Time-based One-time Password Algorithm (TOTP) and HMAC-based One-time Password algorithm (HOTP)&amp;rdquo;
Smartphone TOTP, a form of Two-factor authentication (2FA), displays a 6-digit code derived from a shared secret, updating every thirty seconds.
The shared secret is presented only once to the user, typically with a QR (Quick Response) Code which is scanned by the authenticator app.</description>
    </item>
    
    <item>
      <title>Troubleshooting Obscure OpenSSH Failures</title>
      <link>https://blog.nono.io/post/ssh_handshake_failed/</link>
      <pubDate>Wed, 28 Nov 2018 17:16:22 +0000</pubDate>
      
      <guid>https://blog.nono.io/post/ssh_handshake_failed/</guid>
      <description>Abstract By using tcpdump to troubleshoot an elusive error, we uncovered a man-in-the-middle (MITM) ssh proxy installed by our information security (InfoSec) team to harden/protect a set of machines which were accessible from the internet. The ssh proxy in question was Palo Alto Network’s (PAN) Layer 7 (i.e. it worked on any port, not solely ssh’s port 22) proxy, and was discovered when we observed a failure to negotiate ciphers during the ssh key exchange.</description>
    </item>
    
    <item>
      <title>Safely Upgrading PAS 2.2 with NSX-T Load Balancers</title>
      <link>https://blog.nono.io/post/upgrade_2.2-2.3_on_nsx-t/</link>
      <pubDate>Thu, 06 Sep 2018 10:44:30 +0000</pubDate>
      
      <guid>https://blog.nono.io/post/upgrade_2.2-2.3_on_nsx-t/</guid>
      <description>When customers with vSphere+NSX-T-based foundations apply a stemcell update, update a tile, or upgrade PAS (Pivotal Application Service) from 2.2 to 2.3, their Cloud Foundry may become unreachable as their NSX-T static load balancer server pools have been emptied.
This blog post describes a method to ensure availability during upgrades. We use a combination of customized Operations Manager resource configs and BOSH VM Extensions.
The sample workflow in this post is for upgrading PAS 2.</description>
    </item>
    
    <item>
      <title>How to Install a TLS Certificate on vCenter Server Appliance (VCSA) 6.7 [Updated for vCenter 7]</title>
      <link>https://blog.nono.io/post/vcenter_6.7_tls/</link>
      <pubDate>Wed, 09 May 2018 23:16:22 +0000</pubDate>
      
      <guid>https://blog.nono.io/post/vcenter_6.7_tls/</guid>
      <description>The following section is the new Quickstart for installing a TLS certificate on vCenter 7
vCenter 7 Quickstart On your vCenter, navigate to Menu → Administration → Certificates → Certificate Management
On the __MACHINE_CERT tile, click Actions, select Generate Certificate Signing Request (CSR).
Enter the appropriate info; for inspiration, this is what we entered:
Common name: vcenter-70.nono.io Organization: nono.io Organizational Unit: homelab Country: United States State/Province: California Locality: San Francisco Email Address: yoyo@nono.</description>
    </item>
    
    <item>
      <title>Benchmarking the Disk Speed of IaaSes</title>
      <link>https://blog.nono.io/post/gobonniego_results/</link>
      <pubDate>Fri, 16 Mar 2018 20:00:22 +0000</pubDate>
      
      <guid>https://blog.nono.io/post/gobonniego_results/</guid>
      <description>0. Overview [Disclaimer: the author works for Pivotal Software, of which Dell is an investor. Dell is also an owner of VMware]
It&amp;rsquo;s helpful to know the performance characteristics of disks when selecting a disk type. For example, the performance of a database server will be greatly affected by the IOPS of the underlying storage. Similarly, a video-streaming server will be affected by the underlying read throughput.
0.0 Highlights: If you need a fast disk, nothing beats a local vSphere NVMe drive.</description>
    </item>
    
    <item>
      <title>Deploying BOSH VMs with IPv6 Addresses on vSphere</title>
      <link>https://blog.nono.io/post/bosh-on-ipv6-2/</link>
      <pubDate>Tue, 16 Jan 2018 19:12:22 +0000</pubDate>
      
      <guid>https://blog.nono.io/post/bosh-on-ipv6-2/</guid>
      <description>0. Abstract BOSH is a VM orchestrator; a BOSH Director creates, configures, monitors, and deletes VMs. The BOSH Director interoperates with a number of IaaSes (Infrastructure as a Service), one of which is VMware vSphere, a virtualization platform. BOSH traditionally operates exclusively within the IPv4 networking space (i.e. the BOSH Director has an IPv4 address (e.g. 10.0.0.6), and the VMs which it deploys also have IPv4 addresses); however, recent changes have enabled IPv6 networking within the BOSH Framework.</description>
    </item>
    
    <item>
      <title>Maintaining BOSH Directors with Concourse CI and bosh-deployment</title>
      <link>https://blog.nono.io/post/bosh-deployed-with-concourse/</link>
      <pubDate>Fri, 24 Nov 2017 14:06:25 +0000</pubDate>
      
      <guid>https://blog.nono.io/post/bosh-deployed-with-concourse/</guid>
      <description>&amp;ldquo;BOSH deploys Concourse, and Concourse deploys BOSH&amp;rdquo; —Cloud Foundry koan
A BOSH Director is a VM (virtual machine) orchestrator which is itself a VM. BOSH solves the problem of keeping its VMs&amp;rsquo; applications (operating systems (stemcells) and releases) up-to-date with the command, bosh deploy; however, this begs the question, &amp;ldquo;what keeps the BOSH Director itself up-to-date?&amp;rdquo;. [Quis custodiet?]
We explore using Concourse, a Continuous Integration (CI) server, and bosh-deployment [Updating BOSH], in order to create a Concourse pipeline which updates, in turn, a BOSH director on AWS (Amazon Web Services), on Microsoft Azure, and GCP (Google Cloud Platform).</description>
    </item>
    
    <item>
      <title>Deploying a BOSH Director With SSL Certificates Issued by Commercial CA</title>
      <link>https://blog.nono.io/post/bosh-ssl/</link>
      <pubDate>Wed, 16 Aug 2017 17:16:22 +0000</pubDate>
      
      <guid>https://blog.nono.io/post/bosh-ssl/</guid>
      <description>0. Abstract A BOSH director is a virtual machine (VM) orchestrator which deploys VMs to various Infrastructures as a Service (IaaS) such as Amazon Web Services (AWS) and Google Cloud Platform (GCP). The BOSH Command Line (CLI) communicates with the director over Secure Sockets Layer (SSL). While most BOSH directors are deployed with self-signed certificates, it is possible to configure a BOSH director with certificates issued by a recognized certificate authority (CA) (e.</description>
    </item>
    
    <item>
      <title>Deploy To vSphere NSX-T Opaque Networks Using BOSH</title>
      <link>https://blog.nono.io/post/bosh-vsphere-opaque-networks/</link>
      <pubDate>Mon, 17 Apr 2017 11:43:14 -0700</pubDate>
      
      <guid>https://blog.nono.io/post/bosh-vsphere-opaque-networks/</guid>
      <description>VMware&amp;rsquo;s vSphere is an Infrastructure as a Service (IaaS) which runs Virtual Machines (VMs). BOSH is a VM orchestrator which automates the creation of VMs. NSX-T is a pluggable Network backend for vSphere (and other hypervisors). NSX-T allows the creation of opaque networks in vSphere, networks whose detail and configuration of the network is unknown to vSphere and which is managed outside vSphere.
With the release of BOSH vSphere CPI v40, users can attach their BOSH-deployed VMs to an NSX-T opaque network.</description>
    </item>
    
    <item>
      <title>Why Is My NTP Server Costing $500/Year? Part 3</title>
      <link>https://blog.nono.io/post/ntp-costs-500/</link>
      <pubDate>Sat, 28 Jan 2017 12:38:13 -0800</pubDate>
      
      <guid>https://blog.nono.io/post/ntp-costs-500/</guid>
      <description>When Hacker News picked up Part 1 of our series of blog posts on running public NTP servers, a contributor said, &amp;ldquo;I wish he&amp;rsquo;d explained &amp;hellip; what they ultimately did (since there&amp;rsquo;s no part 3 that I can find).&amp;rdquo;
We had dropped the ball — we had never concluded the series, had never written part 3, had never described the strategies to mitigate the data transfer costs.
This blog post remedies that oversight; it consists of two parts: the first part addresses strategies to reduce the cost of running an NTP server, and the second part discusses side topics (aspects of running an NTP server).</description>
    </item>
    
    <item>
      <title>Using the beta BOSH CLI to Deploy an IPv6-enabled nginx Server to AWS</title>
      <link>https://blog.nono.io/post/bosh-on-ipv6/</link>
      <pubDate>Tue, 20 Dec 2016 20:49:50 +0000</pubDate>
      
      <guid>https://blog.nono.io/post/bosh-on-ipv6/</guid>
      <description>This blog post describes the procedure we followed to use the beta BOSH command line interface (CLI) to deploy an nginx webserver with a native IPv6 address (i.e. 2600:1f16:0a62:5c00::4) to AWS in addition to its IPv4 Elastic IP address (i.e. 52.15.73.90). We were then able to browse the webserver via the IPv6 protocol.
BOSH does not support IPv6. This is a proof-of-concept. Do not apply IPv6 to your production BOSH Directors or to BOSH CLI-deployed systems.</description>
    </item>
    
    <item>
      <title>Leveraging NSX&#39;s Features with BOSH&#39;s vSphere CPI</title>
      <link>https://blog.nono.io/post/nsx_with_bosh/</link>
      <pubDate>Tue, 01 Nov 2016 09:15:02 -0700</pubDate>
      
      <guid>https://blog.nono.io/post/nsx_with_bosh/</guid>
      <description>VMWare NSX is a network virtualization platform (frequently paired with the vSphere IaaS (Infrastructure as a Service)). It includes features such as Load Balancers (LBs) and firewall rules, features often found in public-facing IaaSes (e.g. AWS (Amazon Web Services), GCE (Google Compute Engine), and Microsoft Azure) but not native to vSphere.
BOSH, a VM orchestrator, includes hooks to interoperate with NSX&amp;rsquo;s LB and Distributed Firewall features. These hooks enable BOSH to attach created VMs to existing NSX Load Balancer Pools and NSX Distributed Firewall rulesets.</description>
    </item>
    
    <item>
      <title>How to Customize a BOSH Stemcell</title>
      <link>https://blog.nono.io/post/bosh-customize-stemcell/</link>
      <pubDate>Fri, 23 Sep 2016 05:35:01 -0700</pubDate>
      
      <guid>https://blog.nono.io/post/bosh-customize-stemcell/</guid>
      <description>In this blog post, we describe the procedure we followed in order to create a custom Google Compute Engine (GCE) stemcell with a user cunnie whose ~/.ssh/authorized_keys is pre-populated with a specific public key.
Customizing stemcells is highly discouraged — it voids your warranty, and opens a host of problems which will only cause pain. This post is intended as an educational demonstration of the stemcell building process. You have been warned.</description>
    </item>
    
    <item>
      <title>Updating a BOSH Release</title>
      <link>https://blog.nono.io/post/updating-a-bosh-release/</link>
      <pubDate>Sun, 18 Sep 2016 10:15:30 -0700</pubDate>
      
      <guid>https://blog.nono.io/post/updating-a-bosh-release/</guid>
      <description>When PowerDNS released version 4.0.1 of their authoritative nameserver, we rushed to update our BOSH Release (which was at version 4.0.0). We thought it would be a walk in the park, but instead it was an epic fail (a final release which couldn&amp;rsquo;t be deployed because the blobs were broken).
In this blog post we describe the procedure we ultimately followed to successfully create an updated BOSH final release of version 4.</description>
    </item>
    
    <item>
      <title>Concourse has Badges</title>
      <link>https://blog.nono.io/post/concourse-badges/</link>
      <pubDate>Thu, 01 Sep 2016 17:29:49 -0700</pubDate>
      
      <guid>https://blog.nono.io/post/concourse-badges/</guid>
      <description>The Concourse Continuous Integration (CI) server has an API endpoint that displays a badge which shows health of your project:
http(s)://concourse-server/api/v1/pipelines/pipeline-name/jobs/job-name/badge
0. Abstract Open Source projects that have CI (e.g. Bootstrap, Node.js) often feature status badges (also known as images or icons) to display the health of their projects. CI servers such as Travis CI offer status badges. Concourse CI also offers status badges.
The status badge is a Scalable Vector Graphics (SVG) image available from the Concourse API.</description>
    </item>
    
    <item>
      <title>Concourse without a Load Balancer</title>
      <link>https://blog.nono.io/post/concourse-no-elb/</link>
      <pubDate>Fri, 26 Aug 2016 06:58:07 -0700</pubDate>
      
      <guid>https://blog.nono.io/post/concourse-no-elb/</guid>
      <description>Abstract Concourse is a continuous integration (CI) server. It can be deployed manually or via BOSH.
In this blog post, we describe the BOSH deployment of a Concourse CI server to natively accept Secure Sockets Layer (SSL) connections without using a load balancer. This may reduce the complexity and cost [ELB-pricing] of a Concourse deployment.
2016-09-12: This blog post is obsolete. Newer (v2.0.0+) versions of Concourse allow binding to the privileged ports 80 and 443, eliminating the need for an nginx proxy.</description>
    </item>
    
    <item>
      <title>How to Deploy a Multi-homed BOSH Director to a vSphere Environment</title>
      <link>https://blog.nono.io/post/multi-homed-bosh-director/</link>
      <pubDate>Fri, 13 May 2016 13:37:44 -0700</pubDate>
      
      <guid>https://blog.nono.io/post/multi-homed-bosh-director/</guid>
      <description>vSphere users ask, &amp;ldquo;How do I configure my BOSH director so that it can communicate with my vCenter but the director&amp;rsquo;s deployed VMs can&amp;rsquo;t?&amp;rdquo;
One technique is to use a multi-homed BOSH director combined with the BOSH Networking Release (a BOSH release which enables customization of the VM&amp;rsquo;s routing table, allowing more routes than the default gateway).
Network Diagram The following is a network diagram of our deployment. We want to protect the assets at the top (in blue): the vCenter server and its associated ESXi servers.</description>
    </item>
    
    <item>
      <title>The World&#39;s Smallest Concourse CI Server</title>
      <link>https://blog.nono.io/post/worlds-smallest-concourse-server/</link>
      <pubDate>Sat, 24 Oct 2015 13:52:48 -0700</pubDate>
      
      <guid>https://blog.nono.io/post/worlds-smallest-concourse-server/</guid>
      <description>[2016-04-06: This Blog Post is out-of-date; Please refer to the official Concourse documentation for instructions how to install a Concourse server] Continuous Integration (CI) is often used in conjunction with test-driven development (TDD); however, CI servers often bring their own set of challenges: they are usually &amp;ldquo;snowflakes&amp;rdquo;, uniquely configured machines that are difficult to upgrade, re-configure, or re-install. [snowflakes]
In this blog post, we describe deploying a publicly-accessible, lean (1GB RAM, 1 vCPU, 15GB disk) Concourse CI server using a 350-line manifest.</description>
    </item>
    
  </channel>
</rss>
