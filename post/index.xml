<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Brian Cunnie&#39;s Technical Blog</title>
    <link>https://blog.nono.io/post/</link>
    <description>Recent content in Posts on Brian Cunnie&#39;s Technical Blog</description>
    <image>
      <title>Brian Cunnie&#39;s Technical Blog</title>
      <url>https://nono.io/images/brian_cunnie_profile.jpg</url>
      <link>https://nono.io/images/brian_cunnie_profile.jpg</link>
    </image>
    <generator>Hugo -- 0.140.2</generator>
    <language>en</language>
    <lastBuildDate>Mon, 01 Jan 2024 14:51:43 -0800</lastBuildDate>
    <atom:link href="https://blog.nono.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Debugging the vSphere API via the BOSH vSphere CPI from Your Workstation</title>
      <link>https://blog.nono.io/post/debugging_vsphere_cpi_api/</link>
      <pubDate>Mon, 01 Jan 2024 14:51:43 -0800</pubDate>
      <guid>https://blog.nono.io/post/debugging_vsphere_cpi_api/</guid>
      <description>&lt;h3 id=&#34;this-blog-post-is-not-for-you&#34;&gt;This Blog Post Is Not For You&lt;/h3&gt;
&lt;p&gt;This blog post is directed towards people who are working with the BOSH vSphere
CPI (Cloud Provider Interface), which is not you. There are more
interesting things to read. If you want suggestions, try
&lt;em&gt;&lt;a href=&#34;https://www.poetryfoundation.org/poems/45392/ulysses&#34;&gt;Ulysses&lt;/a&gt;&lt;/em&gt; by Sir Alfred
Lord Tennyson, a poem about an aged hero seeking to recapture his adventures of
youth.&lt;/p&gt;
&lt;h3 id=&#34;challenge-extending-the-size-of-the-root-disk&#34;&gt;Challenge: Extending the Size of the Root Disk&lt;/h3&gt;
&lt;p&gt;I&amp;rsquo;d like to extend the size of the root partition, but it&amp;rsquo;s a challenge: if I
don&amp;rsquo;t make exactly the correct vSphere API calls, my changes will fail, and the
feedback loop is slow (hot-patching a BOSH Director and then running a deploy
takes at least 5 minutes each attempt).&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to Install a TLS Certificate on NSX 4.1</title>
      <link>https://blog.nono.io/post/nsx_tls/</link>
      <pubDate>Sun, 02 Jul 2023 08:05:37 -0800</pubDate>
      <guid>https://blog.nono.io/post/nsx_tls/</guid>
      <description>&lt;p&gt;If you don&amp;rsquo;t like seeing the &amp;ldquo;Your connection is not private&amp;rdquo; or &amp;ldquo;Warning: Potential Security Risk Ahead&amp;rdquo; when you browse to your NSX Manager, then you may want to install a TLS certificate from a commercial CA (Certificate Authority). This post tells you how.&lt;/p&gt;
&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;https://github.com/cunnie/cunnie.github.io/assets/1020675/971b47cc-dc82-4c26-9adc-6b3fa20883c3&#34;
         alt=&#34;NSX Manager certificate hierarchy&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;This NSX manager has a certificate from issued from Sectigo. Note that the padlock in the address bar shows no warning and the certificate&amp;rsquo;s chain-of-trust can be examined.&lt;/p&gt;</description>
    </item>
    <item>
      <title>On-premise is Almost Four Times Cheaper * than the Cloud</title>
      <link>https://blog.nono.io/post/on-premise_vs_cloud/</link>
      <pubDate>Wed, 04 Jan 2023 19:42:50 -0800</pubDate>
      <guid>https://blog.nono.io/post/on-premise_vs_cloud/</guid>
      <description>&lt;p&gt;&lt;em&gt;* If you don&amp;rsquo;t count the amount of time spent maintaining the on-premise equipment.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;My 48-VM (virtual machine) homelab configuration costs me approximately
&lt;strong&gt;$430/month&lt;/strong&gt; in hardware, electricity, virtualization software, and internet,
but an equivalent configuration on AWS (Amazon Web Services) would cost
&lt;strong&gt;$1,660/month&lt;/strong&gt; (almost four times as expensive)!&lt;/p&gt;
&lt;p&gt;Disclosures:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I work for VMware, which sells on-premise virtualization software (i.e.
vSphere).&lt;/li&gt;
&lt;li&gt;I didn&amp;rsquo;t put a dollar value on the time spent maintaining on-premise because
I had a hard time assigning a dollar value. For one thing, I don&amp;rsquo;t track how
much time I spend maintaining my on-premise equipment. For another, I enjoy
maintaining on-premise equipment, so it doesn&amp;rsquo;t feel like work.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;shortcomings-of-on-premise&#34;&gt;Shortcomings of On-Premise&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Time and Effort&lt;/strong&gt;: Before you leap into on-premise, you need to ask
yourself the following, &amp;ldquo;Am I interested, and do I have the time, to maintain
my own infrastructure?&amp;rdquo; If you like swapping out broken hard drives,
troubleshooting failed power supplies, creating VLANs, building firewalls,
configuring backups, and flashing BIOS—if you like getting your hands
dirty—then on-premise is for you.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Only one IPv4 address&lt;/strong&gt;: This is a big drawback. Who gets the sole IPv4
(73.189.219.4) address&amp;rsquo;s HTTPS port—the Kubernetes cluster or the Cloud
Foundry foundation? In my case, the Cloud Foundry foundation won that battle.
On the IPv6 front there&amp;rsquo;s no scarcity: Xfinity has allocated me
2601:646&amp;#x1f4af;69f0/60 (eight /64 subnets!).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Poor upload speed&lt;/strong&gt;: Although my Xfinity download speed at 1.4 Gbps can
rival the cloud VMs&amp;rsquo;, the anemic 40 Mbps upload speed can&amp;rsquo;t. I don&amp;rsquo;t host
large files on my on-premise home lab. This may not be a problem if your
internet connection has symmetric speeds (e.g. fiber).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: I can&amp;rsquo;t easily scale up my home lab. For example, my 15 amp
outlet won&amp;rsquo;t support more than what it already has (2 ESXi hosts, 1 TrueNAS
ZFS fileserver, two switches, an access point, a printer). Similarly, my
modestly-sized San Francisco apartment&amp;rsquo;s closet doesn&amp;rsquo;t have room to
accommodate additional hardware.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Widespread outages&lt;/strong&gt;: When I upgraded my TrueNAS ZFS fileserver that
supports the VMs, I had to power-off every single VM. Only then could I
safely upgrade the fileserver.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ground-up Rebuilds&lt;/strong&gt;: One time I made the mistake of &lt;em&gt;not&lt;/em&gt; powering down my
48 VMs before rebooting my fileserver, and I spent a significant portion of
my winter break recovering corrupted VMs (re-installing vCenter, rebuilding
my Unifi console from scratch).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-i-calculated-the-aws-costs&#34;&gt;How I Calculated the AWS Costs&lt;/h3&gt;
&lt;p&gt;First, I pulled a list of my VMs and their hardware configuration (number of CPUs (cores), amount of RAM (Random Access Memory))
I used the following &lt;a href=&#34;https://github.com/vmware/govmomi/blob/main/govc/README.md&#34;&gt;&lt;code&gt;govc&lt;/code&gt;&lt;/a&gt; command:&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Least Secure Way to Back Up vCenter 8.0 with TrueNAS 13.0</title>
      <link>https://blog.nono.io/post/backup_vcenter_w_truenas/</link>
      <pubDate>Mon, 02 Jan 2023 16:34:10 -0800</pubDate>
      <guid>https://blog.nono.io/post/backup_vcenter_w_truenas/</guid>
      <description>&lt;p&gt;We&amp;rsquo;re going to set up automated backups for a vCenter which we were forced to
rebuild over the winter break because the unexpected reboot of the file server
hosting the iSCSI datastore backing the vCenter&amp;rsquo;s disk drive caused
unrecoverable database corruption, and we had no backups.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Log into your &lt;a href=&#34;https://www.truenas.com/&#34;&gt;TrueNAS&lt;/a&gt; server via its web
interface, e.g. &lt;a href=&#34;https://nas.nono.io&#34;&gt;https://nas.nono.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Browse to &amp;ldquo;Services&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Start FTP (by toggling the &amp;ldquo;Running&amp;rdquo; slider) and configure it to start
automatically&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;https://user-images.githubusercontent.com/1020675/210378500-cee6db21-5ec0-40f4-a58a-a7e6484e2018.png&#34;
         alt=&#34;TrueNAS Services Configuration Screen&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Remember to start the FTP service and configure it to start automatically. Once that&amp;rsquo;s done, you can configure it by clicking on the ✎ icon in the Actions column.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Creating Multi-Platform Docker Images with Concourse</title>
      <link>https://blog.nono.io/post/multi-platform_docker_images_with_concourse/</link>
      <pubDate>Fri, 25 Nov 2022 08:13:55 -0800</pubDate>
      <guid>https://blog.nono.io/post/multi-platform_docker_images_with_concourse/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://concourse-ci.org/&#34;&gt;Concourse CI/CD&lt;/a&gt; (continuous integration/continuous
delivery) can create multi-platform Docker images. This blog post describes how.&lt;/p&gt;
&lt;p&gt;A multi-platform docker image is one that contains &amp;ldquo;&lt;a href=&#34;https://docs.docker.com/build/building/multi-platform/&#34;&gt;variants for
different architectures&lt;/a&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Docker images are often created for a single architecture (&amp;ldquo;instruction set
architecture&amp;rdquo; or
&amp;ldquo;&lt;a href=&#34;https://en.wikipedia.org/wiki/Instruction_set_architecture&#34;&gt;ISA&lt;/a&gt;&amp;rdquo;), typically
Intel&amp;rsquo;s/AMD&amp;rsquo;s &lt;a href=&#34;https://en.wikipedia.org/wiki/X86-64&#34;&gt;x86-64&lt;/a&gt;, but with the
advent of ARM64-based offerings such as AWS&amp;rsquo;s
&lt;a href=&#34;https://aws.amazon.com/ec2/graviton/&#34;&gt;Graviton&lt;/a&gt; and Apple&amp;rsquo;s
&lt;a href=&#34;https://en.wikipedia.org/wiki/Apple_M1&#34;&gt;M1&lt;/a&gt;/&lt;a href=&#34;https://en.wikipedia.org/wiki/Apple_M2&#34;&gt;M2&lt;/a&gt;,
It&amp;rsquo;s becoming more common to build multi-platform images to avoid the heavy
emulation performance penalty (typically &amp;gt;10x) when running an image on a
different architecture. Multi-platform images enable a developer, for example,
to run a container just as fast on their Apple M1 laptop as their GCP (Google
Cloud Platform) Kubernetes cluster.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to Install a TLS Certificate on vCenter Server Appliance (VCSA) 8.0</title>
      <link>https://blog.nono.io/post/vcenter_8.0_tls/</link>
      <pubDate>Wed, 02 Nov 2022 10:16:22 +0000</pubDate>
      <guid>https://blog.nono.io/post/vcenter_8.0_tls/</guid>
      <description>&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;https://user-images.githubusercontent.com/1020675/199730887-ac22531d-8741-4120-b6c0-3e793bc00587.png&#34;/&gt; 
&lt;/figure&gt;

&lt;h2 id=&#34;quickstart&#34;&gt;Quickstart&lt;/h2&gt;
&lt;p&gt;First, create your key and your CSR (Certificate Signing Request). In the
following example, we are creating a CSR for our vCenter host,
&amp;ldquo;vcenter-80.nono.io&amp;rdquo;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;CN&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;vcenter-80.nono.io &lt;span style=&#34;color:#75715e&#34;&gt;# &amp;#34;CN&amp;#34; is the abbreviation for &amp;#34;Common Name&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;openssl genrsa -out $CN.key &lt;span style=&#34;color:#ae81ff&#34;&gt;3072&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;openssl req &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -new &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -key $CN.key &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -out $CN.csr &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -sha256 &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -subj &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/C=US/ST=California/L=San Francisco/O=nono.io/OU=homelab/CN=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;${&lt;/span&gt;CN&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;/emailAddress=brian.cunnie@gmail.com&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -config &amp;lt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;cat &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt;EOF
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;[ req ]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;distinguished_name = req_distinguished_name
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;req_extensions     = req_ext
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;[ req_distinguished_name ]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;[ req_ext ]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;subjectAltName = @alt_names
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;[alt_names]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;DNS.1   = ${CN}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You&amp;rsquo;ll have two files, &lt;code&gt;vcenter-80.nono.io.key&lt;/code&gt; and &lt;code&gt;vcenter-80.nono.io.csr&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tuning HAProxy in a vSphere Environment</title>
      <link>https://blog.nono.io/post/tuning_haproxy/</link>
      <pubDate>Sat, 10 Sep 2022 09:33:02 -0700</pubDate>
      <guid>https://blog.nono.io/post/tuning_haproxy/</guid>
      <description>&lt;!-- https://gohugo.io/content-management/shortcodes/#figure --&gt;
&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;https://docs.google.com/drawings/d/e/2PACX-1vRrDRY6cxfp4AOz2pEPSXHcM2KWnwtWnC_tSoQNwUkRKxs-oxdWo51GBxDaaR7dNfTYbB-gioM3bkjX/pub?w=1999&amp;amp;amp;h=662&#34;
         alt=&#34;Network Diagram&#34; width=&#34;100%&#34; height=&#34;100%&#34;/&gt; &lt;figcaption&gt;
            &lt;p&gt;Network Diagram. We want to maximize the throughput from the blue box (the client) to the green box (HAProxy)&lt;/p&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;p&gt;We were able to push through almost 450 MB/sec through HAProxy (which
terminated our SSL) by carefully matching our 4-core HAProxy with 2 x 4-core
Gorouters (which were on a &lt;em&gt;much&lt;/em&gt; slower ESXi host).&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: right&#34;&gt;Bandwidth MB/second&lt;/th&gt;
          &lt;th&gt;Configuration&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;201.27MB&lt;/td&gt;
          &lt;td&gt;1 HAProxy: 1 vCPU&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;136.47MB&lt;/td&gt;
          &lt;td&gt;1 HAProxy: 2 vCPUs&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;270.56MB&lt;/td&gt;
          &lt;td&gt;2 Gorouters: 1 vCPU&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;350.48MB&lt;/td&gt;
          &lt;td&gt;2 Gorouters: 2 vCPUs&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: right&#34;&gt;447.49MB&lt;/td&gt;
          &lt;td&gt;1 HAProxy, 2 Gorouters: 4 vCPUs&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;0-haproxy-with-1-vcpu&#34;&gt;0. HAProxy with 1 vCPU&lt;/h4&gt;
&lt;p&gt;HAProxy had only 1 vCPU during this iteration, and the CPU was maxed to 100%
during the test (according to &lt;code&gt;htop&lt;/code&gt;). We suspect that TLS was the culprit for
much of the traffic: HAProxy terminated TLS traffic inbound, and initiated TLS
to the Gorouters.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Underground Guide to Cloud Foundry Acceptance Tests</title>
      <link>https://blog.nono.io/post/underground_guide_to_cf_acceptance/</link>
      <pubDate>Mon, 04 Jul 2022 12:46:03 -0700</pubDate>
      <guid>https://blog.nono.io/post/underground_guide_to_cf_acceptance/</guid>
      <description>&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;https://user-images.githubusercontent.com/1020675/177218158-146d64ab-80fd-45e7-b116-fdff2da22620.png&#34;
         alt=&#34;Cloud Foundry logo&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/cloudfoundry/cf-acceptance-tests&#34;&gt;Cloud Foundry Acceptance
Tests&lt;/a&gt; are the gold
standard to test the proper functioning of your Cloud Foundry deployment. This
guide tells you how to run them. When in doubt, refer to the
&lt;a href=&#34;https://github.com/cloudfoundry/cf-acceptance-tests&#34;&gt;README&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&#34;quick-start&#34;&gt;Quick Start&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cd ~/workspace/
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git clone git@github.com:cloudfoundry/cf-acceptance-tests.git
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cd cf-acceptance-tests
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;. ./.envrc
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cp example-cats-config.json cats-config.json
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;export CONFIG&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;cats-config.json
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cf api api.cf.nono.io &lt;span style=&#34;color:#75715e&#34;&gt;# or whatever your Cloud Foundry&amp;#39;s API endpoint is&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cf login -u admin
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cf create-space -o system system &lt;span style=&#34;color:#75715e&#34;&gt;# don&amp;#39;t worry if it&amp;#39;s already created&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cf t -o system -s system
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cf enable-feature-flag diego_docker &lt;span style=&#34;color:#75715e&#34;&gt;# necessary if you&amp;#39;re running the Docker tests (`&amp;#34;include_docker&amp;#34;: true`)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cf enable-feature-flag service_instance_sharing &lt;span style=&#34;color:#75715e&#34;&gt;# necessary if you&amp;#39;re running the sharing tests (`&amp;#34;include_service_instance_sharing&amp;#34;: true`)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you don&amp;rsquo;t have the Cloud Foundry CLI (command line interface), follow the
&lt;a href=&#34;https://docs.cloudfoundry.org/cf-cli/install-go-cli.html&#34;&gt;installation
instructions&lt;/a&gt;.
Install the latest version (v8).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Concourse CI on Kubernetes (GKE), Part 6: Concourse &amp; Vault: Backup &amp; Restore</title>
      <link>https://blog.nono.io/post/concourse_on_k8s-6/</link>
      <pubDate>Sat, 08 Jan 2022 04:55:18 -0800</pubDate>
      <guid>https://blog.nono.io/post/concourse_on_k8s-6/</guid>
      <description>&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;https://upload.wikimedia.org/wikipedia/commons/2/29/Postgresql_elephant.svg&#34;
         alt=&#34;PostgreSQL logo&#34;/&gt; 
&lt;/figure&gt;

&lt;h3 id=&#34;recreating-the-cluster&#34;&gt;Recreating the Cluster&lt;/h3&gt;
&lt;p&gt;We want to recreate our cluster while &lt;strong&gt;preserving our Vault and Concourse
data&lt;/strong&gt; (we want to recreate our GKE regional cluster as a zonal cluster to take
advantage of the &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/pricing#cluster_management_fee_and_free_tier&#34;&gt;GKE free
tier&lt;/a&gt;
which saves us $74.40 per month).&lt;/p&gt;
&lt;p&gt;Note: when we say, &amp;ldquo;recreate the cluster&amp;rdquo;, we really mean, &amp;ldquo;recreate the
cluster&amp;rdquo;. We destroy the old cluster, including our worker nodes and persistent
volumes.&lt;/p&gt;
&lt;h3 id=&#34;backup-vault&#34;&gt;Backup Vault&lt;/h3&gt;
&lt;p&gt;In the following example, our storage path is &lt;code&gt;/vault/data&lt;/code&gt;, but there&amp;rsquo;s a
chance that yours is different. If it is, replace occurrences of &lt;code&gt;/vault/data&lt;/code&gt;
with your storage path:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Disk Controller Benchmarks: VMware Paravirtual&#39;s vs. LSI Logic Parallel&#39;s</title>
      <link>https://blog.nono.io/post/pvscsi/</link>
      <pubDate>Fri, 19 Nov 2021 08:12:28 -0700</pubDate>
      <guid>https://blog.nono.io/post/pvscsi/</guid>
      <description>&lt;p&gt;Is it worth switching your VMware vSphere VM&amp;rsquo;s SCSI (small computer system
interface) from the LSI Logic Parallel controller to the VMware Paravirtual SCSI
controller? Except for ultra-high-end database servers (&amp;gt; 1M IOPS ( input/output
operations per second)), the answer is &amp;ldquo;no&amp;rdquo;; the difference is negligible.&lt;/p&gt;
&lt;p&gt;Our benchmarks show that VMware&amp;rsquo;s Paravirtual SCSI (small computer system
interface) controller offered a 2-3% performance increase in IOPS (I/O
(input/output) operations per second) over the LSI Logic Parallel SCSI
controller at the cost of a similar decrease in sequential performance (both
read &amp;amp; write). Additionally the Paravirtual SCSI controller (pvscsi) had a
slight reduction in CPU (central processing unit) usage on the host (best-case
scenario is 3% lower CPU usage).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Concourse CI on Kubernetes (GKE), Part 5: Vault</title>
      <link>https://blog.nono.io/post/concourse_on_k8s-5/</link>
      <pubDate>Thu, 18 Nov 2021 03:46:53 -0800</pubDate>
      <guid>https://blog.nono.io/post/concourse_on_k8s-5/</guid>
      <description>&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;https://d1q6f0aelx0por.cloudfront.net/product-logos/library-vault-logo.png&#34;
         alt=&#34;Vault logo&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;In our previous post, we configured our GKE Concourse CI server, which was the
capstone of the series. But we were wrong: &lt;em&gt;this post&lt;/em&gt; is the capstone in the
series. In this post, we install Vault and configure our Concourse CI server to
use Vault to retrieve secrets.&lt;/p&gt;
&lt;h3 id=&#34;installation&#34;&gt;Installation&lt;/h3&gt;
&lt;p&gt;Most of these instructions are derived from the Hashicorp tutorial, &lt;em&gt;&lt;a href=&#34;https://learn.hashicorp.com/tutorials/vault/kubernetes-raft-deployment-guide?in=vault/kubernetes&#34;&gt;Vault on
Kubernetes Deployment
Guide&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Create a DNS A record which points to the IP address of your GKE load balancer.
In our case, we created &lt;code&gt;vault.nono.io&lt;/code&gt; which points to &lt;code&gt;104.155.144.4&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Concourse CI on Kubernetes (GKE), Part 4: Concourse</title>
      <link>https://blog.nono.io/post/concourse_on_k8s-4/</link>
      <pubDate>Wed, 01 Sep 2021 18:39:26 -0700</pubDate>
      <guid>https://blog.nono.io/post/concourse_on_k8s-4/</guid>
      <description>&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;https://pbs.twimg.com/profile_images/971772210821070848/jnsjSQcw_400x400.jpg&#34;
         alt=&#34;Concourse logo&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;In our previous post, we configured our GKE (Google Kubernetes Engine) to
use Let&amp;rsquo;s Encrypt TLS certificates. In this post, the capstone of our series, we
install Concourse CI.&lt;/p&gt;
&lt;h3 id=&#34;installation&#34;&gt;Installation&lt;/h3&gt;
&lt;p&gt;These instructions are a more-opinionated version of the canonical instructions
for the Concourse CI Helm chart found here:
&lt;a href=&#34;https://github.com/concourse/concourse-chart&#34;&gt;https://github.com/concourse/concourse-chart&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&#34;first-install-with-helm&#34;&gt;First Install: with Helm&lt;/h4&gt;
&lt;p&gt;We use &lt;code&gt;helm&lt;/code&gt; to install Concourse. We first add the Helm repo, and then install
it. We take the opportunity to bump the default login time from 24 hours to ten
days (&lt;code&gt;duration=240h&lt;/code&gt;) because we hate re-authenticating to our Concourse every
morning.  &lt;strong&gt;Replace &lt;code&gt;gke.nono.io&lt;/code&gt; with your DNS record&lt;/strong&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Concourse CI on Kubernetes (GKE), Part 3: TLS</title>
      <link>https://blog.nono.io/post/concourse_on_k8s-3/</link>
      <pubDate>Wed, 11 Aug 2021 16:55:31 -0700</pubDate>
      <guid>https://blog.nono.io/post/concourse_on_k8s-3/</guid>
      <description>&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;https://letsencrypt.org/images/letsencrypt-logo-horizontal.svg&#34;
         alt=&#34;Let&amp;#39;s Encrypt logo&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;In our previous blog post, we configured ingress to our Kubernetes cluster but
were disappointed to discover that the TLS certificates were self-signed. In
this post we&amp;rsquo;ll remedy that by installing cert-manager, the Cloud native
certificate management tool.&lt;/p&gt;
&lt;p&gt;Disclaimer: most of this blog post was lifted whole cloth from the
most-excellent &lt;a href=&#34;https://cert-manager.io/docs/&#34;&gt;cert-manager documentation&lt;/a&gt;.
We merely condensed it &amp;amp; made it more opinionated.&lt;/p&gt;
&lt;h3 id=&#34;installation&#34;&gt;Installation&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s add the Jetstack Helm Repository:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Concourse CI on Kubernetes (GKE), Part 2: Ingress</title>
      <link>https://blog.nono.io/post/concourse_on_k8s-2/</link>
      <pubDate>Sat, 07 Aug 2021 14:49:17 -0700</pubDate>
      <guid>https://blog.nono.io/post/concourse_on_k8s-2/</guid>
      <description>&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;https://www.nginx.com/wp-content/uploads/2018/08/NGINX-logo-rgb-large.png&#34;
         alt=&#34;nginx logo&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;In our previous blog post, we set up our Kubernetes cluster and deployed a pod
running nginx, but the experience was disappointing—we couldn&amp;rsquo;t browse to our
pod. Let&amp;rsquo;s fix that by deploying the nginx Ingress controller.&lt;/p&gt;
&lt;h3 id=&#34;acquire-the-external-ip-address-elastic-ip&#34;&gt;Acquire the External IP Address (Elastic IP)&lt;/h3&gt;
&lt;p&gt;We&amp;rsquo;ll use the &lt;a href=&#34;https://console.cloud.google.com&#34;&gt;Google Cloud console&lt;/a&gt; to
acquire the external address
&lt;sup&gt;[&lt;a href=&#34;https://blog.nono.io/post/concourse_on_k8s-2/#external_address&#34;&gt;external address&lt;/a&gt;]&lt;/sup&gt;
for our load balancer.&lt;/p&gt;
&lt;p&gt;Navigate to &lt;em&gt;VPC network → External IP addresses → Reserve Static Address:&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Concourse CI on Kubernetes (GKE), Part 1: Terraform</title>
      <link>https://blog.nono.io/post/concourse_on_k8s-1/</link>
      <pubDate>Fri, 06 Aug 2021 16:38:00 -0700</pubDate>
      <guid>https://blog.nono.io/post/concourse_on_k8s-1/</guid>
      <description>&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;https://d15shllkswkct0.cloudfront.net/wp-content/blogs.dir/1/files/2020/06/gke.png&#34;
         alt=&#34;GKE logo&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;Let&amp;rsquo;s deploy &lt;a href=&#34;https://concourse-ci.org/&#34;&gt;Concourse&lt;/a&gt;, a continuous-integration,
continuous delivery (CI/CD) application (similar to
&lt;a href=&#34;https://www.jenkins.io/&#34;&gt;Jenkins&lt;/a&gt; and &lt;a href=&#34;https://circleci.com/&#34;&gt;CircleCI&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll deploy it to &lt;a href=&#34;https://console.cloud.google.com/&#34;&gt;Google Cloud&lt;/a&gt;, to our
&lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt; (GKE).&lt;/p&gt;
&lt;p&gt;In this post, we&amp;rsquo;ll use &lt;a href=&#34;https://www.hashicorp.com/&#34;&gt;HashiCorp&lt;/a&gt;&amp;rsquo;s
&lt;a href=&#34;https://www.hashicorp.com/products/terraform&#34;&gt;Terraform&lt;/a&gt; to create our cluster.&lt;/p&gt;
&lt;p&gt;We assume you&amp;rsquo;ve already installed the terraform command-line interface (CLI)
and created a Google Cloud account.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mkdir -p ~/workspace/gke
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cd ~/workspace/gke
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next we download the terraform templates and terraform vars file:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl -OL https://raw.githubusercontent.com/cunnie/deployments/6b230118399f4326094b4d60e21cda32e8c6f321/terraform/gcp/gke/gke.tf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl -OL https://raw.githubusercontent.com/cunnie/deployments/6b230118399f4326094b4d60e21cda32e8c6f321/terraform/gcp/gke/vpc.tf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl -OL https://raw.githubusercontent.com/cunnie/deployments/6b230118399f4326094b4d60e21cda32e8c6f321/terraform/gcp/gke/terraform.tfvars
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl -OL https://raw.githubusercontent.com/cunnie/deployments/6b230118399f4326094b4d60e21cda32e8c6f321/terraform/gcp/gke/outputs.tf
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;At this point we hear cries of protest, &amp;ldquo;What?! Downloading dubious files
from sketchy software developers on the internet? Files whose provenance is
murky at best?&amp;rdquo;&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Old Blog is Dead. Long Live the New Blog!</title>
      <link>https://blog.nono.io/post/why_new_blog/</link>
      <pubDate>Sun, 18 Jul 2021 10:34:36 -0700</pubDate>
      <guid>https://blog.nono.io/post/why_new_blog/</guid>
      <description>&lt;p&gt;Why am I creating a new blog? What was wrong with the old blog? Why don&amp;rsquo;t I use &lt;a href=&#34;https://medium.com&#34;&gt;Medium&lt;/a&gt;?&lt;/p&gt;
&lt;p&gt;The short version: The old blog is frozen in time, like a prince caught in amber
&lt;sup&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Nine_Princes_in_Amber&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; or a dandy
in aspic &lt;sup&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/A_Dandy_in_Aspic&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. I can no
longer post to it.&lt;/p&gt;
&lt;p&gt;The old blog, the &lt;em&gt;&lt;a href=&#34;https://tanzu.vmware.com/content/pivotal-engineering-journal&#34;&gt;Pivotal Engineering
Journal&lt;/a&gt;&lt;/em&gt;, which
many of Pivotal&amp;rsquo;s engineers contributed to, was archived a year after VMware
acquired Pivotal. Every acquisition brings changes, and this was, in the scheme
of things, a very minor one. At least VMware kept the blog instead of simply
discarding it.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Flow Your Tests Like Your Code</title>
      <link>https://blog.nono.io/post/go-flow-tests-like-code/</link>
      <pubDate>Sun, 16 Feb 2020 17:16:22 +0000</pubDate>
      <guid>https://blog.nono.io/post/go-flow-tests-like-code/</guid>
      <description>&lt;p&gt;My co-worker Belinda Liu turned to me and said, &amp;ldquo;I don&amp;rsquo;t like these tests at all;
they&amp;rsquo;re hard to follow, and I&amp;rsquo;m not sure what they&amp;rsquo;re testing.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;I looked at the tests that I had spent much of yesterday afternoon working on.
She was right: they &lt;em&gt;were&lt;/em&gt; hard to follow (even for me, who had written some of
them!).&lt;/p&gt;
&lt;p&gt;How had we gotten here? Our code was straightforward, but our tests were
byzantine (excessively complicated). We identified two problems:&lt;/p&gt;</description>
    </item>
    <item>
      <title>How To Enable IPv6 on Your Cloud Foundry&#39;s HAProxy</title>
      <link>https://blog.nono.io/post/haproxy-ipv6/</link>
      <pubDate>Sat, 01 Feb 2020 01:16:22 +0000</pubDate>
      <guid>https://blog.nono.io/post/haproxy-ipv6/</guid>
      <description>&lt;h3 id=&#34;0-abstract&#34;&gt;0. Abstract&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://www.haproxy.org/&#34;&gt;HAProxy&lt;/a&gt; is an optional load balancer included in
the canonical open source &lt;a href=&#34;https://github.com/cloudfoundry/cf-deployment&#34;&gt;Cloud Foundry
deployment&lt;/a&gt;. Its intended use is
on IaaSes (Infrastructures as a Service) that do not offer built-in load
balancers [0]. On vSphere, this means without the optional network
virtualization solutions, NSX-T and NSX-V. This blog post describes how to
assign an IPv6 address to an HAProxy load balancer in a Cloud Foundry
deployment.&lt;/p&gt;
&lt;h3 id=&#34;1-pre-requisites&#34;&gt;1. Pre-requisites&lt;/h3&gt;
&lt;p&gt;Users following this blog post should be familiar with BOSH, BOSH&amp;rsquo;s manifest
operations files, IPv6, and deploying Cloud Foundry using &lt;em&gt;cf-deployment&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A High-performing Mid-range NAS Server, Part 3: 10 GbE</title>
      <link>https://blog.nono.io/post/nas-performance-tuning/</link>
      <pubDate>Sat, 25 May 2019 17:16:22 +0000</pubDate>
      <guid>https://blog.nono.io/post/nas-performance-tuning/</guid>
      <description>&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;&amp;ldquo;How much faster will my VM&amp;rsquo;s disks be if I upgrade my
&lt;a href=&#34;https://en.wikipedia.org/wiki/ZFS&#34;&gt;ZFS&lt;/a&gt;-based (Z File System) NAS to 10 GbE?&amp;rdquo;
The disks will be faster, in some cases, much faster. Our experience is that
sequential read throughput will be 1.4✕ faster, write throughput, 10✕ faster,
and IOPS, 1.6✕ faster.&lt;/p&gt;
&lt;p&gt;We ran a three-hour benchmark on our NAS server before and after upgrading to 10
GbE. We ran the benchmark again after upgrading. The benchmark looped through
write-read-iops tests continuously.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Transferring Time-based One-time Passwords to a New Smartphone</title>
      <link>https://blog.nono.io/post/totp/</link>
      <pubDate>Mon, 21 Jan 2019 11:16:22 +0000</pubDate>
      <guid>https://blog.nono.io/post/totp/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Smartphone authenticator apps such as &lt;a href=&#34;https://play.google.com/store/apps/details?id=com.google.android.apps.authenticator2&#34;&gt;Google
Authenticator&lt;/a&gt;
and &lt;a href=&#34;https://authy.com/&#34;&gt;Authy&lt;/a&gt; implement software tokens that are &lt;a href=&#34;https://en.wikipedia.org/wiki/Google_Authenticator&#34;&gt;&amp;ldquo;two-step
verification services using the Time-based One-time Password Algorithm (TOTP)
and HMAC-based One-time Password algorithm
(HOTP)&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Smartphone TOTP, a form of Two-factor authentication (2FA), displays a 6-digit
code derived from a shared secret, updating every thirty seconds.&lt;/p&gt;
&lt;p&gt;The shared secret is presented only once to the user, typically with a &lt;a href=&#34;https://en.wikipedia.org/wiki/QR_code&#34;&gt;QR
(Quick Response) Code&lt;/a&gt; which is scanned
by the authenticator app.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Troubleshooting Obscure OpenSSH Failures</title>
      <link>https://blog.nono.io/post/ssh_handshake_failed/</link>
      <pubDate>Wed, 28 Nov 2018 17:16:22 +0000</pubDate>
      <guid>https://blog.nono.io/post/ssh_handshake_failed/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;By using tcpdump to troubleshoot an elusive error, we uncovered a
man-in-the-middle (MITM) ssh proxy installed by our information security
(InfoSec) team to harden/protect a set of machines which were accessible
from the internet. The ssh proxy in question was Palo Alto Network’s
(PAN) Layer 7 (i.e. it worked on any port, not solely ssh’s port 22)
proxy, and was discovered when we observed a failure to negotiate
ciphers during the ssh key exchange.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Safely Upgrading PAS 2.2 with NSX-T Load Balancers</title>
      <link>https://blog.nono.io/post/upgrade_2.2-2.3_on_nsx-t/</link>
      <pubDate>Thu, 06 Sep 2018 10:44:30 +0000</pubDate>
      <guid>https://blog.nono.io/post/upgrade_2.2-2.3_on_nsx-t/</guid>
      <description>&lt;p&gt;When customers with vSphere+NSX-T-based foundations apply a stemcell update,
update a tile, or upgrade PAS (Pivotal Application Service) from 2.2 to 2.3,
their Cloud Foundry may become unreachable as their NSX-T static load balancer
server pools have been emptied.&lt;/p&gt;
&lt;p&gt;This blog post describes a method to ensure availability during upgrades. We
use a combination of customized Operations Manager &lt;a href=&#34;https://docs.pivotal.io/pivotalcf/2-2/customizing/config-er-vmware.html#resources&#34;&gt;resource
configs&lt;/a&gt;
and BOSH &lt;a href=&#34;https://bosh.io/docs/terminology/#vm-extension&#34;&gt;VM Extensions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The sample workflow in this post is for upgrading PAS 2.2 to PAS 2.3 with an
Operations Manager upgrade; however, it can also be adapted to stemcell or
tile upgrades as well.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to Install a TLS Certificate on vCenter Server Appliance (VCSA) 6.7 [Updated for vCenter 7]</title>
      <link>https://blog.nono.io/post/vcenter_6.7_tls/</link>
      <pubDate>Wed, 09 May 2018 23:16:22 +0000</pubDate>
      <guid>https://blog.nono.io/post/vcenter_6.7_tls/</guid>
      <description>&lt;!--
Install root certificate
Install chain including root but not including vcenter certificate
install vcenter certificate
install vcenter key
--&gt;
&lt;div class=&#34;alert alert-success&#34; role=&#34;alert&#34;&gt;
&lt;p&gt;The following section is the new Quickstart for installing a TLS certificate on vCenter 7&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&#34;vcenter-7-quickstart&#34;&gt;vCenter 7 Quickstart&lt;/h2&gt;
&lt;p&gt;On your vCenter, navigate to &lt;strong&gt;Menu → Administration → Certificates → Certificate Management&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;On the &lt;em&gt;__MACHINE_CERT&lt;/em&gt; tile, click &lt;strong&gt;Actions&lt;/strong&gt;, select &lt;strong&gt;Generate Certificate
Signing Request (CSR)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Enter the appropriate info; for inspiration, this is what we entered:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Benchmarking the Disk Speed of IaaSes</title>
      <link>https://blog.nono.io/post/gobonniego_results/</link>
      <pubDate>Fri, 16 Mar 2018 20:00:22 +0000</pubDate>
      <guid>https://blog.nono.io/post/gobonniego_results/</guid>
      <description>&lt;h2 id=&#34;0-overview&#34;&gt;0. Overview&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;[Disclaimer: the author works for Pivotal Software, of which Dell is an
investor. Dell is also an owner of VMware]&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s helpful to know the performance characteristics of disks when selecting a
disk type. For example, the performance of a database server will be greatly
affected by the &lt;a href=&#34;https://en.wikipedia.org/wiki/IOPS&#34;&gt;IOPS&lt;/a&gt; of the underlying
storage. Similarly, a video-streaming server will be affected by the
underlying read throughput.&lt;/p&gt;
&lt;h3 id=&#34;00-highlights&#34;&gt;0.0 Highlights:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If you need a fast disk, nothing beats a local vSphere NVMe drive. Nothing.
Whether its IOPS, read throughput, or write throughput, NVMe is the
winner hands down.&lt;/li&gt;
&lt;li&gt;Google&amp;rsquo;s SSD (Solid State Drive) storage has 22× the IOPS of its standard
storage. For general purpose use, always go with the SSD; however, if you&amp;rsquo;re
doing streaming (long reads or writes), the standard storage may be the better
(and cheaper) choice.&lt;/li&gt;
&lt;li&gt;AWS&amp;rsquo;s io1 disk is a waste of money unless you need an IOPS &amp;gt; 4k (the
gp2 disk has an IOPS of ~4k). AWS&amp;rsquo;s now-deprecated standard storage
has a decent IOPS of ~2k.&lt;/li&gt;
&lt;li&gt;The key to getting IOPS out of Azure is to enable &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/virtual-machines/windows/premium-storage-performance#disk-caching&#34;&gt;Host Disk
Caching&lt;/a&gt;,
which can catapult an anemic 120 IOPS to a competitive 8k IOPS.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;01-metrics-iaases-and-results&#34;&gt;0.1 Metrics, IaaSes, and Results&lt;/h3&gt;
&lt;p&gt;In this blog post we record three metrics:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deploying BOSH VMs with IPv6 Addresses on vSphere</title>
      <link>https://blog.nono.io/post/bosh-on-ipv6-2/</link>
      <pubDate>Tue, 16 Jan 2018 19:12:22 +0000</pubDate>
      <guid>https://blog.nono.io/post/bosh-on-ipv6-2/</guid>
      <description>&lt;h2 id=&#34;0-abstract&#34;&gt;0. Abstract&lt;/h2&gt;
&lt;p&gt;BOSH is a VM orchestrator; a BOSH Director creates, configures, monitors, and
deletes VMs. The BOSH Director interoperates with a number of IaaSes
(Infrastructure as a Service), one of which is VMware vSphere, a virtualization
platform. BOSH traditionally operates exclusively within the IPv4 networking
space (i.e. the BOSH Director has an IPv4 address (e.g. 10.0.0.6), and the VMs
which it deploys also have IPv4 addresses); however, recent changes have
enabled IPv6 networking within the BOSH Framework.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Maintaining BOSH Directors with Concourse CI and bosh-deployment</title>
      <link>https://blog.nono.io/post/bosh-deployed-with-concourse/</link>
      <pubDate>Fri, 24 Nov 2017 14:06:25 +0000</pubDate>
      <guid>https://blog.nono.io/post/bosh-deployed-with-concourse/</guid>
      <description>&lt;p&gt;&lt;em&gt;&amp;ldquo;BOSH deploys Concourse, and Concourse deploys BOSH&amp;rdquo; —Cloud Foundry koan&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A BOSH Director is a VM (virtual machine) orchestrator which is itself a VM.
BOSH solves the problem of keeping its VMs&amp;rsquo; applications (operating systems
(stemcells) and releases) up-to-date with the command, &lt;code&gt;bosh deploy&lt;/code&gt;; however,
this begs the question, &amp;ldquo;what keeps the BOSH Director itself up-to-date?&amp;rdquo;.
&lt;sup&gt;&lt;a href=&#34;#quis&#34; class=&#34;alert-link&#34;&gt;[Quis custodiet?]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;We explore using &lt;a href=&#34;https://concourse.ci/&#34;&gt;Concourse&lt;/a&gt;, a Continuous Integration
(CI) server, and
&lt;a href=&#34;https://github.com/cloudfoundry/bosh-deployment/&#34;&gt;bosh-deployment&lt;/a&gt; &lt;sup&gt;&lt;a
href=&#34;#bosh_up_to_date&#34; class=&#34;alert-link&#34;&gt;[Updating BOSH]&lt;/a&gt;&lt;/sup&gt;, in order
to create a &lt;a href=&#34;https://github.com/cloudfoundry/bosh-deployment/&#34;&gt;Concourse
pipeline&lt;/a&gt; which updates, in
turn, a BOSH director on AWS (Amazon Web Services), on Microsoft Azure, and GCP
(Google Cloud Platform). Updating all three BOSH directors can be accomplished
with a single click. &lt;sup&gt;&lt;a href=&#34;#one_click&#34; class=&#34;alert-link&#34;&gt;[One
click]&lt;/a&gt;&lt;/sup&gt; Best of all, our directors are re-deployed with a recent
stemcell, BOSH release, and CPI release.  &lt;sup&gt;&lt;a href=&#34;#how_recent&#34;
class=&#34;alert-link&#34;&gt;[How recent?]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deploying a BOSH Director With SSL Certificates Issued by Commercial CA</title>
      <link>https://blog.nono.io/post/bosh-ssl/</link>
      <pubDate>Wed, 16 Aug 2017 17:16:22 +0000</pubDate>
      <guid>https://blog.nono.io/post/bosh-ssl/</guid>
      <description>&lt;h2 id=&#34;0-abstract&#34;&gt;0. Abstract&lt;/h2&gt;
&lt;p&gt;A BOSH director is a virtual machine (VM) orchestrator which deploys VMs to
various Infrastructures as a Service (IaaS) such as Amazon Web Services (AWS)
and Google Cloud Platform (GCP). The BOSH Command Line (CLI) communicates with
the director over Secure Sockets Layer (SSL). While most BOSH directors are
deployed with self-signed certificates, it is possible to configure a BOSH
director with certificates issued by a recognized certificate authority (CA)
(e.g. Comodo, Symantec, Let&amp;rsquo;s Encrypt). This blog post describes a technique to
deploy a BOSH director with a CA-issued SSL certificate.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deploy To vSphere NSX-T Opaque Networks Using BOSH</title>
      <link>https://blog.nono.io/post/bosh-vsphere-opaque-networks/</link>
      <pubDate>Mon, 17 Apr 2017 11:43:14 -0700</pubDate>
      <guid>https://blog.nono.io/post/bosh-vsphere-opaque-networks/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.vmware.com/products/vsphere.html&#34;&gt;VMware&amp;rsquo;s vSphere&lt;/a&gt; is an
Infrastructure as a Service (IaaS) which runs Virtual Machines (VMs).
&lt;a href=&#34;http://bosh.io/docs/about.html&#34;&gt;BOSH&lt;/a&gt; is a VM orchestrator which automates the
creation of VMs.  &lt;a href=&#34;https://www.vmware.com/support/pubs/nsxt_pubs.html&#34;&gt;NSX-T&lt;/a&gt;
is a pluggable Network backend for vSphere (and other hypervisors). NSX-T
allows the creation of opaque networks in vSphere, networks whose detail and
configuration of the network is unknown to vSphere and which is managed outside
vSphere.&lt;/p&gt;
&lt;p&gt;With the release of &lt;a href=&#34;http://bosh.io/releases/github.com/cloudfoundry-incubator/bosh-vsphere-cpi-release?all=1&#34;&gt;BOSH vSphere CPI
v40&lt;/a&gt;,
users can attach their BOSH-deployed VMs to an NSX-T opaque network.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Why Is My NTP Server Costing $500/Year? Part 3</title>
      <link>https://blog.nono.io/post/ntp-costs-500/</link>
      <pubDate>Sat, 28 Jan 2017 12:38:13 -0800</pubDate>
      <guid>https://blog.nono.io/post/ntp-costs-500/</guid>
      <description>&lt;p&gt;When &lt;a href=&#34;https://news.ycombinator.com/item?id=13249562&#34;&gt;Hacker News picked up Part
1&lt;/a&gt; of our series of blog posts on
running public NTP servers, a contributor said, &amp;ldquo;I wish he&amp;rsquo;d explained &amp;hellip; what
they ultimately did (since there&amp;rsquo;s no part 3 that I can find).&amp;rdquo;&lt;/p&gt;
&lt;p&gt;We had dropped the ball — we had never concluded the series, had never written
part 3, had never described the strategies to mitigate the data transfer costs.&lt;/p&gt;
&lt;p&gt;This blog post remedies that oversight; it consists of two parts: the first part
addresses strategies to reduce the cost of running an NTP server, and the second
part discusses side topics (aspects of running an NTP server).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using the beta BOSH CLI to Deploy an IPv6-enabled nginx Server to AWS</title>
      <link>https://blog.nono.io/post/bosh-on-ipv6/</link>
      <pubDate>Tue, 20 Dec 2016 20:49:50 +0000</pubDate>
      <guid>https://blog.nono.io/post/bosh-on-ipv6/</guid>
      <description>&lt;p&gt;This blog post describes the procedure we followed to use the &lt;a href=&#34;https://bosh.io/docs/cli-v2.html&#34;&gt;beta BOSH command
line interface (CLI)&lt;/a&gt; to deploy an nginx
webserver with a native IPv6 address (i.e. 2600:1f16:0a62:5c00::4) to AWS in
addition  to its IPv4 Elastic IP address (i.e. 52.15.73.90).  We were then able
to browse the webserver &lt;em&gt;via the IPv6 protocol.&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;alert alert-error&#34; role=&#34;alert&#34;&gt;
&lt;p&gt;BOSH does not support IPv6. This is a proof-of-concept. Do not apply IPv6 to
your production BOSH Directors or to BOSH CLI-deployed systems.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Leveraging NSX&#39;s Features with BOSH&#39;s vSphere CPI</title>
      <link>https://blog.nono.io/post/nsx_with_bosh/</link>
      <pubDate>Tue, 01 Nov 2016 09:15:02 -0700</pubDate>
      <guid>https://blog.nono.io/post/nsx_with_bosh/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.vmware.com/products/nsx.html&#34;&gt;VMWare NSX&lt;/a&gt; is a network
virtualization platform (frequently paired with the vSphere IaaS (Infrastructure
as a Service)). It includes features such as Load Balancers (LBs) and firewall
rules, features often found in public-facing IaaSes (e.g. AWS (Amazon Web
Services), GCE (Google Compute Engine), and Microsoft Azure) but not native to
vSphere.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://bosh.io/&#34;&gt;BOSH&lt;/a&gt;, a VM orchestrator, includes hooks to interoperate with
NSX&amp;rsquo;s LB and Distributed Firewall features. These hooks enable BOSH to attach
created VMs to existing NSX Load Balancer Pools and NSX Distributed Firewall
rulesets. BOSH uses NSX&amp;rsquo;s Security Groups &lt;sup&gt;&lt;a href=&#34;#nsx_security_groups&#34;
class=&#34;alert-link&#34;&gt;[NSX Security Groups]&lt;/a&gt;&lt;/sup&gt; as the underlying mechanism.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to Customize a BOSH Stemcell</title>
      <link>https://blog.nono.io/post/bosh-customize-stemcell/</link>
      <pubDate>Fri, 23 Sep 2016 05:35:01 -0700</pubDate>
      <guid>https://blog.nono.io/post/bosh-customize-stemcell/</guid>
      <description>&lt;p&gt;In this blog post, we describe the procedure we followed in order to create a
custom Google Compute Engine (GCE) stemcell with a user &lt;code&gt;cunnie&lt;/code&gt; whose
&lt;code&gt;~/.ssh/authorized_keys&lt;/code&gt; is pre-populated with a specific public key.&lt;/p&gt;
&lt;div class=&#34;alert alert-error&#34; role=&#34;alert&#34;&gt;
&lt;p&gt;Customizing stemcells is highly discouraged — it voids your warranty, and opens
a host of problems which will only cause pain. This post is intended as an
educational demonstration of the stemcell building process. You have been warned.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Updating a BOSH Release</title>
      <link>https://blog.nono.io/post/updating-a-bosh-release/</link>
      <pubDate>Sun, 18 Sep 2016 10:15:30 -0700</pubDate>
      <guid>https://blog.nono.io/post/updating-a-bosh-release/</guid>
      <description>&lt;p&gt;When &lt;a href=&#34;https://www.powerdns.com/&#34;&gt;PowerDNS&lt;/a&gt; released version 4.0.1 of their
authoritative nameserver, we rushed to update our &lt;a href=&#34;https://github.com/cloudfoundry-community/pdns-release&#34;&gt;BOSH
Release&lt;/a&gt; (which was at
version 4.0.0). We thought it would be a walk in the park, but instead it was an
&lt;a href=&#34;https://github.com/cloudfoundry-community/pdns-release/commit/9e71c74bbf232896fb1865b19568b7eb1dfa6fa7&#34;&gt;epic
fail&lt;/a&gt;
(a final release which couldn&amp;rsquo;t be deployed because the blobs were broken).&lt;/p&gt;
&lt;p&gt;In this blog post we describe the procedure we ultimately followed to
successfully create an updated BOSH final release of version 4.0.1 of the
PowerDNS authoritative nameserver, highlighting some of the tricky and
non-obvious steps.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Concourse has Badges</title>
      <link>https://blog.nono.io/post/concourse-badges/</link>
      <pubDate>Thu, 01 Sep 2016 17:29:49 -0700</pubDate>
      <guid>https://blog.nono.io/post/concourse-badges/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;http://concourse.ci/&#34;&gt;Concourse&lt;/a&gt; Continuous Integration (CI) server has an
API endpoint that displays a badge which shows health of your project:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;http(s)://&lt;span style=&#34;color: green; font-style: italic&#34;&gt;concourse-server&lt;/span&gt;/api/v1/pipelines/&lt;span style=&#34;color: green; font-style: italic&#34;&gt;pipeline-name&lt;/span&gt;/jobs/&lt;span style=&#34;color: green; font-style: italic&#34;&gt;job-name&lt;/span&gt;/badge&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;0-abstract&#34;&gt;0. Abstract&lt;/h2&gt;
&lt;figure&gt;
    &lt;img loading=&#34;lazy&#34; src=&#34;https://blog.nono.io/images/passing.svg&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;Open Source projects that have CI (e.g.
&lt;a href=&#34;https://github.com/twbs/bootstrap&#34;&gt;Bootstrap&lt;/a&gt;,
&lt;a href=&#34;https://github.com/nodejs/node&#34;&gt;Node.js&lt;/a&gt;) often feature status badges (also
known as images or icons) to display the health of their projects.  CI servers
such as &lt;a href=&#34;https://travis-ci.org/&#34;&gt;Travis CI&lt;/a&gt; offer status badges. Concourse CI
also offers status badges.&lt;/p&gt;
&lt;p&gt;The status badge is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Scalable_Vector_Graphics&#34;&gt;Scalable Vector
Graphics&lt;/a&gt; (SVG) image
available from the Concourse API. &lt;sup&gt;&lt;a href=&#34;https://blog.nono.io/post/concourse-badges/#concourse_versions&#34;&gt;[Concourse
versions]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Concourse without a Load Balancer</title>
      <link>https://blog.nono.io/post/concourse-no-elb/</link>
      <pubDate>Fri, 26 Aug 2016 06:58:07 -0700</pubDate>
      <guid>https://blog.nono.io/post/concourse-no-elb/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://concourse.ci/&#34;&gt;Concourse&lt;/a&gt; is a continuous integration (CI) server. It
can be deployed manually or via
&lt;a href=&#34;http://concourse.ci/clusters-with-bosh.html&#34;&gt;BOSH&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this blog post, we describe the BOSH deployment of a Concourse CI server to
natively accept Secure Sockets Layer (SSL) connections without using a load
balancer. This may reduce the complexity and cost
&lt;sup&gt;&lt;a href=&#34;https://blog.nono.io/post/concourse-no-elb/#ELB-pricing&#34;&gt;[ELB-pricing]&lt;/a&gt;&lt;/sup&gt; of a Concourse deployment.&lt;/p&gt;
&lt;div class=&#34;alert alert-danger&#34; role=&#34;alert&#34;&gt; 2016-09-12: This blog post is
obsolete. Newer (v2.0.0+) versions of Concourse allow binding to the privileged
ports 80 and 443, eliminating the need for an nginx proxy. Here is an example
of a BOSH-deployed Concourse server that binds natively to ports 80 &amp; 443: &lt;a
href=&#34;https://github.com/cunnie/deployments/blob/62d0ed813879440f656b6e0bd6f984d708c4bff2/concourse-ntp-pdns-gce.yml#L48-L51&#34;&gt;BOSH
manifest&lt;/a&gt;.  &lt;/div&gt;
&lt;h3 id=&#34;0-pre-requisites&#34;&gt;0. Pre-requisites&lt;/h3&gt;
&lt;p&gt;Deploy Concourse with BOSH. Follow the instructions
&lt;a href=&#34;http://concourse.ci/clusters-with-bosh.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to Deploy a Multi-homed BOSH Director to a vSphere Environment</title>
      <link>https://blog.nono.io/post/multi-homed-bosh-director/</link>
      <pubDate>Fri, 13 May 2016 13:37:44 -0700</pubDate>
      <guid>https://blog.nono.io/post/multi-homed-bosh-director/</guid>
      <description>&lt;p&gt;vSphere users ask, &amp;ldquo;How do I configure my BOSH director so that it can
communicate with my vCenter but the director&amp;rsquo;s deployed VMs can&amp;rsquo;t?&amp;rdquo;&lt;/p&gt;
&lt;p&gt;One technique is to use a multi-homed BOSH director combined with the BOSH &lt;a href=&#34;https://github.com/cloudfoundry/networking-release&#34;&gt;Networking
Release&lt;/a&gt; (a BOSH release
which enables customization of the VM&amp;rsquo;s routing table, allowing more routes
than the default gateway).&lt;/p&gt;
&lt;h2 id=&#34;network-diagram&#34;&gt;Network Diagram&lt;/h2&gt;
&lt;p&gt;The following is a network diagram of our deployment. We want to protect the
assets at the top (in blue): the vCenter server and its associated ESXi servers.
These machines are particularly sensitive as they control hundreds of VMs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The World&#39;s Smallest Concourse CI Server</title>
      <link>https://blog.nono.io/post/worlds-smallest-concourse-server/</link>
      <pubDate>Sat, 24 Oct 2015 13:52:48 -0700</pubDate>
      <guid>https://blog.nono.io/post/worlds-smallest-concourse-server/</guid>
      <description>&lt;h3 id=&#34;2016-04-06-this-blog-post-is-out-of-date-please-refer-to-the-official-concourse-documentationhttpconcourseciinstallinghtml-for-instructions-how-to-install-a-concourse-server&#34;&gt;&lt;em&gt;[2016-04-06: This Blog Post is out-of-date; Please refer to the official &lt;a href=&#34;http://concourse.ci/installing.html&#34;&gt;Concourse documentation&lt;/a&gt; for instructions how to install a Concourse server]&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Continuous_integration&#34;&gt;Continuous Integration&lt;/a&gt;
(CI) is often used in conjunction with test-driven development (TDD);
however, CI servers often
bring their own set of challenges: they are usually &amp;ldquo;snowflakes&amp;rdquo;,
uniquely configured machines that are difficult to upgrade, re-configure, or
re-install. &lt;sup&gt;&lt;a href=&#34;https://blog.nono.io/post/worlds-smallest-concourse-server/#snowflakes&#34;&gt;[snowflakes]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;In this blog post, we describe deploying a publicly-accessible, lean
(1GB RAM, 1 vCPU, 15GB disk)
&lt;a href=&#34;http://concourse.ci/&#34;&gt;Concourse&lt;/a&gt; CI server using a 350-line manifest.
Upgrades/re-configurations/re-installs are as simple as editing a file
and typing one command (&lt;em&gt;bosh-init&lt;/em&gt;).&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
