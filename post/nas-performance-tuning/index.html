<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>A High-performing Mid-range NAS Server, Part 3: 10 GbE | Brian Cunnie's Technical Blog</title><meta name=keywords content><meta name=description content="Abstract &ldquo;How much faster will my VM&rsquo;s disks be if I upgrade my ZFS-based (Z File System) NAS to 10 GbE?&rdquo; The disks will be faster, in some cases, much faster. Our experience is that sequential read throughput will be 1.4✕ faster, write throughput, 10✕ faster, and IOPS, 1.6✕ faster.
We ran a three-hour benchmark on our NAS server before and after upgrading to 10 GbE. We ran the benchmark again after upgrading."><meta name=author content="Brian Cunnie"><link rel=canonical href=https://blog.nono.io/post/nas-performance-tuning/><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://blog.nono.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.nono.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.nono.io/favicon-32x32.png><link rel=apple-touch-icon href=https://blog.nono.io/apple-touch-icon.png><link rel=mask-icon href=https://blog.nono.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-0NJ11E527T"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-0NJ11E527T",{anonymize_ip:!1})}</script><meta property="og:title" content="A High-performing Mid-range NAS Server, Part 3: 10 GbE"><meta property="og:description" content="Abstract &ldquo;How much faster will my VM&rsquo;s disks be if I upgrade my ZFS-based (Z File System) NAS to 10 GbE?&rdquo; The disks will be faster, in some cases, much faster. Our experience is that sequential read throughput will be 1.4✕ faster, write throughput, 10✕ faster, and IOPS, 1.6✕ faster.
We ran a three-hour benchmark on our NAS server before and after upgrading to 10 GbE. We ran the benchmark again after upgrading."><meta property="og:type" content="article"><meta property="og:url" content="https://blog.nono.io/post/nas-performance-tuning/"><meta property="og:image" content="https://nono.io/images/brian_cunnie_profile.jpg"><meta property="article:section" content="post"><meta property="article:published_time" content="2019-05-25T17:16:22+00:00"><meta property="article:modified_time" content="2019-05-25T17:16:22+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://nono.io/images/brian_cunnie_profile.jpg"><meta name=twitter:title content="A High-performing Mid-range NAS Server, Part 3: 10 GbE"><meta name=twitter:description content="Abstract &ldquo;How much faster will my VM&rsquo;s disks be if I upgrade my ZFS-based (Z File System) NAS to 10 GbE?&rdquo; The disks will be faster, in some cases, much faster. Our experience is that sequential read throughput will be 1.4✕ faster, write throughput, 10✕ faster, and IOPS, 1.6✕ faster.
We ran a three-hour benchmark on our NAS server before and after upgrading to 10 GbE. We ran the benchmark again after upgrading."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.nono.io/post/"},{"@type":"ListItem","position":2,"name":"A High-performing Mid-range NAS Server, Part 3: 10 GbE","item":"https://blog.nono.io/post/nas-performance-tuning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"A High-performing Mid-range NAS Server, Part 3: 10 GbE","name":"A High-performing Mid-range NAS Server, Part 3: 10 GbE","description":"Abstract \u0026ldquo;How much faster will my VM\u0026rsquo;s disks be if I upgrade my ZFS-based (Z File System) NAS to 10 GbE?\u0026rdquo; The disks will be faster, in some cases, much faster. Our experience is that sequential read throughput will be 1.4✕ faster, write throughput, 10✕ faster, and IOPS, 1.6✕ faster.\nWe ran a three-hour benchmark on our NAS server before and after upgrading to 10 GbE. We ran the benchmark again after upgrading.","keywords":[],"articleBody":"Abstract “How much faster will my VM’s disks be if I upgrade my ZFS-based (Z File System) NAS to 10 GbE?” The disks will be faster, in some cases, much faster. Our experience is that sequential read throughput will be 1.4✕ faster, write throughput, 10✕ faster, and IOPS, 1.6✕ faster.\nWe ran a three-hour benchmark on our NAS server before and after upgrading to 10 GbE. We ran the benchmark again after upgrading. The benchmark looped through write-read-iops tests continuously.\nThe Sequential Read Results: We were underwhelmed with the 10 GbE read performance — we felt it should have been better, and surprised that it wasn’t. The 1 GbE results were close to the theoretical network maximum, and we thought the upgrade would have unleashed its full potential. Little did we know that its full potential was barely above 1 GbE.\nThe lackluster performance wasn’t due to RAM pressure: there was enough RAM on the FreeNAS server (128 GiB) to cache the contents of the sequential read test (16 GiB) several times over, and this was borne out by running zpool iostat 5, which showed the disks were barely touched during the read test.\nWe also noticed dips in 10 GbE performance at regular intervals (where the read throughput dropped to less than 120 MB/sec); we are not sure what caused these dips, but they seemed to occur almost exactly 35 minutes apart (although timestamps aren’t included in the graph above, they are included in the raw benchmark results, which can be viewed in the References section below).\nAs an aside, there are fewer 1 GbE tests than 10 GbE tests in the above chart. That is, there are fewer blue dots than red dots, for the 1 GbE tests took longer to run, hence there were fewer results reported within the three-hour test window.\nThe Sequential Write Results: The sequential write performance, shown in the above graph, was the star of the show: a 10✕ increase, linear with the network bandwidth increase.\nThere are some caveats. Most importantly, we sacrificed safety for speed. Specifically, we did not override the default ZFS setting, sync=standard. As pointed out in the article, “Sync writes, or: Why is my ESXi NFS so slow, and why is iSCSI faster?”:\niSCSI by default does not implement sync writes. As such, it often appears to users to be much faster…. However, your VM data is being written async, which is hazardous to your VM\nYou may ask, “What are these sync writes to which you refer?” Robert Milkowski, the author of the ZFS sync feature, describes it succinctly in his blog post:\nSynchronous file system transactions (fsync, O_DSYNC, O_SYNC, etc) are written out [to disk before returning from the system call]\nIn other words, these are system calls (fsync(2)) or flags to system calls ( open(2)’s ‘O_DSYNC and O_SYNC) to make sure that the data has really, truly been written out to disk before returning from a write(2). When the system call returns, you know that the data’s on the disk, not in some buffer somewhere.\n[Editor’s note: this is not always true. Linux usually writes the data to disk, but sometimes it doesn’t. For the dirty details, see this post.]\nsync is a great feature when ZFS is used as a fileserver (technically a distributed file system), via protocols such as NFS (Network File System), SMB (Server Message Block), or even the deprecated AFP (Apple Filing Protocol, formerly AppleTalk Filing Protocol):\nThe applications that depend on synchronous behavior (that have opted-in via system call or flag) are guaranteed that their data has been written to disk The majority of applications who don’t need synchronous behavior can take advantage of the high write speeds (often ten times as fast) But here’s the rub: iSCSI is not a distributed file system. It’s a distributed block device. Which means it’s at a lower layer than NFS, SMB, and AFP. There are no system calls, no directories, no file permissions, no files; there are only reads and writes.\nThe upshot is that if power is cut to the NAS server while the VM is active, the and the NAS hasn’t yet flushed the outstanding writes to disk, then the VM’s file system may be corrupted beyond the ability of file system repair tools to fix. We witnessed this problem firsthand by cutting the power to the NAS server while actively exercising a Linux VM’s iSCSI disk by running the gobonniego file system benchmark. Our Linux system’s btrfs filesystem was corrupted beyond the ability of btrfsck to fix, emitting cryptic errors such as “child eb corrupted” and “parent transid verify failed” before finally giving up. We lost our VM, and had to reinstall Linux from scratch.\nThe IOPS Results: The IOPS results were good, averaging 21k. As a comparison, they were much better than a mechanical hard drive’s (44 -203), and better than even SATA 3 Gbit/s’s 5k-20k, but nowhere near as performant as the higher end NVMe drives (e.g. Samsung SSD 960 PRO’s 360k).\nConfiguration Storage Component 10 GbE\n(new, 2019) 1 GbE\n(old, 2104) Motherboard $820 1 × Supermicro X10SDV-8C-TLN4F+ Mini-ITX 1.7GHz 35W 8-Core Intel Xeon D-1537 $375 1 × Supermicro A1SAi-2750F Mini-ITX 2.4GHz 20W 8-Core Intel C2750 Ethernet controller (built-in) (built-in) RAM $1,336 4 × D760R Samsung DDR4-2666 32GB ECC Registered $372 4 × Kingston KVR13LSE9/8 8GB ECC SODIMM HBA (same HBA) $238 1 × LSI SAS 9211-8i 6Gb/s SAS Host Bus Adapter Disk (same Disk) $1,190 7 × Seagate 4TB NAS HDD ST4000VN000 Power Supply (same Power Supply) $110 1 × Corsair HX650 650 watt power supply Ethernet Switch $589 1 × QNAP QSW-1208-8C 12-port 10GbE unmanaged switch $18 1 × TP-Link 8-Port Gigabit Desktop Switch TLSG1008D SFP+ Modules $79 2 × Ubiquiti UF-MM-10G U Fiber SFP+ Module 2-pack N/A Software (same Software) FreeNAS-11.2-U4.1 Virtual Machine: Hypervisor: VMware ESXi, 6.7.0, 13644319 Underlying hardware: (same as file server’s): Supermicro X10SDV-8C-TLN4F+ Mini-ITX 1.7GHz 35W 8-Core Intel Xeon D-1537 vCPUs: 8 RAM: 8 GiB OS: Fedora 30 (Server Edition) Linux kernel: 5.0.16-300.fc30.x86_64 Filesystem: tmpfs (not ext4 nor btrfs) Benchmarking Software: GoBonnieGo v1.0.9 Benchmarking invocation: ./gobonniego -dir /tmp/ -json -seconds 14400 -v -iops-duration 60 Shortcomings We didn’t control solely for the 1 GbE → 10 GbE differences: the ethernet controllers were built into the motherboards, hence the ethernet controller upgrade entailed a motherboard upgrade in turn entailing CPU \u0026 RAM upgrades.\nIn other words, the performance increases can’t be attributed solely to the change in network controllers; faster CPUs and larger (32 GiB → 128 GiB) and faster (DDR3 → DDR4) RAM may also have been a factor.\nWe have seen at least one dramatic speed-up that we could not attribute to the ethernet upgrade: when browsing an AFP (Apple Filing Protocol) share from our WiFi-based laptop, we noticed that one large directory (4.5k entries), which previously took ~30 seconds to display in Apple’s Finder, now displayed in sub-second time. We know the WiFi’s throughput (200 Mb/s) was minuscule compared to the file server’s (1 Gb/s, 10 Gb/s), so the reason for the speed-up lay elsewhere.\nOther VMs had disks that were placed on the iSCSI datastore; they may have contended with benchmark VM for disk access. We feel the contention was minor at most.\nReferences Raw benchmark results: 1 GbE 10 GbE ZFS configuration: zdb -eC tank zfs get all tank/vSphere zpool status tank A High-performing Mid-range NAS Server, Part 1: Initial Set-up and Testing A High-performing Mid-range NAS Server, Part 2: Performance Tuning for iSCSI ","wordCount":"1247","inLanguage":"en","datePublished":"2019-05-25T17:16:22Z","dateModified":"2019-05-25T17:16:22Z","author":{"@type":"Person","name":"Brian Cunnie"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.nono.io/post/nas-performance-tuning/"},"publisher":{"@type":"Organization","name":"Brian Cunnie's Technical Blog","logo":{"@type":"ImageObject","url":"https://blog.nono.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.nono.io/ accesskey=h title="Brian Cunnie's Technical Blog (Alt + H)">Brian Cunnie's Technical Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://blog.nono.io/>Home</a>&nbsp;»&nbsp;<a href=https://blog.nono.io/post/>Posts</a></div><h1 class=post-title>A High-performing Mid-range NAS Server, Part 3: 10 GbE</h1><div class=post-meta><span title='2019-05-25 17:16:22 +0000 UTC'>May 25, 2019</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Brian Cunnie&nbsp;|&nbsp;<a href=https://github.com/cunnie/cunnie.github.io/tree/master/content/post/nas-performance-tuning.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><h3 id=abstract>Abstract<a hidden class=anchor aria-hidden=true href=#abstract>#</a></h3><p>&ldquo;How much faster will my VM&rsquo;s disks be if I upgrade my
<a href=https://en.wikipedia.org/wiki/ZFS>ZFS</a>-based (Z File System) NAS to 10 GbE?&rdquo;
The disks will be faster, in some cases, much faster. Our experience is that
sequential read throughput will be 1.4✕ faster, write throughput, 10✕ faster,
and IOPS, 1.6✕ faster.</p><p>We ran a three-hour benchmark on our NAS server before and after upgrading to 10
GbE. We ran the benchmark again after upgrading. The benchmark looped through
write-read-iops tests continuously.</p><h4 id=the-sequential-read-results>The Sequential Read Results:<a hidden class=anchor aria-hidden=true href=#the-sequential-read-results>#</a></h4><figure><img loading=lazy src="https://docs.google.com/spreadsheets/d/e/2PACX-1vTddYLAn6UFpesWIPH5S6ptr9sm3ECcHxf5aYobpfKqT1pdp8IyTZu4D9yV7SOmwQEVkhgwpy5xnlUW/pubchart?oid=955621268&amp;format=image"></figure><p>We were underwhelmed with the 10 GbE read performance — we felt it should have
been better, and surprised that it wasn&rsquo;t. The 1 GbE results were close to the
theoretical network maximum, and we thought the upgrade would have unleashed its
full potential. Little did we know that its full potential was barely above 1
GbE.</p><p>The lackluster performance wasn&rsquo;t due to RAM pressure: there was enough RAM on
the FreeNAS server (128 GiB) to cache the contents of the sequential read test
(16 GiB) several times over, and this was borne out by running <code>zpool iostat 5</code>,
which showed the disks were barely touched during the read test.</p><p>We also noticed dips in 10 GbE performance at regular intervals (where the read
throughput dropped to less than 120 MB/sec); we are not sure what caused these
dips, but they seemed to occur <em>almost exactly 35 minutes apart</em> (although
timestamps aren&rsquo;t included in the graph above, they are included in the raw
benchmark results, which can be viewed in the <a href=#references>References</a> section
below).</p><p>As an aside, there are fewer 1 GbE tests than 10 GbE tests in the above chart.
That is, there are fewer blue dots than red dots, for the 1 GbE tests took
longer to run, hence there were fewer results reported within the three-hour
test window.</p><h4 id=the-sequential-write-results>The Sequential Write Results:<a hidden class=anchor aria-hidden=true href=#the-sequential-write-results>#</a></h4><figure><img loading=lazy src="https://docs.google.com/spreadsheets/d/e/2PACX-1vTddYLAn6UFpesWIPH5S6ptr9sm3ECcHxf5aYobpfKqT1pdp8IyTZu4D9yV7SOmwQEVkhgwpy5xnlUW/pubchart?oid=321002413&amp;format=image"></figure><p>The sequential write performance, shown in the above graph, was the star of the
show: a 10✕ increase, linear with the network bandwidth increase.</p><p>There are some caveats. Most importantly, we sacrificed safety for speed.
Specifically, we did not override the default ZFS setting, <code>sync=standard</code>. As
pointed out in the article, &ldquo;<a href=https://www.ixsystems.com/community/resources/sync-writes-or-why-is-my-esxi-nfs-so-slow-and-why-is-iscsi-faster.40/>Sync writes, or: Why is my ESXi NFS so slow, and
why is iSCSI
faster?</a>&rdquo;:</p><blockquote><p>iSCSI by default does not implement sync writes. As such, it often appears to
users to be much faster&mldr;. However, your VM data is being written async,
which is hazardous to your VM</p></blockquote><p>You may ask, &ldquo;What are these sync writes to which you refer?&rdquo; Robert Milkowski,
the author of the ZFS <code>sync</code> feature, describes it succinctly in his <a href=http://milek.blogspot.com/2010/05/zfs-synchronous-vs-asynchronous-io.html>blog
post</a>:</p><blockquote><p>Synchronous file system transactions (fsync, O_DSYNC, O_SYNC, etc) are written
out [to disk before returning from the system call]</p></blockquote><p>In other words, these are system calls
(<a href=http://man7.org/linux/man-pages/man2/fsync.2.html><code>fsync(2)</code></a>) or flags to
system calls ( <a href=http://man7.org/linux/man-pages/man2/open.2.html><code>open(2)</code></a>&rsquo;s
&lsquo;<code>O_DSYNC</code> and <code>O_SYNC</code>) to make sure that the data has really, truly been
written out to disk before returning from a
<a href=http://man7.org/linux/man-pages/man2/write.2.html><code>write(2)</code></a>. When the system
call returns, you know that the data&rsquo;s on the disk, not in some buffer
somewhere.</p><p><em>[Editor&rsquo;s note: this is not always true. Linux <em>usually</em> writes the data to
disk, but sometimes it doesn&rsquo;t. For the dirty details, see <a href=http://milek.blogspot.com/2010/12/linux-osync-and-write-barriers.html>this
post</a>.]</em></p><p><code>sync</code> is a great feature when ZFS is used as a fileserver (technically a
<a href=https://en.wikipedia.org/wiki/Clustered_file_system#Distributed_file_systems>distributed file
system</a>),
via protocols such as <a href=https://en.wikipedia.org/wiki/Network_File_System>NFS</a>
(Network File System), <a href=https://en.wikipedia.org/wiki/Server_Message_Block>SMB</a>
(Server Message Block), or even the deprecated
<a href=https://en.wikipedia.org/wiki/Apple_Filing_Protocol>AFP</a> (Apple Filing
Protocol, formerly AppleTalk Filing Protocol):</p><ul><li>The applications that depend on synchronous behavior (that have opted-in via
system call or flag) are guaranteed that their data has been written to disk</li><li>The majority of applications who don&rsquo;t need synchronous behavior
can take advantage of the high write speeds (often ten times as fast)</li></ul><p>But here&rsquo;s the rub: iSCSI is <strong>not</strong> a distributed file system. It&rsquo;s a
distributed block device. Which means it&rsquo;s at a lower layer than NFS, SMB, and
AFP. There are no system calls, no directories, no file permissions, no files;
there are only reads and writes.</p><p>The upshot is that if power is cut to the NAS server while the VM is active, the
and the NAS hasn&rsquo;t yet flushed the outstanding writes to disk, then the VM&rsquo;s
file system may be corrupted beyond the ability of file system repair tools to
fix. We witnessed this problem firsthand by cutting the power to the NAS server
while actively exercising a Linux VM&rsquo;s iSCSI disk by running the <code>gobonniego</code>
file system benchmark. Our Linux system&rsquo;s <code>btrfs</code> filesystem was corrupted
beyond the ability of <code>btrfsck</code> to fix, emitting cryptic errors such as &ldquo;<code>child eb corrupted</code>&rdquo; and &ldquo;<code>parent transid verify failed</code>&rdquo; before finally giving up. We
lost our VM, and had to reinstall Linux from scratch.</p><h4 id=the-iops-results>The IOPS Results:<a hidden class=anchor aria-hidden=true href=#the-iops-results>#</a></h4><figure><img loading=lazy src="https://docs.google.com/spreadsheets/d/e/2PACX-1vTddYLAn6UFpesWIPH5S6ptr9sm3ECcHxf5aYobpfKqT1pdp8IyTZu4D9yV7SOmwQEVkhgwpy5xnlUW/pubchart?oid=1489787822&amp;format=image"></figure><p>The IOPS results were good, averaging 21k. As a comparison, they were much
better than a <a href=https://en.wikipedia.org/wiki/IOPS#Mechanical_hard_drives>mechanical hard
drive&rsquo;s</a> (44 -203),
and better than even <a href=https://en.wikipedia.org/wiki/IOPS#Solid-state_devices>SATA 3
Gbit/s</a>&rsquo;s 5k-20k, but
nowhere near as performant as the higher end NVMe drives (e.g. Samsung
SSD 960 PRO&rsquo;s 360k).</p><h3 id=configuration>Configuration<a hidden class=anchor aria-hidden=true href=#configuration>#</a></h3><figure><img loading=lazy src="https://docs.google.com/drawings/d/e/2PACX-1vTBv3cUiVxCkfjKnN0iV1tjG-wo3GfnXF14zPSlSM4sDnU508y0GE6rsyR3ADNG6iRV7qVtKjyeByTM/pub?w=1128&amp;h=858"></figure><h4 id=storage>Storage<a hidden class=anchor aria-hidden=true href=#storage>#</a></h4><table><thead><tr><th>Component</th><th>10 GbE<br>(new, 2019)</th><th>1 GbE<br>(old, 2104)</th></tr></thead><tbody><tr><td>Motherboard</td><td>$820 1 × <a href=https://www.supermicro.com/products/motherboard/Xeon/D/X10SDV-8C-TLN4F_.cfm>Supermicro X10SDV-8C-TLN4F+ Mini-ITX 1.7GHz 35W 8-Core Intel Xeon D-1537</a></td><td>$375 1 × <a href=https://www.supermicro.com/products/motherboard/Atom/X10/A1SAi-2750F.cfm>Supermicro A1SAi-2750F Mini-ITX 2.4GHz 20W 8-Core Intel C2750</a></td></tr><tr><td>Ethernet controller</td><td>(built-in)</td><td>(built-in)</td></tr><tr><td>RAM</td><td>$1,336 4 × D760R Samsung DDR4-2666 32GB ECC Registered</td><td>$372 4 × Kingston KVR13LSE9/8 8GB ECC SODIMM</td></tr><tr><td>HBA</td><td>(same HBA)</td><td>$238 1 × <a href=https://docs.broadcom.com/docs/12352062>LSI SAS 9211-8i 6Gb/s SAS Host Bus Adapter</a></td></tr><tr><td>Disk</td><td>(same Disk)</td><td>$1,190 7 × Seagate 4TB NAS HDD ST4000VN000</td></tr><tr><td>Power Supply</td><td>(same Power Supply)</td><td>$110 1 × <a href=https://www.corsair.com/us/en/Categories/Products/Power-Supply-Units/hx-series-config/p/CP-9020030-NA>Corsair HX650 650 watt power supply</a></td></tr><tr><td>Ethernet Switch</td><td>$589 1 × <a href=https://www.qnap.com/en-us/product/qsw-1208-8c>QNAP QSW-1208-8C 12-port 10GbE unmanaged switch</a></td><td>$18 1 × <a href=https://www.tp-link.com/us/home-networking/8-port-switch/tl-sg1008d/>TP-Link 8-Port Gigabit Desktop Switch TLSG1008D</a></td></tr><tr><td>SFP+ Modules</td><td>$79 2 × <a href=https://store.ui.com/collections/all/products/uf-mm-10g-20-20-pack>Ubiquiti UF-MM-10G U Fiber SFP+ Module 2-pack</a></td><td>N/A</td></tr><tr><td>Software</td><td>(same Software)</td><td>FreeNAS-11.2-U4.1</td></tr></tbody></table><h4 id=virtual-machine>Virtual Machine:<a hidden class=anchor aria-hidden=true href=#virtual-machine>#</a></h4><ul><li>Hypervisor: <strong>VMware ESXi, 6.7.0, 13644319</strong></li><li>Underlying hardware: (same as file server&rsquo;s): <a href=https://www.supermicro.com/products/motherboard/Xeon/D/X10SDV-8C-TLN4F_.cfm>Supermicro X10SDV-8C-TLN4F+ Mini-ITX 1.7GHz 35W 8-Core Intel Xeon D-1537</a></li><li>vCPUs: <strong>8</strong></li><li>RAM: <strong>8 GiB</strong></li><li>OS: <strong>Fedora 30 (Server Edition)</strong></li><li>Linux kernel: <strong>5.0.16-300.fc30.x86_64</strong></li><li>Filesystem: <strong><a href=https://en.wikipedia.org/wiki/Tmpfs#Linux>tmpfs</a></strong> (not ext4 nor btrfs)</li><li>Benchmarking Software: <strong><a href=https://github.com/cunnie/gobonniego>GoBonnieGo v1.0.9</a></strong></li><li>Benchmarking invocation: <code>./gobonniego -dir /tmp/ -json -seconds 14400 -v -iops-duration 60</code></li></ul><h3 id=shortcomings>Shortcomings<a hidden class=anchor aria-hidden=true href=#shortcomings>#</a></h3><p>We didn&rsquo;t control solely for the 1 GbE → 10 GbE differences: the ethernet
controllers were built into the motherboards, hence the ethernet controller
upgrade entailed a motherboard upgrade in turn entailing CPU & RAM upgrades.</p><p>In other words, the performance increases can&rsquo;t be attributed solely to the
change in network controllers; faster CPUs and larger (32 GiB → 128 GiB) and
faster (DDR3 → DDR4) RAM may also have been a factor.</p><p>We have seen at least one dramatic speed-up that we could <em>not</em> attribute to the
ethernet upgrade: when browsing an AFP (Apple Filing Protocol) share from our
WiFi-based laptop, we noticed that one large directory (4.5k entries), which
previously took ~30 seconds to display in Apple&rsquo;s Finder, now displayed in
sub-second time. We know the WiFi&rsquo;s throughput (200 Mb/s) was minuscule compared
to the file server&rsquo;s (1 Gb/s, 10 Gb/s), so the reason for the speed-up lay
elsewhere.</p><p>Other VMs had disks that were placed on the iSCSI datastore; they may have
contended with benchmark VM for disk access. We feel the contention was minor at
most.</p><h2 id=a-namereferencesreferencesa><a name=references>References</a><a hidden class=anchor aria-hidden=true href=#a-namereferencesreferencesa>#</a></h2><ul><li>Raw benchmark results:<ul><li><a href=https://raw.githubusercontent.com/cunnie/cloud_storage_benchmarks/5e0d329f1b386de5ee7cf43e0619d0d61076ea4e/10g_upgrade/1g.json>1 GbE</a></li><li><a href=https://raw.githubusercontent.com/cunnie/cloud_storage_benchmarks/5e0d329f1b386de5ee7cf43e0619d0d61076ea4e/10g_upgrade/10g.json>10 GbE</a></li></ul></li><li>ZFS configuration:<ul><li><a href=https://github.com/cunnie/cloud_storage_benchmarks/blob/master/10g_upgrade/zdb_-eC_tank.txt><code>zdb -eC tank</code></a></li><li><a href=https://github.com/cunnie/cloud_storage_benchmarks/blob/master/10g_upgrade/zfs_get_all_tank_slash_vSphere.txt><code>zfs get all tank/vSphere</code></a></li><li><a href=https://github.com/cunnie/cloud_storage_benchmarks/blob/master/10g_upgrade/zpool_status_tank.txt><code>zpool status tank</code></a></li></ul></li><li><a href=https://content.pivotal.io/blog/a-high-performing-mid-range-nas-server>A High-performing Mid-range NAS Server, Part 1: Initial Set-up and Testing</a></li><li><a href=https://content.pivotal.io/blog/a-high-performing-mid-range-nas-server-part-2-performance-tuning-for-iscsi>A High-performing Mid-range NAS Server, Part 2: Performance Tuning for iSCSI</a></li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://blog.nono.io/post/haproxy-ipv6/><span class=title>« Prev</span><br><span>How To Enable IPv6 on Your Cloud Foundry's HAProxy</span></a>
<a class=next href=https://blog.nono.io/post/totp/><span class=title>Next »</span><br><span>Transferring Time-based One-time Passwords to a New Smartphone</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share A High-performing Mid-range NAS Server, Part 3: 10 GbE on twitter" href="https://twitter.com/intent/tweet/?text=A%20High-performing%20Mid-range%20NAS%20Server%2c%20Part%203%3a%2010%20GbE&amp;url=https%3a%2f%2fblog.nono.io%2fpost%2fnas-performance-tuning%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share A High-performing Mid-range NAS Server, Part 3: 10 GbE on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fblog.nono.io%2fpost%2fnas-performance-tuning%2f&amp;title=A%20High-performing%20Mid-range%20NAS%20Server%2c%20Part%203%3a%2010%20GbE&amp;summary=A%20High-performing%20Mid-range%20NAS%20Server%2c%20Part%203%3a%2010%20GbE&amp;source=https%3a%2f%2fblog.nono.io%2fpost%2fnas-performance-tuning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share A High-performing Mid-range NAS Server, Part 3: 10 GbE on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fblog.nono.io%2fpost%2fnas-performance-tuning%2f&title=A%20High-performing%20Mid-range%20NAS%20Server%2c%20Part%203%3a%2010%20GbE"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share A High-performing Mid-range NAS Server, Part 3: 10 GbE on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fblog.nono.io%2fpost%2fnas-performance-tuning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share A High-performing Mid-range NAS Server, Part 3: 10 GbE on whatsapp" href="https://api.whatsapp.com/send?text=A%20High-performing%20Mid-range%20NAS%20Server%2c%20Part%203%3a%2010%20GbE%20-%20https%3a%2f%2fblog.nono.io%2fpost%2fnas-performance-tuning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share A High-performing Mid-range NAS Server, Part 3: 10 GbE on telegram" href="https://telegram.me/share/url?text=A%20High-performing%20Mid-range%20NAS%20Server%2c%20Part%203%3a%2010%20GbE&amp;url=https%3a%2f%2fblog.nono.io%2fpost%2fnas-performance-tuning%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://blog.nono.io/>Brian Cunnie's Technical Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>