<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Why Is My NTP Server Costing $500/Year? Part 3 | Brian Cunnie's Technical Blog</title><meta name=keywords content><meta name=description content="When Hacker News picked up Part
1 of our series of blog posts on
running public NTP servers, a contributor said, &ldquo;I wish he&rsquo;d explained &mldr; what
they ultimately did (since there&rsquo;s no part 3 that I can find).&rdquo;
We had dropped the ball — we had never concluded the series, had never written
part 3, had never described the strategies to mitigate the data transfer costs.
This blog post remedies that oversight; it consists of two parts: the first part
addresses strategies to reduce the cost of running an NTP server, and the second
part discusses side topics (aspects of running an NTP server)."><meta name=author content="Brian Cunnie"><link rel=canonical href=https://blog.nono.io/post/ntp-costs-500/><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=https://blog.nono.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.nono.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.nono.io/favicon-32x32.png><link rel=apple-touch-icon href=https://blog.nono.io/apple-touch-icon.png><link rel=mask-icon href=https://blog.nono.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://blog.nono.io/post/ntp-costs-500/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-0NJ11E527T"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-0NJ11E527T")}</script><meta property="og:url" content="https://blog.nono.io/post/ntp-costs-500/"><meta property="og:site_name" content="Brian Cunnie's Technical Blog"><meta property="og:title" content="Why Is My NTP Server Costing $500/Year? Part 3"><meta property="og:description" content="When Hacker News picked up Part 1 of our series of blog posts on running public NTP servers, a contributor said, “I wish he’d explained … what they ultimately did (since there’s no part 3 that I can find).”
We had dropped the ball — we had never concluded the series, had never written part 3, had never described the strategies to mitigate the data transfer costs.
This blog post remedies that oversight; it consists of two parts: the first part addresses strategies to reduce the cost of running an NTP server, and the second part discusses side topics (aspects of running an NTP server)."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2017-01-28T12:38:13-08:00"><meta property="article:modified_time" content="2017-01-28T12:38:13-08:00"><meta property="og:image" content="https://nono.io/images/brian_cunnie_profile.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://nono.io/images/brian_cunnie_profile.jpg"><meta name=twitter:title content="Why Is My NTP Server Costing $500/Year? Part 3"><meta name=twitter:description content="When Hacker News picked up Part
1 of our series of blog posts on
running public NTP servers, a contributor said, &ldquo;I wish he&rsquo;d explained &mldr; what
they ultimately did (since there&rsquo;s no part 3 that I can find).&rdquo;
We had dropped the ball — we had never concluded the series, had never written
part 3, had never described the strategies to mitigate the data transfer costs.
This blog post remedies that oversight; it consists of two parts: the first part
addresses strategies to reduce the cost of running an NTP server, and the second
part discusses side topics (aspects of running an NTP server)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.nono.io/post/"},{"@type":"ListItem","position":2,"name":"Why Is My NTP Server Costing $500/Year? Part 3","item":"https://blog.nono.io/post/ntp-costs-500/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Why Is My NTP Server Costing $500/Year? Part 3","name":"Why Is My NTP Server Costing $500\/Year? Part 3","description":"When Hacker News picked up Part 1 of our series of blog posts on running public NTP servers, a contributor said, \u0026ldquo;I wish he\u0026rsquo;d explained \u0026hellip; what they ultimately did (since there\u0026rsquo;s no part 3 that I can find).\u0026rdquo;\nWe had dropped the ball — we had never concluded the series, had never written part 3, had never described the strategies to mitigate the data transfer costs.\nThis blog post remedies that oversight; it consists of two parts: the first part addresses strategies to reduce the cost of running an NTP server, and the second part discusses side topics (aspects of running an NTP server).\n","keywords":[],"articleBody":"When Hacker News picked up Part 1 of our series of blog posts on running public NTP servers, a contributor said, “I wish he’d explained … what they ultimately did (since there’s no part 3 that I can find).”\nWe had dropped the ball — we had never concluded the series, had never written part 3, had never described the strategies to mitigate the data transfer costs.\nThis blog post remedies that oversight; it consists of two parts: the first part addresses strategies to reduce the cost of running an NTP server, and the second part discusses side topics (aspects of running an NTP server).\nPerhaps the most dismaying discovery of writing this blog post was the realization that the title is no longer accurate — rather than costing us $500/year, our most expensive NTP server was costing us more than $750/year in data transfer charges. [Traffic increase]\nTable of Contents 0. Previous Posts (Parts 1 \u0026 2) 1. Reducing the Cost of Running an NTP Server 1.0 Statistics (traffic) 1.1 The Right Infrastructure Can Drop the Data Transfer Costs to $0 1.2 Connection Speed Setting 1.3 Geographical Placement 1.4 Rate Limiting 1.5 Join the Pool 2. Side Topics 2.0 The Cavalry is Coming: Google’s Public NTP Servers 2.1 The Snapchat Excessive NTP Query Event Cost $13 - $18 (Per Server) 2.2 Are Virtual Machines Adequate NTP Servers? Yes. 2.3 Sometimes It’s the Network, not the VM Footnotes Corrections \u0026 Updates 0. Previous Posts These posts provide background, but reading them isn’t necessary:\nWhy Is My NTP Server Costing $500/Year? Part 1. We analyze our sudden increase in AWS data transfer charges and conclude that adding our server into the NTP pool is the sole reason for the data transfer increase.\nWhy Is My NTP Server Costing Me $500/Year? Part 2: Characterizing the NTP Clients. We characterize the demand that each NTP client places on an NTP server, by operating system. To our surprise, FreeBSD and Ubuntu place the greatest demand on the NTP servers, and Windows and macOS the least.\n1. Reducing the Cost of Running an NTP Server We maintain several servers in the pool.ntp.org project. These servers are personal, not corporate, so we’re quite sensitive to cost: we don’t want to spend a bundle if we don’t have to. Also, these servers have roles other than NTP servers (in fact, their primary purpose is to provide Domain Name System (DNS) service and one is also a Concourse continuous integration (CI) server).\nWhich begs the question: given the expense, why do it? We have several motives:\nWe have benefited greatly from the open source community, and providing this service is a modest way of giving back.\nOur day job is a developer on BOSH, a tool which, at its simplest, creates VMs in the cloud based on specifications passed to it in a file. We use BOSH to deploy our NTP servers, and on at least two occasions we have uncovered obscure bugs as a result.\nOn the rare occasions when our systems fail, often our first warning is an email from the pool with the subject, “NTP Pool: Problems with your NTP service”. In other words, being in the pool is a great monitoring system, or, at the very least, better than nothing.\n1.0 Statistics (traffic) We have two NTP servers in the pool.ntp.org project whose country is set to “United States” and whose connection speed is set to “1000 Mbit”. We have gathered the following statistics [NTP statistics] over a seven-day period (2017-01-21 15:00 UTC - 2017-01-28 15:00 UTC). Note that other than data transfer pricing, the choice of underlying IaaS is unimportant (assuming proper functioning of VM/disk/network). In other words, although the Google Compute Engine (GCE) server carries more traffic than the Amazon Web Services (AWS) server, the roles could have easily been reversed. The mechanism underlying pool.ntp.org project (a multi-stage mechanism which “targets the users to servers in/near their country and does a weighted round-robin just on those servers”), is not a perfectly precise balancing mechanism (e.g. some clients will “stick” to a server long after the pool.ntp.org record has updated).\nMetric Amazon Web Services Google Compute Engine packets recvd/sec 3033.89 3115.38 packets sent/sec 2794.60 2909.26 GiB recvd/month 564.71 579.88 GiB sent/month 520.17 541.51 $ / GiB sent $0.09 $0.12 $ / month $46.82 $64.98 Although we present statistics for both inbound (NTP queries to our server) and outbound traffic (NTP responses from our server), it is the outbound traffic which is of particular interest to us, for the inbound traffic is usually free, and the outbound traffic is usually metered (both AWS and GCE charge for outbound traffic but not inbound). We have attempted to maintain a consistent coloring scheme for our charts, using blue for inbound (free) and green for outbound (metered). The mnemonic is that green, the color of US currency, is the metric for which we pay.\n1.1 The Right Infrastructure Can Drop the Data Transfer Costs to $0 We believe that the choice of IaaS is the most important factor in determining costs. For example, running an NTP server on GCE would have an annual cost of $945.36, and running a similar server on DigitalOcean would cost $120 — that represents an 87% reduction in total cost and an annual savings of $825.36.\nOur GCE configuration assumes a g1-small instance (1 shared CPU, 1.7 GiB) RAM ($0.019 per hour) running 24x7 (30% Sustained Use Discount) for a monthly cost of $13.80. We then add the monthly data transfer costs of $64.98 for a monthly total of $78.78, annual total of $945.36.\nDigitalOcean offers 2TB/month free on their $10/month server; which should be adequate for the highest-trafficked NTP servers in the pool (i.e. US-based, 1000 Mbit connection speed), which typically have 1.1 - 1.4 TiB aggregate inbound and outbound traffic. [DigitalOcean bandwidth metering]\nThe NTP server need not reside in an IaaS; it is equally effective in a residence. In fact, savvy home users who have a static IP, have set up an NTP server, and who are comfortable sharing their NTP service with the community at large are encouraged to join the pool.\nIt is particularly important when setting up an NTP server in a residence to set an appropriate connection speed (see next section).\n1.2 Connection Speed Setting The most important tool to control the amount of traffic your NTP server receives is to use the “Net speed/connection speed” setting in the Manage Servers page. Its thirteen settings cover more than three orders of magnitude.\nUsing our GCE NTP server as an example, at the highest setting (1000 Mbit), we incur $64.98/month, and at the lowest setting (384 Kbit), $0.02/month.\nThe tool isn’t precise: as previously mentioned, round-robin DNS is a blunt instrument. We have two NTP servers based in the US whose connection speed is set to 1000Mbit, and yet their outbound traffic differs by 4% (our AWS server carries 4% less traffic than our GCE server). Using the connection speed will get you in the ballpark, but don’t expect precision.\nAccording to pool.ntp.org:\nThe net speed is used to balance the load between the pool servers. If your connection is asymmetric (like most DSL connections) you should use the lower speed.\nIn our residence, our cable download speed is 150Mbps, but our upload speed is a mere 10Mbps, so we set our pool.ntp.org’s server setting to “10Mbit”\nThe pool will only use a fraction of the “netspeed setting”\nThe million-dollar question: what is the fraction, exactly? about 2% worst-case scenario (i.e. server is placed in the United States). Assuming 90 bytes per NTP packet, 1,000,000,000 bits per Gbit, aggregating inbound and outbound (2Gbps total aggregate bandwidth per server), we calculate the bandwidth to be, on our 4 servers, 2.10%, 2.16%, 0.67%, and 0.35%.\nOur home connection is not metered, so we don’t incur bandwidth charges (i.e. it’s free).\nThe pool.ntp.org project’s menu option on the server management page allows you to throttle the traffic on your server. The lower your connection speed, the less traffic your server will receive, and the lower the bandwidth costs.\nThe aggregate “netspeed” for the US zone is 86798173 kbps. This implies the following:\nOur GCE server (and also our AWS server) accounts for 1.15% of the US NTP pool traffic The entire US NTP pool is queried 270,376× every second The entire US NTP pool responds (assuming rate-limiting) 252,495× every second The entire US NTP pool transfers 45.9TiB in NTP responses monthly Using GCE, the entire US NTP pool would cost $4,078 in monthly data transfer costs (GCE’s pricing tiers for $/GiB-month are $0.12 for TiB 0-1, $0.11 for TiB 1-10, $0.08 for TiB 10+) 1.3 Geographical Placement The placement of the NTP server has dramatic effect on the bandwidth and cost. For example, our German NTP server’s typical outbound monthly traffic is 82.63 GiB, which is 85% less bandwidth than our US/Google server’s monthly 541.51 GiB.\nA note about placement: The pool.ntp.org project allows participants to place servers in various zones. A “zone” consists of a country and that country’s continent (there are non-geographic vendor zones as well, e.g. Ubuntu has a zone “ubuntu.pool.ntp.org”, but those zones fall outside the scope of this discussion). The pool will “will try finding the closest available servers” for NTP clients, which may or may not be in the same zone.\nWe don’t have NTP servers on all continents (we’re missing Antarctica, Oceania, and South America), so we don’t have insight on the bandwidth requirements for NTP servers placed there; however, we do know that our server in Asia is not as stressed as our US/Google server (166.00 GiB vs. 541.51 Gib).\nWe were curious why the load on our German server was so light, and we believe that part of the reason is that there is much greater participation in the pool.ntp.org project in Europe. Europe, with 2,686 servers, has almost exactly 3× North America’s meager 895 servers.\nThe ntp.pool.org’s servers, broken out by continent.\n1.4 Rate Limiting We have found that by enabling NTP’s rate limiting feature can reduce the outbound traffic by 9.56% on average (as high as 16.60% for our German server and as low as 6.57% for our GCE server).\nBut don’t be fooled into thinking that rate limiting’s sole purpose is to reduce traffic a measly 9%. Rate limiting has an unintended side benefit: it throttles traffic during excessive NTP query events. For example, Snapchat released a broken iOS client which placed excessive load on NTP servers (see below for more information) in mid-December. Traffic to our AWS server, normally a steady 600 MiB / hour, spiked viciously. In one particular hour (12/18/2016 04:00 - 0500 UTC), the inbound traffic climbed almost ninefold to 4.97 GiB! Fortunately, rate limiting kicked in, and rejected 69.30% of the traffic, which reduced our cost for that hour by 69.30%, for we are charged only for outbound traffic.\nRate limiting is enabled by adding the kod and limited directives in the NTP servers configuration file. In our servers’ configuration, we use the suggested restrictions for servers “who allow others to get the time” and “to see your server status information”. Links to our ntp.conf files can be found here.\nrestrict default limited kod nomodify notrap nopeer restrict -6 default limited kod nomodify notrap nopeer Disclaimer: we’re not 100% sure it was rate-limiting that clamped down on our outbound traffic. There is a small chance that it was another factor, e.g. ntpd was overwhelmed and dropped packets, AWS (or GCE) stepped in and limited outbound NTP traffic. We haven’t run the numbers.\n1.5 Join the Pool We encourage those with NTP servers with static IPs to join the pool; the experience has been personally rewarding and professionally enriching (how many can claim to operate a service with thousands of requests per second?).\nWe also lay out a cautionary tale: when joining, keep an eye to costs, especially bandwidth. Opting for a lower connection speed initially (e.g. 10Mbps), and ratcheting it up over time is a prudent course of action.\n2. Side Topics 2.0 The Cavalry is Coming: Google’s Public NTP Servers Google has announced public NTP servers. Over time, we suspect that this will reduce the load on the pool.ntp.org project’s servers.\nServers in the NTP Pool should not use Google’s NTP servers as upstream time providers, nor should they use any upstream provider which “smears” the leap second.\nThere is a schism in the community regarding leap seconds:\nThe NTP pool supports the leap second, which is the UTC standard. The advantage of the leap second is that every second is always the same length, i.e. “9,192,631,770 periods of the radiation emitted by a caesium-133 atom in the transition between the two hyperfine levels of its ground state”.\nGoogle, on the other hand, smears the leap second, which lengthens the second by 13.9µs during the ten hours leading up to and following the leap seconds. Their reasoning is, “No commonly used operating system is able to handle a minute with 61 seconds”.\nFor readers interested in using Google’s NTP service, the server is time.google.com.\n2.1 The Snapchat Excessive NTP Query Event Cost $13 - $18 (Per Server) In December Snapchat released a version of its iOS app that placed undue stress on the pool.ntp.org servers.\nThis event caused great consternation among the NTP server operators, and words such as “decimated”, “server loss”, and “sad” were used.\nThe effect on two of our servers can be readily seen by our bandwidth graph. First, our AWS server:\nOur AWS chart presents hourly inbound and outbound traffic, measured in MiB. Times are in UTC.\nNote the following leading up to the event:\nInbound traffic is fairly steady, averaging 611 MiB/hour, and so is outbound traffic, averaging 562 MiB/hr. There’s little difference between inbound and outbound traffic (i.e. ntpd’s rate-limiting is kicking in at a modest ~8%). Note the following about the event and its aftermath:\nThe onset of the event was sudden: on 12/13/2016, inbound traffic jumped from 12.7 GiB the previous day to 18.2 GiB. There were unexplained dips in traffic during the event. For example, on 12/16/2016 0:00 - 3:00 UTC, the inbound traffic fell below 300 MiB/hr. Not only was this extremely low in the midst of an NTP excessive query-event, but it would have been abnormally low during regular service. The event had a long tail. Even though Snapchat released a fix, traffic hadn’t normalized by the end of December. Things had improved, but they hadn’t gotten gotten back to normal. Next, we present our GCE bandwidth chart:\nOur GCE chart presents per second inbound and outbound traffic, measured in packets. Times are in EST.\nNote the following:\nUnlike AWS, there were no unexplained dips in traffic. Google smoothed the graph — it’s not as jagged as AWS’s. Similar to AWS, the traffic is steady leading up to the event. The traffic during the event can clearly be seen to follow a daily rhythm. The peak-to-baseline ratio matches that of the AWS graph (baseline of 3.1 kpackets/sec, peak of 25kpackets/sec, shows an 8.3× increase; AWS’s was 9×). Rate-limiting clamped down on the most egregious traffic, containing costs. For our last chart, we took our AWS statistics and did the following:\nWe stretched the timescale: we went as far back as 2016-11-01 and as far forward as mid-January, 2017. We examined traffic daily, not hourly, to reduce the spikiness. It was easy to mark the beginning of the event (2016-12-13): it came in with a bang. It was difficult to mark the ending of the event: it went out with a whimper. There was no sudden cliff as traffic fell, rather, it was a slow dwindling of traffic. We chose, for better or worse, to delineate the end of the event by marking a local minimum of traffic (2017-01-12). Note that traffic remained significantly above the baseline after that point. overly-annotated chart of NTP data transfer\nHow much did the NTP event cost us? By our reckoning, the event lasted 30 days, and the average amount of daily traffic above the baseline was 4.97 GiB, for a total of 149.1 GiB. Given that AWS charges $0.09 per GiB, the total cost of the Snapchat event for our AWS server was $13.42. We can extrapolate for our GCE server: the amount of traffic would be similar, but Google’s bandwidth is 33% more expensive ($0.12 vs. AWS’s $0.09), giving us an estimate of $17.90.\nThere were no additional costs for our German server (we did not exceed the bundled bandwidth).\n2.2 Are Virtual Machines Adequate NTP Servers? Yes. Are Virtual Machines adequate NTP servers? The short answer is, “yes”, but the long answer is more complex.\nFirst, timekeeping within a VM is complicated (see the excellent VMware Paper for a thorough analysis): there are two ways that a computer (VM) measures the passage of time (tick counting \u0026 tickless timekeeping). Tick counting can result in “lost ticks”, which means the clock loses time (it slows down compared to true time), and tickless timekeeping, which, although eliminates the “lost ticks” problem, brings its own set of baggage with it (e.g. the hypervisor must know or be notified that the VM is using tickless timekeeping).\nThe result is that a VM’s clock can drift quite a bit. In one serverfault.com post, a contributor stated,\nin the pure-VM environment would probably be within, oh, 30 to 100ms of true\nAnd another contributor added:\nRunning NTP in a virtualised [sic] environment, you’ll be luck to achieve 20ms accuracy (that’s what we’ve done using VMware)…. NTP servers should always be on physical hosts\nBut that flies in the face of our experience. Our VM NTP server on Google’s cloud has excellent timekeeping: 99% of its time is within +2ms/-2ms of true. Don’t take our word for it, look at the chart [NTP charts] :\nDisclaimer: we cherry-picked our best NTP server; our other servers aren’t as accurate. Our Hetzner server, via IPv6, is typically +4ms/-4ms, and via IPv4 is typically +10ms/-10ms, our AWS server +20ms/-20ms, and so is our Microsoft Azure server.\nOur numbers are surprisingly good especially given that the monitoring system used to collect the numbers is susceptible to random network latencies:\nThe monitoring system works roughly like an SNTP (RFC 2030) client, so it is more susceptible by random network latencies between the server and the monitoring system than a regular ntpd server would be.\nThe monitoring system can be inaccurate as much as 10ms or more.\nWe caution the reader not to extrapolate the efficacy of various IaaSes based on the timekeeping of a VM on that IaaS. For example, it would be unwise to assume that Google Cloud is 5 times better than AWS because our Google VM’s NTP server is 5 times more accurate. In the Google vs. AWS case, our AWS NTP server is a lower-tier t2.micro, A Burstable Performance Instance which Amazon recommends against using for applications which consistently require CPU:\nIf you need consistently high CPU performance for applications such as video encoding, high volume websites or HPC applications, we recommend you [don’t use Burstable Performance Instance].\nWe also find that the network plays a role in the accuracy of the NTP servers. We suspect that is one of the reasons that our Microsoft Azure VM, which is located across the globe (from the Los Angeles-based monitoring station) in Singapore, has among the least accurate metrics. Which leads into our next topic.\n2.3 Sometimes It’s the Network, not the VM We were convinced that the network was often a bigger factor than virtualization in NTP latency, but how to prove it? If only we were able to have two exactly identical NTP servers on different networks and measure differences in latency. But how to accomplish that?\nDual-stack.\nThat’s right: one of our NTP servers (shay.nono.io) was dual-stack: it had both IPv4 and IPv6 addresses on its single ethernet interface, which eliminated differences in IaaSes, operating systems, RAM, cores, etc… as factors in latency.\nAnd the difference between the IPv4 and IPv6 was striking:\nThe IPv6 stack never dropped a packet; The IPv4 stack dropped a 4 packets over the course of three days. The IPv6 stack is tightly concentrated in the +3/-4ms range; the IPv4 stack, on the other hand, sprawls across the +20/-20ms range. The upshot is that the network can affect the latency as much as threefold. In the charts below, you can see differences between the IPv6 (top chart) and IPv4 (bottom chart). Note that the latency (offset) is measured on the left axis:\nThere are those who might say, “But the comparison is unfair — IPv6 has builtin QOS (Quality Of Service). Of course IPv6 would have better performance!”\nRather than argue the relative merits of IPv6 vs. IPv4, we would prefer to present a counter-example: we have a second timeserver (time-home.nono.io) that is dual stack, and it exhibits the opposite behavior (the IPv4 latency is better):\nThe IPv4 stack is concentrated on the -2/-10ms range (8 millisecond spread); the IPv6 traffic has spread that’s twice as wide, +10/-5ms (15 millisecond spread). Although this server is not as an extreme example of latency differences as the previous one, it supports our contention that the network can have a powerful effect on latency.\nFor your consumption, we present the charts below, you can see differences between the IPv6 (top chart) and IPv4 (bottom chart). Note that the latency (offset) is measured on the left axis:\nFootnotes [Traffic increase]\nOur costs increased 50% for a simple reason: the amount of NTP traffic increased. When we wrote the original blog post in two and a half years ago in June 2014, our monthly outbound traffic was 332 GiB; in January 2017 it had climbed to 542 GiB (for our GCE NTP server, 520 GiB for our AWS NTP server).\nWe are not sure the reason behind the increase, but it tracks closely with the growth of the public cloud infrastructure. According to IDC:\nThe public cloud IaaS market grew 51% in 2015. IDC expects this high growth to continue through 2016 and 2017 with a CAGR of more than 41%.\n[AWS Network Data]\nWe needed more data. We turned to Amazon’s Usage Reports.\nAWS Console → My Account → Reports → AWS Usage Report → Amazon Elastic Compute Cloud\nWe ran a report requesting the following information:\nUsage Types: DataTransfer-Out-Bytes Operation: All Operations Time Period: Custom date range from: Dec 1 2016 to: Jan 18 2017 Report Granularity: Hours click Download report (CSV) We downloaded the report and imported it into a spreadsheet (Google Sheets).\n[NTP statistics]\nNTP statistics are derived from two servers in the NTP pool (ns-aws.nono.io and ns-gce.nono.io) by using the ntpq command after they had been running for a week. Here is the output of the command when run on the ns-gce.nono.io NTP server:\n$ ntpq -c sysstats uptime: 601851 sysstats reset: 601851 packets received: 1874993118 current version: 1455746472 older version: 418760288 bad length or format: 534317 authentication failed: 361395 declined: 393 restricted: 15845 rate limited: 123140728 KoD responses: 15874848 processed for time: 2554 These numbers are necessary but not sufficient: we want to know, “how much will my data transfer cost each month?” [AWS data transfer cost] To determine that, we’ll need to know Amazon data transfer pricing, our inbound and especially outbound traffic, and the size of NTP packets in bytes.\nWe derive additional statistics using the above numbers. In the above example (which was the output from our Google NTP server on Thu Jan 5 04:02:45 UTC 2017) we were able to calculate the traffic in GiB per month as follows:\npackets sent = packets received - bad length or format - authentication failed - declined - restricted - rate limited = 1750940440 [NTP packets sent]\npackets received per second = packets received / uptime = 3115.38 packets sent per second = packets sent / uptime = 2909.26 packets received per month = packets received / uptime × 60 secs/min × 60 mins/hr × 24 hr/day × 30.436875 day/month = 8192651756 packets sent per month = packets sent / uptime × 60 secs/min × 60 mins/hr × 24 hr/day × 30.436875 day/month = 7650612225 gigabytes received per month = packets received per month × bytes per packet [NTP packet size] × gigabytes per byte = 579.88 gigabytes sent per month = packets sent per month × bytes per packet × gigabytes per byte = 541.51 The average number of days per month in the Gregorian calendar is 365.2425 / 12 = 30.436875. It would be irresponsible for a post about NTP to casually peg the number of days in a month to 30. Respect time.\nThe raw data from which the numbers in this blog post are derived can be found in a spreadsheet. The organization is haphazard. The data contained therein is released into the public domain.\n[AWS data transfer]\nAWS charges $0.09 per GiB for Data Transfer (bandwidth) OUT from Amazon EC2 us-east-1 (Virginia) region to Internet. Inbound data transfer is free.\nAs with much of Amazon pricing, this is a rule-of-thumb and subject to qualifiers:\nAmazon has volume discounts; e.g. traffic above 10TB/month is charged at $0.085/GiB, above 50TB/month is charged at $0.07/GiB, etc… Amazon charges much less for traffic to other Amazon datacenters (e.g. outbound traffic to another AWS Region is $0.02/GiB) Inbound traffic is not always free; Amazon charges, for example, $0.01/GiB for traffic originating from the same Availability Zone using a public or Elastic IPv4 address In spite of these qualifiers, we feel the $0.09/GiB is an appropriate value to use in our calculations.\nAWS measures their data transfer pricing in terms of GiB (230) instead of GB (109) (although their documentation refer to the units as “GB”).\n[Google data transfer]\nGoogle charges $0.12 per GiB for Data Transfer (bandwidth) OUT from Google Cloud Platform.\nSimilar to AWS’s data transfer pricing, there are qualifiers:\nGoogle has volume discounts (e.g. with tiers at 1TB ($0.11) and 10TB ($0.08)) Egress to China (but not Hong Kong) is almost double (e.g. $0.23). Network Egress to Australia is also more expensive ($0.19) Prices do not apply to Google’s Content Deliver Network (CDN) service We find Google’s pricing to be simpler than Amazon’s — Google’s pricing is uniform across Google’s datacenters, whereas Amazon’s data transfer costs can vary by region (datacenter).\nGoogle explicitly defines their data transfer pricing in terms of GiB (230) instead of GB (109). A GB is 0.931 the size of a GiB (the GiB is bigger, and handsomer too, I might add):\nDisk size, machine type memory, and network usage are calculated in gigabytes (GB), where 1 GB is 230 bytes\n[DigitalOcean bandwidth metering]\nDigitalOcean’s bandwidth pricing is more aspirational than actual: They have not yet implemented bandwidth metering, though they plan to do so in the future. Per their response to our ticket opened on 2017-01-17 (boldface ours):\nWe do not have a pricing scheme set up yet for bandwidth. As such, we do not actually charge for it. Our engineering team is working on a solution for this, but it has not been given an ETA. We often suggest customers who are heavy BW utilizes to just be kind to our platform \u0026 be mindful when we reach out with requests to slow down just a bit, as times, it can become disruptive to customers downstream.\nSome may be tempted, knowing that the bandwidth is not measured, to opt for the lower-tier $5/month server (with 1TB bandwidth) to provide an NTP server to the community. We find such a decision to be on ethically shaky ground, for we know that our bandwidth would most likely exceed that amount (we estimate 1.1 - 1.4 TiB aggregate inbound and outbound), and we’d be taking advantage of DigitalOcean’s momentary bandwidth blind spot. As such, any benefits we would provide to the community would be, in a sense, fruit of a poisonous tree.\n[NTP packets sent]\nAlthough ntpq -c sysstats does not display the number of packets sent (only the number of packets received), the number can be calculated from the remaining fields, i.e. the number of packets sent minus the number of packets dropped. According to opsenswitch.net, these are the categories of packets that are dropped:\nbad length or format authentication failed declined restricted rate limited [NTP packet size]\nIPv4 NTP packets are 76 octets (bytes) (IPv6 NTP packets are 96 octets, but we ignore the IPv6 packet size for the purposes of our calculations — the two servers (AWS \u0026 Google) from which we gather statistics are IPv4-only).\nWe calculate the size as follows: we know the size of an NTP packet is almost always 48 bytes (it can be longer; the NTP RFC allows for for extension fields, key identifiers, and digests in the packet, but in practice we rarely see those fields populated).\nThe NTP packet is encapsulated in a User Datagram Protocol (UDP) packet, which adds 8 octets to the length of the packet, bringing the total length to 56.\nThe UDP packet in turn is encapsulated in an IP (IPv4 or IPv6) packet. IPv4 adds 20 octets to the length of the packet, bringing the total to 76. IPv6 adds 40 octets to the length, for a total of 96.\nThe IP packet in turn is encapsulated in an Ethernet II packet. The Ethernet II header does not include the 7-octet Preamble, the 1-octet Start of frame delimiter, the optional 802.1Q tag (we’re not using VLAN-tagging), nor the 4-octet Frame check sequence. It does include the 6-octet MAC destination and the 6-octet source, as well as the 2-octet ethertype field, which adds 14 octets to the length, for a grand total packet size of 90 bytes for IPv4 and 110 bytes for IPv6.\nTo confirm our calculations, we turn to tcpdump, a popular network sniffer (a tools which listens, filters, and decodes network traffic). We use it to expose the lengths of an IPv4 NTP packet.\nThe Ethernet II packet has a length of 90 octets (“length 90”) The IPv4 portion has a length of 76 octets (“length 76”) The NTP packet has a length of 48 octets (“NTPv4, length 48”) $ sudo tcpdump -ennv -i vtnet0 -c 1 port ntp tcpdump: listening on vtnet0, link-type EN10MB (Ethernet), capture size 262144 bytes 07:42:13.181157 d2:74:7f:6e:37:e3 \u003e 52:54:a2:01:66:7d, ethertype IPv4 (0x0800), length 90: (tos 0x0, ttl 44, id 0, offset 0, flags [DF], proto UDP (17), length 76) 107.137.130.39.123 \u003e 172.31.1.100.123: NTPv4, length 48 Client, Leap indicator: clock unsynchronized (192), Stratum 0 (unspecified), poll 4 (16s), precision -6 Root Delay: 1.000000, Root dispersion: 1.000000, Reference-ID: (unspec) Reference Timestamp: 0.000000000 Originator Timestamp: 0.000000000 Receive Timestamp: 0.000000000 Transmit Timestamp: 3692878933.085937805 (2017/01/08 07:42:13) Originator - Receive Timestamp: 0.000000000 Originator - Transmit Timestamp: 3692878933.085937805 (2017/01/08 07:42:13) 1 packet captured 2 packets received by filter 0 packets dropped by kernel Note the flags passed to tcpdump:\n-e capture the Ethernet frame, needed for Ethernet II length. -nn don’t lookup hosts, don’t lookup ports -v breaks out the size of the UDP packet (56 octets) -i vtnet0 specifies a particular Ethernet interface (typically eth0 on Linux) -c 1 capture one packet then exit port ntp listen for IPv4 packets whose source or destination port is NTP’s (123) To capture an IPv6 packet, use this variation:\nsudo tcpdump -ennv -i vtnet0 -c 1 ip6 and port ntp AWS does not bill for the layer 2 (data link layer, Ethernet II) of traffic, only for the IP portion. In other words, NTP packets have a length of 76 octets (as far as billing measurement is concerned). [AWS Network Metrics]\nGoogle, too, does not bill for layer 2 traffic, only for IP traffic. [Google Network Metrics]\n[AWS Network Metrics]\nWe are confident that AWS charges only for the IP-portion of network packets and not the Ethernet portion, but we were unable to find this explicitly in writing. We deduced it by running the following test.\nFirst, we measured the amount of NTP traffic over a one-hour period using the following command on our AWS NTP server. We kicked off the command on Wed Jan 25 05:00:00 UTC 2017:\nntpq -c sysstat; sleep 3600; ntpq -c sysstat; date ... Wed Jan 25 06:00:00 UTC 2017 The number of inbound packets received during the hour was 9,961,945. Now that we know the number of packets, we need to find out the number of bytes. And then it’s simple math: we divide the number of bytes by the number of packets. If the number is close to 90, then AWS is measuring the Ethernet frame, 76, the IP frame. We expect the number of inbound bytes to be approximately 757,107,820 (76 × 9,961,945).\nWe generate an AWS Usage report of inbound bytes:\nAWS Console → My Account → Reports → AWS Usage Report → Amazon Elastic Compute Cloud\nCalamity has struck! The number of inbound bytes from 05:00-06:00 is 657,882,293 is much too small. Not even close. Not even possible. The absolute minimum number of bytes is 757,107,820 (that’s the number of NTP packets × the minimum NTP packet size, 76 bytes). It’s possible to have more traffic (e.g. larger NTP packets, non-NTP traffic (ssh, DNS)), but not less.\nHere’s a snippet of the CSV downloaded from AWS:\nService, Operation, UsageType, Resource, StartTime, EndTime, UsageValue ... AmazonEC2,RunInstances,DataTransfer-In-Bytes,,01/25/17 05:00:00,01/25/17 06:00:00,657882293 ... With the numbers we have, there are ~66 bytes per packet, and we need to get to 76 bytes per packet.\nMaybe the report is in a different time zone? No, we know that “All usage reports for AWS services are in GMT”.\nAha! We forgot the Usage Type DataTransfer-Regional-Bytes. We run that report and see the following data:\nService, Operation, UsageType, Resource, StartTime, EndTime, UsageValue ... AmazonEC2,InterZone-In,DataTransfer-Regional-Bytes,,01/25/17 05:00:00,01/25/17 06:00:00,2052 AmazonEC2,PublicIP-In,DataTransfer-Regional-Bytes,,01/25/17 05:00:00,01/25/17 06:00:00,76761576 Here is an interesting tidbit: 10% of the inbound NTP traffic originates from within the Amazon cloud.\nWe decide to create a chart to visually express how closely our NTP server’s statistics matches AWS’s data transfer metrics assuming that NTP packets are 76 bytes:\nThe NTP server’s statistics are collected every five minutes, and are thus a line. The AWS data transfer statistics are coarser, only by the hour, and show up as purple dots. It’s evident that the two correlate, within a few percentage points.\nWe expected the AWS’s numbers to exceed ours by a slight amount, for our numbers collected via ntpq only report the NTP traffic, and we know that our server carries other traffic as well (it’s a DNS server, too); however, we found the opposite to be true: the traffic reported by AWS was consistently smaller than our NTP traffic, albeit by a small amount. We are not sure why, and are presenting this as a mystery.\nTo collect our statistics, we ran the following on our NTP server:\nwhile :; do /var/vcap/packages/ntp/bin/ntpq -c sysstat \u003e /tmp/ntp.$(date +%s) sleep 300 done We waited several hours, then collated everything:\ncd /tmp/ grep -h received ntp.* | awk '{print $3}' \u003e five_minute_intervals.ntp We uploaded the file into Google Sheets, calculate the delta between the five minute intervals, created a rolling hourly sum, and compared it with the output of the AWS usage report.\n[Google Network Metrics]\nWe are confident that Google charges only for the IP-portion of network packets and not the Ethernet portion, but we were unable to find this explicitly in writing. Instead, we inferred this via the manner in which Google measures packet size, and were so pleased with our methodology that we would like to share it.\nFirst, we brought up in our browser the Google Compute Engine Dashboard, twice. Then we selected our NTP server VM, twice. On one, we selected, “Network Packets”, and the other we selected “Network Bytes”. The following is a mash-up of the two charts:\nHaving both numbers allowed us to calculate the number of bytes per packet. If the number was close to 90 bytes per packet, then Google was including the Ethernet/data link layer. If the number was close to 76 bytes per packet, then Google was only counting the IPv4 portion of the packet.\nOur numbers? 76.19 and 76.06, within 0.25% of 76 bytes. Our conclusion? Google is only counting the IPv4 portion of the packet. (You may ask why we are not concerned that the average number of bytes per packet is not exactly 76 bytes. The answer? The servers carry traffic other than NTP (e.g. the Google server is both a DNS server and a Concourse continuous integration (CI) server, which often have packet sizes other than 76 bytes. Also, a small portion of NTP packets are greater than 76 bytes).\n[NTP charts]\nThe NTP Pool project provides publicly-accessible charts for the servers within the pool. Here are links to the charts of the servers that we maintain and to their ntpd (and, in one case, their chronyd) configuration files:\nGoogle server, US (ns-gce.nono.io) (ntp.conf) AWS server, US (ns-aws.nono.io) (ntp.conf) Azure server, Singapore (ns-azure.nono.io) (ntp.conf) Hetzner server, Germany (shay.nono.io) (ntp.conf): IPv4 IPv6 Comcast server, US (time-home.nono.io) (chrony.conf): IPv4 IPv6 Corrections \u0026 Updates 2017-02-04\nA quote on the mechanism that pool.ntp.org uses to select servers was missing the phrase, “just on those”. The quote has been corrected.\nThe calculation for cost of aggregate data transfer for the entire US pool.ntp.org did not take into account tiered pricing. Pricing was adjusted: original cost was $5,640, adjusted cost is $4,078.\nPhrasing was changed to improved readability.\nWe removed a comment that pointed out we had not gathered statistics for our Azure NTP server; it seemed pointless.\n2017-02-01\nThe post mis-characterized the mechanism behind the NTP pool as “round-robin DNS”; the mechanism is more sophisticated: It targets the users to servers in/near their country and does a weighted round-robin just on those servers.\nAsk Bjørn Hansen said:\nThe system is a little more sophisticated than just round-robin DNS. It targets the users to servers in/near their country and does a weighted round-robin just on those servers.\nWe have added sections describing our motives for operating NTP servers and encouraging others to join the pool. Thanks Leo Bodnar.\nWe wrongly encouraged NTP pool servers to use Google’s NTP servers as upstream providers. We now warn against using Google’s NTP servers, and provide reasons why (leap seconds). Thanks Joseph B, Ask.\nWe added statistics regarding the aggregate netspeed for the US zone.\nNTP Pool operators suggested the following IaaSes:\nAmazon Lightsail Linode (we’ve had positive experience with Linode) Vultr BuyVM Ramnode LunaNode Atlantic ARP Networks (we’ve had positive experience with ARP Networks) Scaleway ","wordCount":"6421","inLanguage":"en","image":"https://nono.io/images/brian_cunnie_profile.jpg","datePublished":"2017-01-28T12:38:13-08:00","dateModified":"2017-01-28T12:38:13-08:00","author":{"@type":"Person","name":"Brian Cunnie"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.nono.io/post/ntp-costs-500/"},"publisher":{"@type":"Organization","name":"Brian Cunnie's Technical Blog","logo":{"@type":"ImageObject","url":"https://blog.nono.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.nono.io/ accesskey=h title="Brian Cunnie's Technical Blog (Alt + H)">Brian Cunnie's Technical Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://blog.nono.io/>Home</a>&nbsp;»&nbsp;<a href=https://blog.nono.io/post/>Posts</a></div><h1 class="post-title entry-hint-parent">Why Is My NTP Server Costing $500/Year? Part 3</h1><div class=post-meta><span title='2017-01-28 12:38:13 -0800 -0800'>January 28, 2017</span>&nbsp;·&nbsp;31 min&nbsp;·&nbsp;Brian Cunnie&nbsp;|&nbsp;<a href=https://github.com/cunnie/cunnie.github.io/tree/master/content/post/ntp-costs-500.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>When <a href="https://news.ycombinator.com/item?id=13249562">Hacker News picked up Part
1</a> of our series of blog posts on
running public NTP servers, a contributor said, &ldquo;I wish he&rsquo;d explained &mldr; what
they ultimately did (since there&rsquo;s no part 3 that I can find).&rdquo;</p><p>We had dropped the ball — we had never concluded the series, had never written
part 3, had never described the strategies to mitigate the data transfer costs.</p><p>This blog post remedies that oversight; it consists of two parts: the first part
addresses strategies to reduce the cost of running an NTP server, and the second
part discusses side topics (aspects of running an NTP server).</p><p>Perhaps the most dismaying discovery of writing this blog post was the
realization that the title is no longer accurate — rather than costing us
$500/year, our most expensive NTP server was costing us more than $750/year in
data transfer charges. <sup><a href=#cost_doubling>[Traffic increase]</a></sup></p><h2 id=table-of-contents>Table of Contents<a hidden class=anchor aria-hidden=true href=#table-of-contents>#</a></h2><ul><li><a href=/post/ntp-costs-500/#previous_posts>0. Previous Posts (Parts 1 & 2)</a></li><li><a href=/post/ntp-costs-500/#reducing_costs>1. Reducing the Cost of Running an NTP Server</a><ul><li><a href=/post/ntp-costs-500/#statistics>1.0 Statistics (traffic)</a></li><li><a href=/post/ntp-costs-500/#infrastructure>1.1 The Right Infrastructure Can Drop the Data Transfer Costs to $0</a></li><li><a href=/post/ntp-costs-500/#speed>1.2 Connection Speed Setting</a></li><li><a href=/post/ntp-costs-500/#geography>1.3 Geographical Placement</a></li><li><a href=/post/ntp-costs-500/#rate_limiting>1.4 Rate Limiting</a></li><li><a href=/post/ntp-costs-500/#join>1.5 Join the Pool</a></li></ul></li><li><a href=/post/ntp-costs-500/#side_topics>2. Side Topics</a><ul><li><a href=/post/ntp-costs-500/#google_ntp>2.0 The Cavalry is Coming: Google&rsquo;s Public NTP Servers</a></li><li><a href=/post/ntp-costs-500/#how_much>2.1 The Snapchat Excessive NTP Query Event Cost $13 - $18 (Per Server)</a></li><li><a href=/post/ntp-costs-500/#adequate>2.2 Are Virtual Machines Adequate NTP Servers? Yes.</a></li><li><a href=/post/ntp-costs-500/#network>2.3 Sometimes It&rsquo;s the Network, not the VM</a></li></ul></li><li><a href=/post/ntp-costs-500/#footnotes>Footnotes</a></li><li><a href=/post/ntp-costs-500/#corrections>Corrections & Updates</a></li></ul><h2 id=0-previous-posts><a name=previous_posts>0. Previous Posts</a><a hidden class=anchor aria-hidden=true href=#0-previous-posts>#</a></h2><p>These posts provide background, but reading them isn&rsquo;t necessary:</p><ul><li><p><a href=https://content.pivotal.io/blog/why-is-my-ntp-server-costing-500-year-part-1>Why Is My NTP Server Costing $500/Year? Part
1</a>.
We analyze our sudden increase in AWS data transfer charges and conclude that
adding our server into the NTP pool is the sole reason for the data transfer
increase.</p></li><li><p><a href=https://content.pivotal.io/blog/why-is-my-ntp-server-costing-me-500-year-part-2-characterizing-the-ntp-clients>Why Is My NTP Server Costing Me $500/Year? Part 2: Characterizing the NTP
Clients</a>.
We characterize the demand that each NTP client places on an NTP server, by
operating system. To our surprise, FreeBSD and Ubuntu place the greatest demand
on the NTP servers, and Windows and macOS the least.</p></li></ul><h2 id=1-reducing-the-cost-of-running-an-ntp-server><a name=reducing_costs>1. Reducing the Cost of Running an NTP Server</a><a hidden class=anchor aria-hidden=true href=#1-reducing-the-cost-of-running-an-ntp-server>#</a></h2><p>We maintain several servers in the pool.ntp.org project. These servers are
personal, not corporate, so we&rsquo;re quite sensitive to cost: we don&rsquo;t want to
spend a bundle if we don&rsquo;t have to. Also, these servers have roles other than
NTP servers (in fact, their primary purpose is to provide Domain Name System
(DNS) service and one is also a <a href=https://concourse.ci/>Concourse</a> continuous
integration (CI) server).</p><p>Which begs the question: given the expense, why do it? We have several motives:</p><ul><li><p>We have benefited greatly from the open source community, and providing this
service is a modest way of giving back.</p></li><li><p>Our day job is a developer on
<a href=https://en.wikipedia.org/wiki/BOSH_%28software%29>BOSH</a>, a tool which, at
its simplest, creates VMs in the cloud based on specifications passed to it in a
file. We use BOSH to deploy our NTP servers, and on at least two occasions we
have uncovered obscure bugs as a result.</p></li><li><p>On the rare occasions when our systems fail, often our first warning is an email
from the pool with the subject, &ldquo;NTP Pool: Problems with your NTP service&rdquo;. In
other words, being in the pool is a great monitoring system, or, at the very
least, better than nothing.</p></li></ul><h3 id=10-statistics-traffic><a name=statistics>1.0 Statistics (traffic)</a><a hidden class=anchor aria-hidden=true href=#10-statistics-traffic>#</a></h3><p>We have two NTP servers in the <a href=http://www.pool.ntp.org/en/>pool.ntp.org
project</a> whose country is set to &ldquo;United States&rdquo;
and whose connection speed is set to &ldquo;1000 Mbit&rdquo;. We have gathered the following
statistics <sup><a href=#ntp_statistics>[NTP statistics]</a></sup> over a
seven-day period (2017-01-21 15:00 UTC - 2017-01-28 15:00 UTC). Note that other
than data transfer pricing, the choice of underlying IaaS is unimportant
(assuming proper functioning of VM/disk/network). In other words, although the
Google Compute Engine (GCE) server carries more traffic than the Amazon Web
Services (AWS) server, the roles could have easily been reversed. The mechanism
underlying pool.ntp.org project (a multi-stage mechanism which &ldquo;targets the
users to servers in/near their country and does a weighted
<a href=https://en.wikipedia.org/wiki/Round-robin_DNS>round-robin</a> just on those
servers&rdquo;), is not a perfectly precise balancing mechanism (e.g. some clients
will &ldquo;stick&rdquo; to a server long after the pool.ntp.org record has updated).</p><table><thead><tr><th>Metric</th><th style=text-align:right>Amazon Web Services</th><th style=text-align:right>Google Compute Engine</th></tr></thead><tbody><tr><td>packets recvd/sec</td><td style=text-align:right>3033.89</td><td style=text-align:right>3115.38</td></tr><tr><td>packets sent/sec</td><td style=text-align:right>2794.60</td><td style=text-align:right>2909.26</td></tr><tr><td>GiB recvd/month</td><td style=text-align:right>564.71</td><td style=text-align:right>579.88</td></tr><tr><td>GiB sent/month</td><td style=text-align:right>520.17</td><td style=text-align:right>541.51</td></tr><tr><td>$ / GiB sent</td><td style=text-align:right>$0.09</td><td style=text-align:right>$0.12</td></tr><tr><td>$ / month</td><td style=text-align:right>$46.82</td><td style=text-align:right>$64.98</td></tr></tbody></table><div class="alert alert-success" role=alert><p>Although we present statistics for both inbound (NTP queries to our server) and
outbound traffic (NTP responses from our server), it is the outbound traffic
which is of particular interest to us, for the inbound traffic is usually free,
and the outbound traffic is usually metered (both AWS and GCE charge for
outbound traffic but not inbound). We have attempted to maintain a consistent
coloring scheme for our charts, using blue for inbound (free) and green for
outbound (metered). The mnemonic is that green, the color of US currency, is the
metric for which we pay.</p></div><h3 id=11-the-right-infrastructure-can-drop-the-data-transfer-costs-to-0><a name=infrastructure>1.1 The Right Infrastructure Can Drop the Data Transfer Costs to $0</a><a hidden class=anchor aria-hidden=true href=#11-the-right-infrastructure-can-drop-the-data-transfer-costs-to-0>#</a></h3><p>We believe that the choice of IaaS is the most important factor in determining
costs. For example, running an NTP server on GCE would have an annual cost of
$945.36, and running a similar server on DigitalOcean would cost $120 — that
represents an <strong>87% reduction in total cost and an annual savings of $825.36</strong>.</p><p>Our GCE configuration assumes a <em>g1-small</em> instance (1 shared CPU, 1.7 GiB) RAM
($0.019 per hour) running 24x7 (30% Sustained Use Discount) for a monthly cost
of $13.80. We then add the monthly data transfer costs of $64.98 for a monthly
total of $78.78, annual total of $945.36.</p><p>DigitalOcean offers <a href=https://www.digitalocean.com/pricing/>2TB/month</a> free on
their $10/month server; which should be adequate for the highest-trafficked NTP
servers in the pool (i.e. US-based, 1000 Mbit connection speed), which typically
have 1.1 - 1.4 TiB aggregate inbound and outbound traffic. <sup><a href=#digital_ocean_bandwidth_metering>[DigitalOcean bandwidth
metering]</a></sup></p><p>The NTP server need not reside in an IaaS; it is equally effective in a
residence. In fact, savvy home users who have a static IP, have set up an NTP
server, and who are comfortable sharing their NTP service with the community at
large are encouraged to join the pool.</p><p>It is particularly important when setting up an NTP server in a residence to
set an appropriate connection speed (see next section).</p><h3 id=12-connection-speed-setting><a name=speed>1.2 Connection Speed Setting</a><a hidden class=anchor aria-hidden=true href=#12-connection-speed-setting>#</a></h3><p>The most important tool to control the amount of traffic your NTP server
receives is to use the &ldquo;Net speed/connection speed&rdquo; setting in the <a href=https://manage.ntppool.org/manage>Manage
Servers</a> page. Its thirteen settings cover
more than three orders of magnitude.</p><p>Using our GCE NTP server as an example, at the highest setting (1000 Mbit), we
incur $64.98/month, and at the lowest setting (384 Kbit), $0.02/month.</p><p>The tool isn&rsquo;t precise: as previously mentioned, round-robin DNS is a blunt
instrument. We have two NTP servers based in the US whose connection speed is
set to 1000Mbit, and yet their outbound traffic differs by 4% (our AWS server
carries 4% less traffic than our GCE server). Using the connection speed will
get you in the ballpark, but don&rsquo;t expect precision.</p><p>According to pool.ntp.org:</p><blockquote><p>The net speed is used to balance the load between the pool servers. If your
connection is asymmetric (like most DSL connections) you should use the lower
speed.</p></blockquote><p>In our residence, our cable download speed is 150Mbps, but our upload speed is a
mere 10Mbps, so we set our pool.ntp.org&rsquo;s server setting to &ldquo;10Mbit&rdquo;</p><blockquote><p>The pool will only use a fraction of the &ldquo;netspeed setting&rdquo;</p></blockquote><p>The million-dollar question: what is the fraction, exactly? <strong>about 2%</strong>
worst-case scenario (i.e. server is placed in the United States). Assuming 90
bytes per NTP packet, 1,000,000,000 bits per Gbit, aggregating inbound and
outbound (2Gbps total aggregate bandwidth per server), we calculate the
bandwidth to be, on our 4 servers, 2.10%, 2.16%, 0.67%, and 0.35%.</p><p>Our home connection is not metered, so we don&rsquo;t incur bandwidth charges (i.e.
it&rsquo;s free).</p><figure><img loading=lazy src=https://cloud.githubusercontent.com/assets/1020675/22177615/4e8c1940-dfd6-11e6-9a2f-a33e0bc07c8d.png alt="The pool.ntp.org project&rsquo;s menu option on the server management page allows you to throttle the traffic on your server. The lower your connection speed, the less traffic your server will receive, and the lower the bandwidth costs."><figcaption><p>The pool.ntp.org project&rsquo;s menu option on the server management page allows you to throttle the traffic on your server. The lower your connection speed, the less traffic your server will receive, and the lower the bandwidth costs.</p></figcaption></figure><p>The aggregate &ldquo;netspeed&rdquo; for the US zone is <a href="https://community.ntppool.org/t/feedback-why-is-my-ntp-server-costing-me-500-per-year/146/5?u=cunnie">86798173
kbps</a>. This implies the following:</p><ul><li>Our GCE server (and also our AWS server) accounts for 1.15% of the US NTP pool traffic</li><li>The entire US NTP pool is queried 270,376× every second</li><li>The entire US NTP pool responds (assuming rate-limiting) 252,495× every second</li><li>The entire US NTP pool transfers 45.9TiB in NTP responses monthly</li><li>Using GCE, the entire US NTP pool would cost $4,078 in monthly data transfer costs (GCE&rsquo;s pricing tiers for $/GiB-month are $0.12 for TiB 0-1, $0.11 for TiB 1-10, $0.08 for TiB 10+)</li></ul><h3 id=13-geographical-placement><a name=geography>1.3 Geographical Placement</a><a hidden class=anchor aria-hidden=true href=#13-geographical-placement>#</a></h3><p>The placement of the NTP server has dramatic effect on the bandwidth and cost.
For example, our German NTP server&rsquo;s typical outbound monthly traffic is 82.63
GiB, which is <strong>85% less bandwidth</strong> than our US/Google server&rsquo;s monthly 541.51
GiB.</p><figure><img loading=lazy src="https://docs.google.com/spreadsheets/d/1yvHpDjaD3pOV2C0-Zb9ljdvdWxDc9V8oZnF5IvfLZew/pubchart?oid=1739925391&amp;format=image"></figure><p>A note about placement: The pool.ntp.org project allows participants to place
servers in various zones. A &ldquo;zone&rdquo; consists of a country and that country&rsquo;s
continent (there are non-geographic vendor zones as well, e.g. Ubuntu has a zone
&ldquo;ubuntu.pool.ntp.org&rdquo;, but those zones fall outside the scope of this
discussion). The pool will &ldquo;<a href=http://www.pool.ntp.org/zone>will try finding the closest available
servers</a>&rdquo; for NTP clients, which may or may not be
in the same zone.</p><p>We don&rsquo;t have NTP servers on all continents (we&rsquo;re missing Antarctica, Oceania,
and South America), so we don&rsquo;t have insight on the bandwidth requirements for
NTP servers placed there; however, we do know that our server in Asia is not as
stressed as our US/Google server (166.00 GiB vs. 541.51 Gib).</p><p>We were curious why the load on our German server was so light, and we
believe that part of the reason is that there is much greater participation
in the pool.ntp.org project in Europe. Europe, with 2,686 servers, has almost
exactly 3× North America&rsquo;s meager 895 servers.</p><figure><img loading=lazy src=https://cloud.githubusercontent.com/assets/1020675/22177616/4e8c8772-dfd6-11e6-8ac1-e789b5ca457a.png alt="The ntp.pool.org&rsquo;s servers, broken out by continent."><figcaption><p>The ntp.pool.org&rsquo;s servers, broken out by continent.</p></figcaption></figure><h3 id=14-rate-limiting><a name=rate_limiting>1.4 Rate Limiting</a><a hidden class=anchor aria-hidden=true href=#14-rate-limiting>#</a></h3><p>We have found that by enabling NTP&rsquo;s <a href=http://doc.ntp.org/4.2.4/accopt.html>rate
limiting</a> feature can reduce the outbound
traffic by 9.56% on average (as high as 16.60% for our German server and as low
as 6.57% for our GCE server).</p><p>But don&rsquo;t be fooled into thinking that rate limiting&rsquo;s sole purpose is to reduce
traffic a measly 9%. Rate limiting has an unintended side benefit: it throttles
traffic during excessive NTP query events. For example, Snapchat released a
broken iOS client which placed excessive load on NTP servers (see
<a href=/post/ntp-costs-500/#how_much>below</a> for more information) in mid-December. Traffic to our AWS
server, normally a steady 600 MiB / hour, spiked viciously. In one particular
hour (12/18/2016 04:00 - 0500 UTC), the inbound traffic climbed almost ninefold
to 4.97 GiB! Fortunately, rate limiting kicked in, and rejected 69.30% of the
traffic, which reduced our cost for that hour by 69.30%, for we are charged only
for outbound traffic.</p><p>Rate limiting is enabled by adding the <code>kod</code> and <code>limited</code> directives in the NTP
servers configuration file. In our servers&rsquo; configuration, we use the
<a href=http://support.ntp.org/bin/view/Support/AccessRestrictions#Section_6.5.1.1.3.>suggested</a>
restrictions for servers &ldquo;who allow others to get the time&rdquo; and &ldquo;to see your
server status information&rdquo;. Links to our <code>ntp.conf</code> files can be found
<a href=/post/ntp-costs-500/#ntp_charts>here</a>.</p><pre tabindex=0><code>restrict    default limited kod nomodify notrap nopeer
restrict -6 default limited kod nomodify notrap nopeer
</code></pre><p>Disclaimer: we&rsquo;re not 100% sure it was rate-limiting that clamped down on our
outbound traffic. There is a small chance that it was another factor, e.g. ntpd
was overwhelmed and dropped packets, AWS (or GCE) stepped in and limited
outbound NTP traffic. We haven&rsquo;t run the numbers.</p><h2 id=15-join-the-pool><a name=join>1.5 Join the Pool</a><a hidden class=anchor aria-hidden=true href=#15-join-the-pool>#</a></h2><p>We encourage those with NTP servers with static IPs to join the pool; the
experience has been personally rewarding and professionally enriching (how many
can claim to operate a service with thousands of requests per second?).</p><p>We also lay out a cautionary tale: when joining, keep an eye to costs,
especially bandwidth. Opting for a lower connection speed initially (e.g.
10Mbps), and ratcheting it up over time is a prudent course of action.</p><h2 id=2-side-topics><a name=side_topics>2. Side Topics</a><a hidden class=anchor aria-hidden=true href=#2-side-topics>#</a></h2><h3 id=20-the-cavalry-is-coming-googles-public-ntp-servers><a name=google_ntp>2.0 The Cavalry is Coming: Google&rsquo;s Public NTP Servers</a><a hidden class=anchor aria-hidden=true href=#20-the-cavalry-is-coming-googles-public-ntp-servers>#</a></h3><p><a href=https://cloudplatform.googleblog.com/2016/11/making-every-leap-second-count-with-our-new-public-NTP-servers.html>Google has announced public NTP
servers</a>.
Over time, we suspect that this will reduce the load on the pool.ntp.org
project&rsquo;s servers.</p><div class="alert alert-warning" role=alert><p>Servers in the NTP Pool should <strong>not</strong> use Google&rsquo;s NTP servers as
upstream time providers, nor should they use any upstream provider which
&ldquo;smears&rdquo; the leap second.</p></div><p> </p><p>There is a schism in the community regarding <a href=https://en.wikipedia.org/wiki/Leap_second>leap
seconds</a>:</p><ul><li><p>The NTP pool supports the leap second, which is the UTC standard. The advantage
of the leap second is that every second is always the same length, i.e.
&ldquo;9,192,631,770 periods of the radiation emitted by a caesium-133 atom in the
transition between the two hyperfine levels of its ground state&rdquo;.</p></li><li><p>Google, on the other hand, <a href=https://developers.google.com/time/smear>smears the leap
second</a>, which lengthens the second by
13.9µs during the ten hours leading up to and following the leap seconds. Their
reasoning is, &ldquo;No commonly used operating system is able to handle a minute with
61 seconds&rdquo;.</p></li></ul><p>For readers interested in using Google&rsquo;s NTP service, the server is
<em>time.google.com</em>.</p><h3 id=21-the-snapchat-excessive-ntp-query-event-cost-13---18-per-server><a name=how_much>2.1 The Snapchat Excessive NTP Query Event Cost $13 - $18 (Per Server)</a><a hidden class=anchor aria-hidden=true href=#21-the-snapchat-excessive-ntp-query-event-cost-13---18-per-server>#</a></h3><p>In December Snapchat released a <a href=http://www.theregister.co.uk/2016/12/21/snapchat_coding_error_nearly_destroys_all_of_time_for_the_internet/>version of its iOS app that placed undue
stress</a>
on the pool.ntp.org servers.</p><p>This event caused <a href=https://community.ntppool.org/t/recent-ntp-pool-traffic-increase/18>great
consternation</a>
among the NTP server operators, and words such as &ldquo;decimated&rdquo;, &ldquo;server loss&rdquo;,
and &ldquo;sad&rdquo; were used.</p><p>The effect on two of our servers can be readily seen by our bandwidth graph.
First, our AWS server:</p><figure><img loading=lazy src="https://docs.google.com/spreadsheets/d/1yvHpDjaD3pOV2C0-Zb9ljdvdWxDc9V8oZnF5IvfLZew/pubchart?oid=355797491&amp;format=image"></figure><p>Our AWS chart presents hourly inbound and outbound traffic, measured in MiB.
Times are in UTC.</p><p>Note the following leading up to the event:</p><ul><li>Inbound traffic is fairly steady, averaging 611 MiB/hour, and so is outbound traffic, averaging 562 MiB/hr.</li><li>There&rsquo;s little difference between inbound and outbound traffic (i.e. ntpd&rsquo;s
rate-limiting is kicking in at a modest ~8%).</li></ul><p>Note the following about the event and its aftermath:</p><ul><li>The onset of the event was sudden: on 12/13/2016, inbound traffic jumped from
12.7 GiB the previous day to 18.2 GiB.</li><li>There were unexplained dips in traffic during the event. For example, on
12/16/2016 0:00 - 3:00 UTC, the inbound traffic fell below 300 MiB/hr. Not
only was this extremely low in the midst of an NTP excessive query-event, but
it would have been abnormally low during regular service.</li><li>The event had a long tail. Even though Snapchat released a fix, traffic hadn&rsquo;t
normalized by the end of December. Things had improved, but they hadn&rsquo;t gotten
gotten back to normal.</li></ul><p>Next, we present our GCE bandwidth chart:</p><figure><img loading=lazy src=https://cloud.githubusercontent.com/assets/1020675/21468123/ac026274-c9d2-11e6-8334-2f56e9c9d20f.png></figure><p>Our GCE chart presents per second inbound and outbound traffic, measured in
packets. Times are in EST.</p><p>Note the following:</p><ul><li>Unlike AWS, there were no unexplained dips in traffic.</li><li>Google smoothed the graph — it&rsquo;s not as jagged as AWS&rsquo;s.</li><li>Similar to AWS, the traffic is steady leading up to the event.</li><li>The traffic during the event can clearly be seen to follow a daily rhythm.</li><li>The peak-to-baseline ratio matches that of the AWS graph (baseline of 3.1 kpackets/sec, peak of 25kpackets/sec, shows an 8.3× increase; AWS&rsquo;s was 9×).</li><li>Rate-limiting clamped down on the most egregious traffic, containing costs.</li></ul><p>For our last chart, we took our AWS statistics and did the following:</p><ul><li>We stretched the timescale: we went as far back as 2016-11-01 and as far forward
as mid-January, 2017.</li><li>We examined traffic daily, not hourly, to reduce the spikiness.</li><li>It was easy to mark the beginning of the event (2016-12-13): it came in with a bang.</li><li>It was difficult to mark the ending of the event: it went out with a whimper. There
was no sudden cliff as traffic fell, rather, it was a slow dwindling of traffic. We chose,
for better or worse, to delineate the end of the event by marking a local minimum of
traffic (2017-01-12). Note that traffic remained significantly above the baseline after that point.</li></ul><figure><img loading=lazy src=https://cloud.githubusercontent.com/assets/1020675/22092441/6cacc986-ddb2-11e6-80d8-52f61d3d8364.png alt="overly-annotated chart of NTP data transfer"><figcaption><p>overly-annotated chart of NTP data transfer</p></figcaption></figure><p>How much did the NTP event cost us? By our reckoning, the event lasted 30 days,
and the average amount of daily traffic above the baseline was 4.97 GiB, for a
total of 149.1 GiB. Given that AWS charges $0.09 per GiB, the total cost of the
Snapchat event for our AWS server was <strong>$13.42</strong>. We can extrapolate for our GCE
server: the amount of traffic would be similar, but Google&rsquo;s bandwidth is 33%
more expensive ($0.12 vs. AWS&rsquo;s $0.09), giving us an estimate of <strong>$17.90</strong>.</p><p>There were no additional costs for our German server (we did not exceed the
bundled bandwidth).</p><h3 id=22-are-virtual-machines-adequate-ntp-servers-yes><a name=adequate>2.2 Are Virtual Machines Adequate NTP Servers? Yes.</a><a hidden class=anchor aria-hidden=true href=#22-are-virtual-machines-adequate-ntp-servers-yes>#</a></h3><p>Are Virtual Machines adequate NTP servers? The short answer is, &ldquo;yes&rdquo;, but the
long answer is more complex.</p><p>First, timekeeping within a VM is complicated (see the excellent <a href=http://www.vmware.com/pdf/vmware_timekeeping.pdf>VMware
Paper</a> for a thorough
analysis): there are two ways that a computer (VM) measures the passage of time
(tick counting & tickless timekeeping). Tick counting can result in &ldquo;lost
ticks&rdquo;, which means the clock loses time (it slows down compared to true time),
and tickless timekeeping, which, although eliminates the &ldquo;lost ticks&rdquo; problem,
brings its own set of baggage with it (e.g. the hypervisor must know or be
notified that the VM is using tickless timekeeping).</p><p>The result is that a VM&rsquo;s clock can drift quite a bit. In one
<a href=http://serverfault.com/questions/106501/what-are-the-limits-of-running-ntp-servers-in-virtual-machines>serverfault.com</a>
post, a contributor stated,</p><blockquote><p>in the pure-VM environment would probably be within, oh, 30 to 100ms of true</p></blockquote><p>And another contributor added:</p><blockquote><p>Running NTP in a virtualised [sic] environment, you&rsquo;ll be luck to achieve 20ms accuracy (that&rsquo;s what we&rsquo;ve done using VMware)&mldr;. NTP servers should always be on physical hosts</p></blockquote><p>But that flies in the face of our experience. Our VM NTP server on Google&rsquo;s cloud
has excellent timekeeping: <strong>99% of its time is within +2ms/-2ms of true</strong>.
Don&rsquo;t take our word for it, look at the chart
<sup><a href=#ntp_charts>[NTP charts]</a></sup> :</p><figure><img loading=lazy src=https://cloud.githubusercontent.com/assets/1020675/21662851/2a6a1622-d291-11e6-9ace-dd12baf6929c.png></figure><p>Disclaimer: we cherry-picked our best NTP server; our other servers aren&rsquo;t as
accurate. Our Hetzner server, via IPv6, is typically
<a href=http://www.pool.ntp.org/scores/2a01:4f8:c17:b8f::2>+4ms/-4ms</a>, and via IPv4 is
typically <a href=http://www.pool.ntp.org/scores/78.46.204.247>+10ms/-10ms</a>, our AWS
server <a href=http://www.pool.ntp.org/scores/52.0.56.137>+20ms/-20ms</a>, and so is our
<a href=http://www.pool.ntp.org/scores/52.187.42.158>Microsoft Azure server</a>.</p><p>Our numbers are surprisingly good especially given that the <a href=https://github.com/abh/ntppool/blob/b7d86c1a069e463748c2e54e9545af1183c293c0/docs/ntppool/en/tpl/server/graph_explanation.html#L3-L19>monitoring
system</a>
used to collect the numbers is susceptible to random network latencies:</p><blockquote><p>The monitoring system works roughly like an SNTP (RFC 2030) client, so it is
more susceptible by random network latencies between the server and the
monitoring system than a regular ntpd server would be.</p></blockquote><blockquote><p>The monitoring system can be inaccurate as much as 10ms or more.</p></blockquote><p>We caution the reader not to extrapolate the efficacy of various IaaSes based on
the timekeeping of a VM on that IaaS. For example, it would be unwise to assume
that Google Cloud is 5 times better than AWS because our Google VM&rsquo;s NTP server
is 5 times more accurate. In the Google vs. AWS case, our AWS NTP server is a
lower-tier <a href=https://aws.amazon.com/ec2/instance-types/>t2.micro</a>, A <a href=https://aws.amazon.com/ec2/instance-types/#burst>Burstable
Performance Instance</a> which
Amazon recommends against using for applications which consistently require CPU:</p><blockquote><p>If you need consistently high CPU performance for applications such as video
encoding, high volume websites or HPC applications, we recommend you [don&rsquo;t use
Burstable Performance Instance].</p></blockquote><p>We also find that the network plays a role in the accuracy of the NTP servers.
We suspect that is one of the reasons that our Microsoft Azure VM, which is
located across the globe (from the Los Angeles-based monitoring station) in
Singapore, has among the least accurate metrics. Which leads into our next
topic.</p><h3 id=23-sometimes-its-the-network-not-the-vm><a name=network>2.3 Sometimes It&rsquo;s the Network, not the VM</a><a hidden class=anchor aria-hidden=true href=#23-sometimes-its-the-network-not-the-vm>#</a></h3><p>We were convinced that the network was often a bigger factor than virtualization
in NTP latency, but how to prove it? If only we were able to have two exactly
identical NTP servers on different networks and measure differences in latency.
But how to accomplish that?</p><p>Dual-stack.</p><p>That&rsquo;s right: one of our NTP servers (shay.nono.io) was dual-stack: it had both
IPv4 and IPv6 addresses on its single ethernet interface, which eliminated
differences in IaaSes, operating systems, RAM, cores, etc&mldr; as factors in
latency.</p><p>And the difference between the IPv4 and IPv6 was striking:</p><ul><li>The IPv6 stack never dropped a packet; The IPv4 stack dropped a 4 packets over
the course of three days.</li><li>The IPv6 stack is tightly concentrated in the +3/-4ms range; the IPv4 stack,
on the other hand, sprawls across the +20/-20ms range.</li></ul><p>The upshot is that the network can affect the latency as much as threefold. In
the charts below, you can see differences between the IPv6 (top chart) and
IPv4 (bottom chart). Note that the latency (offset) is measured on the <em>left</em>
axis:</p><figure><img loading=lazy src=https://cloud.githubusercontent.com/assets/1020675/22265983/207b73e8-e233-11e6-9ee6-1a05f9d79998.png></figure><figure><img loading=lazy src=https://cloud.githubusercontent.com/assets/1020675/22265984/207f3834-e233-11e6-9557-de568939901f.png></figure><p>There are those who might say, &ldquo;But the comparison is unfair — IPv6 has builtin QOS
(Quality Of Service). Of course IPv6 would have better performance!&rdquo;</p><p>Rather than argue the relative merits of IPv6 vs. IPv4, we would prefer to
present a counter-example: we have a <em>second</em> timeserver (time-home.nono.io)
that is dual stack, and it exhibits the opposite behavior (the IPv4 latency is
better):</p><ul><li>The IPv4 stack is concentrated on the -2/-10ms range (8 millisecond spread); the
IPv6 traffic has spread that&rsquo;s twice as wide, +10/-5ms (15 millisecond spread).</li></ul><p>Although this server is not as an extreme example of latency differences as the
previous one, it supports our contention that the network can have a powerful
effect on latency.</p><p>For your consumption, we present the charts below, you can see differences
between the IPv6 (top chart) and IPv4 (bottom chart). Note that the latency
(offset) is measured on the <em>left</em> axis:</p><figure><img loading=lazy src=https://cloud.githubusercontent.com/assets/1020675/22265981/20673a68-e233-11e6-8ad9-4d0683b2edfd.png></figure><figure><img loading=lazy src=https://cloud.githubusercontent.com/assets/1020675/22265982/2068c888-e233-11e6-9d3b-ed109947e499.png></figure><h2 id=footnotes><a name=footnotes>Footnotes</a><a hidden class=anchor aria-hidden=true href=#footnotes>#</a></h2><p><sup><a name=cost_doubling>[Traffic increase]</a></sup></p><p>Our costs increased 50% for a simple reason: the amount of NTP traffic
increased. When we wrote the original blog post in two and a half years ago in
June 2014, our monthly outbound traffic was 332 GiB; in January 2017 it had
climbed to 542 GiB (for our GCE NTP server, 520 GiB for our AWS NTP server).</p><p>We are not sure the reason behind the increase, but it tracks closely with the
growth of the public cloud infrastructure. <a href="https://www.idc.com/getdoc.jsp?containerId=prUS41599716">According to
IDC</a>:</p><blockquote><p>The public cloud IaaS market grew 51% in 2015. IDC expects this high growth to
continue through 2016 and 2017 with a CAGR of more than 41%.</p></blockquote><p><sup><a name=aws_network_data>[AWS Network Data]</a></sup></p><p>We needed more data. We turned to Amazon&rsquo;s <a href=http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/usage-reports.html>Usage Reports</a>.</p><p><strong>AWS Console → My Account → Reports → AWS Usage Report → Amazon Elastic Compute Cloud</strong></p><p>We ran a report requesting the following information:</p><ul><li>Usage Types: <strong>DataTransfer-Out-Bytes</strong></li><li>Operation: <strong>All Operations</strong></li><li>Time Period: <strong>Custom date range</strong><ul><li>from: <strong>Dec 1 2016</strong></li><li>to: <strong>Jan 18 2017</strong></li></ul></li><li>Report Granularity: <strong>Hours</strong></li><li>click <strong>Download report (CSV)</strong></li></ul><p>We downloaded the report and imported it into a spreadsheet (<a href="https://docs.google.com/spreadsheets/d/1yvHpDjaD3pOV2C0-Zb9ljdvdWxDc9V8oZnF5IvfLZew/edit?usp=sharing">Google
Sheets</a>).</p><p><sup><a name=ntp_statistics>[NTP statistics]</a></sup></p><p>NTP statistics are derived from two servers in the NTP pool (ns-aws.nono.io and
ns-gce.nono.io) by using the <code>ntpq</code> command after they had been running for a
week. Here is the output of the command when run on the ns-gce.nono.io NTP
server:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>$ ntpq -c sysstats
</span></span><span style=display:flex><span>uptime:                601851
</span></span><span style=display:flex><span>sysstats reset:        601851
</span></span><span style=display:flex><span>packets received:      1874993118
</span></span><span style=display:flex><span>current version:       1455746472
</span></span><span style=display:flex><span>older version:         418760288
</span></span><span style=display:flex><span>bad length or format:  534317
</span></span><span style=display:flex><span>authentication failed: 361395
</span></span><span style=display:flex><span>declined:              393
</span></span><span style=display:flex><span>restricted:            15845
</span></span><span style=display:flex><span>rate limited:          123140728
</span></span><span style=display:flex><span>KoD responses:         15874848
</span></span><span style=display:flex><span>processed for time:    2554
</span></span></code></pre></div><p>These numbers are necessary but not sufficient: we want to know, &ldquo;how much will
my data transfer cost each month?&rdquo; <sup><a href=#aws_data_transfer_cost>[AWS
data transfer cost]</a></sup> To determine that, we&rsquo;ll need to know Amazon
data transfer pricing, our inbound and especially outbound
traffic, and the size of NTP packets in bytes.</p><p>We derive additional statistics using the above numbers. In the above example
(which was the output from our Google NTP server on Thu Jan 5 04:02:45 UTC 2017)
we were able to calculate the traffic in GiB per month as follows:</p><ul><li><strong>packets sent</strong> = packets received - bad length or format - authentication failed - declined - restricted - rate limited = <strong>1750940440</strong> <sup><a href=#ntp_packets_sent>[NTP packets sent]</a></sup><br></li><li><strong>packets received per second</strong> = packets received / uptime = <strong>3115.38</strong></li><li><strong>packets sent per second</strong> = packets sent / uptime = <strong>2909.26</strong></li><li><strong>packets received per month</strong> = packets received / uptime × 60 secs/min × 60 mins/hr × 24 hr/day × 30.436875 day/month = <strong>8192651756</strong></li><li><strong>packets sent per month</strong> = packets sent / uptime × 60 secs/min × 60 mins/hr × 24 hr/day × 30.436875 day/month = <strong>7650612225</strong></li><li><strong>gigabytes received per month</strong> = packets received per month × bytes per packet <sup><a href=#ntp_packet_size>[NTP packet size]</a></sup> × gigabytes per byte = <strong>579.88</strong></li><li><strong>gigabytes sent per month</strong> = packets sent per month × bytes per packet × gigabytes per byte = <strong>541.51</strong></li></ul><p><a href=https://www.quora.com/What-is-the-average-number-of-days-in-a-month>The average number of days per
month</a> in
the Gregorian calendar is 365.2425 / 12 = <strong>30.436875</strong>. It would be
irresponsible for a post about NTP to casually peg the number of days in a month
to 30. Respect time.</p><p>The raw data from which the numbers in this blog post are derived can be found
in a
<a href="https://docs.google.com/spreadsheets/d/1yvHpDjaD3pOV2C0-Zb9ljdvdWxDc9V8oZnF5IvfLZew/edit?usp=sharing">spreadsheet</a>.
The organization is haphazard. The data contained therein is released into the
public domain.</p><p><sup><a name=aws_data_transfer_cost>[AWS data transfer]</a></sup></p><p>AWS charges <a href=https://aws.amazon.com/ec2/pricing/on-demand/#Data_Transfer><strong>$0.09 per
GiB</strong></a> for Data
Transfer (bandwidth) OUT from Amazon EC2 us-east-1 (Virginia) region to
Internet. Inbound data transfer is free.</p><p>As with much of Amazon pricing, this is a rule-of-thumb and subject to qualifiers:</p><ul><li>Amazon has volume discounts; e.g. traffic above 10TB/month is charged at
$0.085/GiB, above 50TB/month is charged at $0.07/GiB, etc&mldr;</li><li>Amazon charges much less for traffic to other Amazon datacenters (e.g. outbound
traffic to another AWS Region is $0.02/GiB)</li><li>Inbound traffic is not always free; Amazon charges, for example, $0.01/GiB
for traffic originating from the same Availability Zone using a public or Elastic IPv4 address</li></ul><p>In spite of these qualifiers, we feel the $0.09/GiB is an appropriate value to
use in our calculations.</p><p>AWS <a href="https://forums.aws.amazon.com/thread.jspa?threadID=151161">measures</a> their
data transfer pricing in terms of <a href=https://wikipedia.org/wiki/Gibibyte>GiB</a>
(2<sup>30</sup>) instead of <a href=https://en.wikipedia.org/wiki/Gigabyte>GB</a>
(10<sup>9</sup>) (although their documentation refer to the units as &ldquo;GB&rdquo;).</p><p><sup><a name=google_data_transfer_cost>[Google data transfer]</a></sup></p><p>Google charges <a href=https://cloud.google.com/compute/pricing><strong>$0.12 per
GiB</strong></a> for Data
Transfer (bandwidth) OUT from Google Cloud Platform.</p><p>Similar to AWS&rsquo;s data transfer pricing, there are qualifiers:</p><ul><li>Google has volume discounts (e.g. with tiers at 1TB ($0.11) and 10TB ($0.08))</li><li>Egress to China (but not Hong Kong) is almost double (e.g. $0.23). Network Egress
to Australia is also more expensive ($0.19)</li><li>Prices do not apply to Google&rsquo;s Content Deliver Network (CDN) service</li></ul><p>We find Google&rsquo;s pricing to be simpler than Amazon&rsquo;s — Google&rsquo;s pricing is
uniform across Google&rsquo;s datacenters, whereas Amazon&rsquo;s data transfer costs can
vary by region (datacenter).</p><p>Google <a href=https://cloud.google.com/compute/pricing>explicitly defines</a> their data
transfer pricing in terms of <a href=https://wikipedia.org/wiki/Gibibyte>GiB</a>
(2<sup>30</sup>) instead of <a href=https://en.wikipedia.org/wiki/Gigabyte>GB</a> (10<sup>9</sup>). A GB is 0.931 the size of a GiB (the GiB is bigger, and handsomer too, I might add):</p><blockquote><p>Disk size, machine type memory, and network usage are calculated in gigabytes (GB), where 1 GB is 2<sup>30</sup> bytes</p></blockquote><p><sup><a name=digital_ocean_bandwidth_metering>[DigitalOcean bandwidth metering]</a></sup></p><p>DigitalOcean&rsquo;s bandwidth pricing is more aspirational than actual: They have not
yet implemented bandwidth metering, though they plan to do so in the future. Per
their response to our ticket opened on 2017-01-17 (boldface ours):</p><blockquote><p>We do not have a pricing scheme set up yet for bandwidth. As such, we do not
actually charge for it. Our engineering team is working on a solution for this,
but it has not been given an ETA. We often suggest customers who are heavy BW
utilizes to just <strong>be kind</strong> to our platform & <strong>be mindful</strong> when we reach out
with requests to slow down just a bit, as times, it can become disruptive to
customers downstream.</p></blockquote><p>Some may be tempted, knowing that the bandwidth is not measured, to opt
for the lower-tier $5/month server (with 1TB bandwidth) to provide an NTP server
to the community. We find such a decision to be on ethically shaky ground, for
we know that our bandwidth would most likely exceed that amount (we estimate 1.1 -
1.4 TiB aggregate inbound and outbound), and we&rsquo;d be taking advantage of
DigitalOcean&rsquo;s momentary bandwidth blind spot. As such, any benefits we would
provide to the community would be, in a sense, fruit of a poisonous tree.</p><p><sup><a name=ntp_packets_sent>[NTP packets sent]</a></sup></p><p>Although <code>ntpq -c sysstats</code> does not display the number of packets sent (only
the number of packets received), the number can be calculated from the remaining
fields, i.e. the number of packets sent minus the number of packets dropped.
According to <a href=http://www.openswitch.net/documents/user/ntp_user_guide#generic-tips-for-troubleshooting>opsenswitch.net</a>, these are the categories of packets that are dropped:</p><ul><li>bad length or format</li><li>authentication failed</li><li>declined</li><li>restricted</li><li>rate limited</li></ul><p><sup><a name=ntp_packet_size>[NTP packet size]</a></sup></p><p>IPv4 NTP packets are <strong>76 octets</strong> (bytes) (IPv6 NTP packets are 96 octets, but
we ignore the IPv6 packet size for the purposes of our calculations — the two
servers (AWS & Google) from which we gather statistics are IPv4-only).</p><p>We calculate the size as follows: we know the size of an NTP packet is almost
always 48 bytes (it can be longer; the <a href=https://tools.ietf.org/html/rfc5905#section-7.3>NTP
RFC</a> allows for for extension
fields, key identifiers, and digests in the packet, but in practice we rarely
see those fields populated).</p><p>The NTP packet is encapsulated in a User Datagram Protocol (UDP) packet, which
adds <a href=https://en.wikipedia.org/wiki/User_Datagram_Protocol#Packet_structure>8 octets</a>
to the length of the packet, bringing the total length to 56.</p><p>The UDP packet in turn is encapsulated in an IP (IPv4 or IPv6) packet. IPv4 adds
<a href=https://en.wikipedia.org/wiki/IPv4#Header>20 octets</a> to the length of the
packet, bringing the total to 76. IPv6 adds <a href=https://en.wikipedia.org/wiki/IPv6_packet#Fixed_header>40
octets</a> to the length,
for a total of 96.</p><p>The IP packet in turn is encapsulated in an Ethernet II packet. The Ethernet II
header does not include the 7-octet Preamble, the 1-octet Start of frame
delimiter, the optional 802.1Q tag (we&rsquo;re not using VLAN-tagging), nor the
4-octet Frame check sequence. It <em>does</em> include the 6-octet MAC destination and
the 6-octet source, as well as the 2-octet ethertype field, which adds <a href=https://en.wikipedia.org/wiki/Ethernet_frame#Structure>14
octets</a> to the length,
for a grand total packet size of <strong>90 bytes for IPv4</strong> and <strong>110 bytes for
IPv6</strong>.</p><p>To confirm our calculations, we turn to <code>tcpdump</code>, a popular network sniffer (a
tools which listens, filters, and decodes network traffic). We use it to expose the
lengths of an IPv4 NTP packet.</p><ul><li>The Ethernet II packet has a length of 90 octets (&ldquo;length 90&rdquo;)</li><li>The IPv4 portion has a length of 76 octets (&ldquo;length 76&rdquo;)</li><li>The NTP packet has a length of 48 octets (&ldquo;NTPv4, length 48&rdquo;)</li></ul><pre tabindex=0><code>$ sudo tcpdump -ennv -i vtnet0 -c 1 port ntp
tcpdump: listening on vtnet0, link-type EN10MB (Ethernet), capture size 262144 bytes
07:42:13.181157 d2:74:7f:6e:37:e3 &gt; 52:54:a2:01:66:7d, ethertype IPv4 (0x0800), length 90: (tos 0x0, ttl 44, id 0, offset 0, flags [DF], proto UDP (17), length 76)
    107.137.130.39.123 &gt; 172.31.1.100.123: NTPv4, length 48
  Client, Leap indicator: clock unsynchronized (192), Stratum 0 (unspecified), poll 4 (16s), precision -6
  Root Delay: 1.000000, Root dispersion: 1.000000, Reference-ID: (unspec)
    Reference Timestamp:  0.000000000
    Originator Timestamp: 0.000000000
    Receive Timestamp:    0.000000000
    Transmit Timestamp:   3692878933.085937805 (2017/01/08 07:42:13)
      Originator - Receive Timestamp:  0.000000000
      Originator - Transmit Timestamp: 3692878933.085937805 (2017/01/08 07:42:13)
1 packet captured
2 packets received by filter
0 packets dropped by kernel
</code></pre><p>Note the flags passed to <code>tcpdump</code>:</p><ul><li><code>-e</code> capture the Ethernet frame, needed for Ethernet II length.</li><li><code>-nn</code> don&rsquo;t lookup hosts, don&rsquo;t lookup ports</li><li><code>-v</code> breaks out the size of the UDP packet (56 octets)</li><li><code>-i vtnet0</code> specifies a particular Ethernet interface (typically <code>eth0</code> on Linux)</li><li><code>-c 1</code> capture one packet then exit</li><li><code>port ntp</code> listen for IPv4 packets whose source or destination port is NTP&rsquo;s (123)</li></ul><p>To capture an IPv6 packet, use this variation:</p><pre tabindex=0><code>sudo tcpdump -ennv -i vtnet0 -c 1 ip6 and port ntp
</code></pre><p>AWS does not bill for the layer 2 (data link layer, Ethernet II) of traffic,
only for the IP portion. In other words, NTP packets have a length of 76 octets
(as far as billing measurement is concerned). <sup><a href=#aws_net_metrics>[AWS Network Metrics]</a></sup></p><p>Google, too, does not bill for layer 2 traffic, only for IP traffic.
<sup><a href=#gce_net_metrics>[Google Network Metrics]</a></sup></p><p><sup><a name=aws_net_metrics>[AWS Network Metrics]</a></sup></p><p>We are confident that AWS charges only for the IP-portion of network packets
and not the Ethernet portion, but we were unable to find this explicitly in
writing. We deduced it by running the following test.</p><p>First, we measured the amount of NTP traffic over a one-hour period using the
following command on our AWS NTP server. We kicked off the command on Wed Jan
25 05:00:00 UTC 2017:</p><pre tabindex=0><code>ntpq -c sysstat; sleep 3600; ntpq -c sysstat; date
...
Wed Jan 25 06:00:00 UTC 2017
</code></pre><p>The number of inbound packets received during the hour was 9,961,945. Now that
we know the number of packets, we need to find out the number of bytes. And then
it&rsquo;s simple math: we divide the number of bytes by the number of packets. If the
number is close to 90, then AWS is measuring the Ethernet frame, 76, the IP
frame. We expect the number of inbound bytes to be approximately 757,107,820
(76 × 9,961,945).</p><p>We generate an AWS Usage report of inbound bytes:</p><p>AWS Console → My Account → Reports → AWS Usage Report → Amazon Elastic Compute Cloud</p><p>Calamity has struck! The number of inbound bytes from 05:00-06:00 is 657,882,293
is much too small. Not even close. Not even possible. The absolute minimum
number of bytes is 757,107,820 (that&rsquo;s the number of NTP packets × the
minimum NTP packet size, 76 bytes). It&rsquo;s possible to have <em>more</em> traffic (e.g.
larger NTP packets, non-NTP traffic (ssh, DNS)), but not less.</p><p>Here&rsquo;s a snippet of the CSV downloaded from AWS:</p><pre tabindex=0><code>Service, Operation, UsageType, Resource, StartTime, EndTime, UsageValue
...
AmazonEC2,RunInstances,DataTransfer-In-Bytes,,01/25/17 05:00:00,01/25/17 06:00:00,657882293
...
</code></pre><p>With the numbers we have, there are ~66 bytes per packet, and we need to get to
76 bytes per packet.</p><p>Maybe the report is in a different time zone? No, we know that &ldquo;<a href="https://forums.aws.amazon.com/thread.jspa?threadID=88004">All usage
reports for AWS services are in
GMT</a>&rdquo;.</p><p>Aha! We forgot the Usage Type <em>DataTransfer-Regional-Bytes</em>. We run that report
and see the following data:</p><pre tabindex=0><code>Service, Operation, UsageType, Resource, StartTime, EndTime, UsageValue
...
AmazonEC2,InterZone-In,DataTransfer-Regional-Bytes,,01/25/17 05:00:00,01/25/17 06:00:00,2052
AmazonEC2,PublicIP-In,DataTransfer-Regional-Bytes,,01/25/17 05:00:00,01/25/17 06:00:00,76761576
</code></pre><p>Here is an interesting tidbit: <strong>10% of the inbound NTP traffic
originates from within the Amazon cloud</strong>.</p><p>We decide to create a chart to visually express how closely our NTP server&rsquo;s
statistics matches AWS&rsquo;s data transfer metrics assuming that NTP packets are 76
bytes:</p><figure><img loading=lazy src="https://docs.google.com/spreadsheets/d/1yvHpDjaD3pOV2C0-Zb9ljdvdWxDc9V8oZnF5IvfLZew/pubchart?oid=1439682529&amp;format=image"></figure><p>The NTP server&rsquo;s statistics are collected every five minutes, and are thus a line.
The AWS data transfer statistics are coarser, only by the hour, and show up as purple dots.
It&rsquo;s evident that the two correlate, within a few percentage points.</p><p>We expected the AWS&rsquo;s numbers to exceed ours by a slight amount, for our numbers
collected via <code>ntpq</code> only report the NTP traffic, and we know that our server
carries other traffic as well (it&rsquo;s a DNS server, too); however, we found the
opposite to be true: the traffic reported by AWS was consistently smaller than
our NTP traffic, albeit by a small amount. We are not sure why, and are
presenting this as a mystery.</p><p>To collect our statistics, we ran the following on our NTP server:</p><pre tabindex=0><code>while :; do
  /var/vcap/packages/ntp/bin/ntpq -c sysstat &gt; /tmp/ntp.$(date +%s)
  sleep 300
done
</code></pre><p>We waited several hours, then collated everything:</p><pre tabindex=0><code>cd /tmp/
grep -h received ntp.* | awk &#39;{print $3}&#39; &gt; five_minute_intervals.ntp
</code></pre><p>We uploaded the file into Google Sheets, calculate the delta between the five
minute intervals, created a rolling hourly sum, and compared it with the output
of the AWS usage report.</p><p><sup><a name=gce_net_metrics>[Google Network Metrics]</a></sup></p><p>We are confident that Google charges only for the IP-portion of network packets
and not the Ethernet portion, but we were unable to find this explicitly in
writing. Instead, we inferred this via the manner in which Google measures
packet size, and were so pleased with our methodology that we would like to
share it.</p><p>First, we brought up in our browser the Google Compute Engine Dashboard, twice.
Then we selected our NTP server VM, twice. On one, we selected, &ldquo;Network
Packets&rdquo;, and the other we selected &ldquo;Network Bytes&rdquo;. The following is a mash-up
of the two charts:</p><figure><img loading=lazy src=https://cloud.githubusercontent.com/assets/1020675/21968913/b43f69b8-db4e-11e6-8718-cc35bea3de02.png></figure><p>Having both numbers allowed us to calculate the number of bytes per packet.
If the number was close to 90 bytes per packet, then Google was including the
Ethernet/data link layer. If the number was close to 76 bytes per packet, then
Google was only counting the IPv4 portion of the packet.</p><p>Our numbers? 76.19 and 76.06, within 0.25% of 76 bytes. Our conclusion? Google
is only counting the IPv4 portion of the packet. (You may ask why we are not
concerned that the average number of bytes per packet is not <em>exactly</em> 76 bytes.
The answer? The servers carry traffic other than NTP (e.g. the Google server is
both a DNS server and a <a href=http://concourse.ci/>Concourse</a> continuous
integration (CI) server, which often have packet sizes other than 76 bytes.
Also, a small portion of NTP packets are greater than 76 bytes).</p><iframe style=width:100% src="https://docs.google.com/spreadsheets/d/1yvHpDjaD3pOV2C0-Zb9ljdvdWxDc9V8oZnF5IvfLZew/pubhtml?gid=421145593&widget=true&headers=false&range=A46:E48"></iframe><p><sup><a name=ntp_charts>[NTP charts]</a></sup></p><p>The NTP Pool project provides publicly-accessible charts for the servers within
the pool. Here are links to the charts of the servers that we maintain and to
their ntpd (and, in one case, their <a href=https://chrony.tuxfamily.org/>chronyd</a>)
configuration files:</p><ul><li><a href=http://www.pool.ntp.org/scores/104.155.144.4>Google server, US (ns-gce.nono.io)</a> <a href=https://github.com/cunnie/deployments/blob/4578625694140ccdb9303997d39686e11cd6ffe6/concourse-ntp-pdns-gce.yml#L178-L192>(ntp.conf)</a></li><li><a href=http://www.pool.ntp.org/scores/52.0.56.137>AWS server, US (ns-aws.nono.io)</a> <a href=https://github.com/cunnie/deployments/blob/4578625694140ccdb9303997d39686e11cd6ffe6/nginx-ntp-pdns-aws.yml#L167-L181>(ntp.conf)</a></li><li><a href=http://www.pool.ntp.org/scores/52.187.42.158>Azure server, Singapore (ns-azure.nono.io)</a> <a href=https://github.com/cunnie/deployments/blob/4578625694140ccdb9303997d39686e11cd6ffe6/nginx-ntp-pdns-azure.yml#L212-L225>(ntp.conf)</a></li><li>Hetzner server, Germany (shay.nono.io) <a href=https://github.com/cunnie/shay.nono.io-etc/blob/86a5548d2f89ee7c0d5fd7cf20b94ea0aea2fb0c/ntp.conf>(ntp.conf)</a>:<ul><li><a href=http://www.pool.ntp.org/scores/78.46.204.247>IPv4</a></li><li><a href=http://www.pool.ntp.org/scores/2a01:4f8:c17:b8f::2>IPv6</a></li></ul></li><li>Comcast server, US (time-home.nono.io) <a href=https://github.com/cunnie/fedora.nono.io-etc/blob/14ce418bdd1e9d57fe145d0f7d44ce39f479019a/chrony.conf>(chrony.conf)</a>:<ul><li><a href=http://www.pool.ntp.org/scores/73.15.134.22>IPv4</a></li><li><a href=http://www.pool.ntp.org/scores/2601:646:100:e8e8::101>IPv6</a></li></ul></li></ul><h2 id=corrections--updates><a name=corrections>Corrections & Updates</a><a hidden class=anchor aria-hidden=true href=#corrections--updates>#</a></h2><p><em>2017-02-04</em></p><p>A quote on the mechanism that pool.ntp.org uses to select servers was missing
the phrase, &ldquo;just on those&rdquo;. The quote has been corrected.</p><p>The calculation for cost of aggregate data transfer for the entire US
pool.ntp.org did not take into account tiered pricing. Pricing was adjusted:
original cost was $5,640, adjusted cost is $4,078.</p><p>Phrasing was changed to improved readability.</p><p>We removed a comment that pointed out we had not gathered statistics for our
Azure NTP server; it seemed pointless.</p><p><em>2017-02-01</em></p><p>The post mis-characterized the mechanism behind the NTP pool as &ldquo;round-robin
DNS&rdquo;; the mechanism is more sophisticated: It targets the users to servers
in/near their country and does a weighted round-robin just on those servers.</p><p><a href=https://www.askask.com/>Ask Bjørn Hansen</a> said:</p><blockquote><p>The system is a little more sophisticated than just round-robin DNS. It
targets the users to servers in/near their country and does a weighted
round-robin just on those servers.</p></blockquote><p>We have added sections describing our motives for operating NTP servers and
encouraging others to join the pool. Thanks Leo Bodnar.</p><p>We wrongly encouraged NTP pool servers to use Google&rsquo;s NTP servers as upstream
providers. We now warn <em>against</em> using Google&rsquo;s NTP servers, and provide reasons
why (leap seconds). Thanks <a href=https://community.ntppool.org/t/feedback-why-is-my-ntp-server-costing-me-500-per-year/146/9>Joseph
B</a>,
Ask.</p><p>We added statistics regarding the aggregate netspeed for the US zone.</p><p><a href=https://community.ntppool.org/t/feedback-why-is-my-ntp-server-costing-me-500-per-year/146>NTP Pool
operators</a>
suggested the following IaaSes:</p><ul><li>Amazon Lightsail</li><li>Linode (we&rsquo;ve had positive experience with Linode)</li><li>Vultr</li><li>BuyVM</li><li>Ramnode</li><li>LunaNode</li><li>Atlantic</li><li>ARP Networks (we&rsquo;ve had positive experience with ARP Networks)</li><li>Scaleway</li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://blog.nono.io/post/bosh-vsphere-opaque-networks/><span class=title>« Prev</span><br><span>Deploy To vSphere NSX-T Opaque Networks Using BOSH</span>
</a><a class=next href=https://blog.nono.io/post/bosh-on-ipv6/><span class=title>Next »</span><br><span>Using the beta BOSH CLI to Deploy an IPv6-enabled nginx Server to AWS</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Why Is My NTP Server Costing $500/Year? Part 3 on x" href="https://x.com/intent/tweet/?text=Why%20Is%20My%20NTP%20Server%20Costing%20%24500%2fYear%3f%20Part%203&amp;url=https%3a%2f%2fblog.nono.io%2fpost%2fntp-costs-500%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Why Is My NTP Server Costing $500/Year? Part 3 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fblog.nono.io%2fpost%2fntp-costs-500%2f&amp;title=Why%20Is%20My%20NTP%20Server%20Costing%20%24500%2fYear%3f%20Part%203&amp;summary=Why%20Is%20My%20NTP%20Server%20Costing%20%24500%2fYear%3f%20Part%203&amp;source=https%3a%2f%2fblog.nono.io%2fpost%2fntp-costs-500%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Why Is My NTP Server Costing $500/Year? Part 3 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fblog.nono.io%2fpost%2fntp-costs-500%2f&title=Why%20Is%20My%20NTP%20Server%20Costing%20%24500%2fYear%3f%20Part%203"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Why Is My NTP Server Costing $500/Year? Part 3 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fblog.nono.io%2fpost%2fntp-costs-500%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Why Is My NTP Server Costing $500/Year? Part 3 on whatsapp" href="https://api.whatsapp.com/send?text=Why%20Is%20My%20NTP%20Server%20Costing%20%24500%2fYear%3f%20Part%203%20-%20https%3a%2f%2fblog.nono.io%2fpost%2fntp-costs-500%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Why Is My NTP Server Costing $500/Year? Part 3 on telegram" href="https://telegram.me/share/url?text=Why%20Is%20My%20NTP%20Server%20Costing%20%24500%2fYear%3f%20Part%203&amp;url=https%3a%2f%2fblog.nono.io%2fpost%2fntp-costs-500%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Why Is My NTP Server Costing $500/Year? Part 3 on ycombinator" href="https://news.ycombinator.com/submitlink?t=Why%20Is%20My%20NTP%20Server%20Costing%20%24500%2fYear%3f%20Part%203&u=https%3a%2f%2fblog.nono.io%2fpost%2fntp-costs-500%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://blog.nono.io/>Brian Cunnie's Technical Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>